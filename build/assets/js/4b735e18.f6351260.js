"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[6010],{8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var i=t(6540);const s={},r=i.createContext(s);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(r.Provider,{value:n},e.children)}},9554:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>a,toc:()=>l});var i=t(4848),s=t(8453);const r={title:"Perception Stacks - Multi-Modal Sensor Processing"},o="Perception Stacks: Multi-Modal Sensor Processing",a={id:"perception-pipelines/perception-stacks",title:"Perception Stacks - Multi-Modal Sensor Processing",description:"Introduction to Perception Stacks",source:"@site/docs/physical-ai/perception-pipelines/perception-stacks.mdx",sourceDirName:"perception-pipelines",slug:"/perception-pipelines/perception-stacks",permalink:"/perception-pipelines/perception-stacks",draft:!1,unlisted:!1,editUrl:"https://github.com/fuzailpalook/new-book/tree/main/docs/physical-ai/perception-pipelines/perception-stacks.mdx",tags:[],version:"current",frontMatter:{title:"Perception Stacks - Multi-Modal Sensor Processing"},sidebar:"tutorialSidebar",previous:{title:"Synthetic Data Generation for AI Perception",permalink:"/perception-pipelines/synthetic-data"},next:{title:"Chapter 10 Summary - AI Perception, Synthetic Data & Manipulation Pipelines",permalink:"/perception-pipelines/chapter-summary"}},c={},l=[{value:"Introduction to Perception Stacks",id:"introduction-to-perception-stacks",level:2},{value:"Multi-Modal Sensor Fusion",id:"multi-modal-sensor-fusion",level:2},{value:"Sensor Characteristics and Complementarity",id:"sensor-characteristics-and-complementarity",level:3},{value:"Fusion Strategies",id:"fusion-strategies",level:3},{value:"Early Fusion",id:"early-fusion",level:4},{value:"Late Fusion",id:"late-fusion",level:4},{value:"Deep Fusion",id:"deep-fusion",level:4},{value:"Visual Perception Stack",id:"visual-perception-stack",level:2},{value:"Feature Extraction and Matching",id:"feature-extraction-and-matching",level:3},{value:"Semantic Segmentation Stack",id:"semantic-segmentation-stack",level:3},{value:"3D Perception Stack",id:"3d-perception-stack",level:2},{value:"Point Cloud Processing",id:"point-cloud-processing",level:3},{value:"Tactile Perception Stack",id:"tactile-perception-stack",level:2},{value:"Force and Tactile Sensing",id:"force-and-tactile-sensing",level:3},{value:"Perception Quality and Uncertainty",id:"perception-quality-and-uncertainty",level:2},{value:"Uncertainty Quantification",id:"uncertainty-quantification",level:3},{value:"Real-time Performance Optimization",id:"real-time-performance-optimization",level:2},{value:"Efficient Processing Pipelines",id:"efficient-processing-pipelines",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"perception-stacks-multi-modal-sensor-processing",children:"Perception Stacks: Multi-Modal Sensor Processing"}),"\n",(0,i.jsx)(n.h2,{id:"introduction-to-perception-stacks",children:"Introduction to Perception Stacks"}),"\n",(0,i.jsx)(n.p,{children:"Perception stacks in robotics refer to the integrated processing pipelines that combine data from multiple sensors to create a comprehensive understanding of the environment. Unlike single-sensor approaches, perception stacks leverage the complementary nature of different sensing modalities to achieve robust, accurate, and reliable perception in complex environments."}),"\n",(0,i.jsx)(n.p,{children:"A typical robotic perception stack includes processing modules for:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Visual sensors (RGB cameras, stereo cameras, event cameras)"}),"\n",(0,i.jsx)(n.li,{children:"Depth sensors (LIDAR, depth cameras, stereo vision)"}),"\n",(0,i.jsx)(n.li,{children:"Inertial sensors (IMU, accelerometers, gyroscopes)"}),"\n",(0,i.jsx)(n.li,{children:"Tactile sensors (force/torque sensors, tactile skins)"}),"\n",(0,i.jsx)(n.li,{children:"Other specialized sensors (thermal, ultrasonic, etc.)"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"multi-modal-sensor-fusion",children:"Multi-Modal Sensor Fusion"}),"\n",(0,i.jsx)(n.h3,{id:"sensor-characteristics-and-complementarity",children:"Sensor Characteristics and Complementarity"}),"\n",(0,i.jsx)(n.p,{children:"Different sensors provide complementary information with distinct advantages and limitations:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Sensor Type"}),(0,i.jsx)(n.th,{children:"Advantages"}),(0,i.jsx)(n.th,{children:"Limitations"}),(0,i.jsx)(n.th,{children:"Typical Use Cases"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"RGB Cameras"}),(0,i.jsx)(n.td,{children:"High resolution, color information, low cost"}),(0,i.jsx)(n.td,{children:"Lighting dependent, no depth"}),(0,i.jsx)(n.td,{children:"Object recognition, scene understanding"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Stereo Cameras"}),(0,i.jsx)(n.td,{children:"Depth estimation, passive sensing"}),(0,i.jsx)(n.td,{children:"Accuracy decreases with distance, computationally expensive"}),(0,i.jsx)(n.td,{children:"3D reconstruction, obstacle detection"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"LIDAR"}),(0,i.jsx)(n.td,{children:"Accurate depth, works in darkness"}),(0,i.jsx)(n.td,{children:"Expensive, limited resolution"}),(0,i.jsx)(n.td,{children:"Mapping, navigation, precise localization"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Depth Cameras"}),(0,i.jsx)(n.td,{children:"Dense depth maps"}),(0,i.jsx)(n.td,{children:"Short range, affected by transparency"}),(0,i.jsx)(n.td,{children:"Manipulation, indoor navigation"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"IMU"}),(0,i.jsx)(n.td,{children:"High frequency, motion detection"}),(0,i.jsx)(n.td,{children:"Drift over time, no absolute position"}),(0,i.jsx)(n.td,{children:"State estimation, motion tracking"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"fusion-strategies",children:"Fusion Strategies"}),"\n",(0,i.jsx)(n.h4,{id:"early-fusion",children:"Early Fusion"}),"\n",(0,i.jsx)(n.p,{children:"Early fusion combines raw sensor data at the lowest level before processing:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport cv2\nfrom scipy.spatial.transform import Rotation as R\n\nclass EarlyFusion:\n    def __init__(self):\n        self.camera_intrinsics = np.array([\n            [525.0, 0.0, 319.5],\n            [0.0, 525.0, 239.5],\n            [0.0, 0.0, 1.0]\n        ])\n\n    def project_lidar_to_camera(self, lidar_points, camera_pose, lidar_pose):\n        """\n        Project LIDAR points to camera coordinate system\n        """\n        # Transform LIDAR points to camera frame\n        relative_transform = np.linalg.inv(camera_pose) @ lidar_pose\n        points_homogeneous = np.hstack([lidar_points, np.ones((lidar_points.shape[0], 1))])\n        points_camera = (relative_transform @ points_homogeneous.T).T[:, :3]\n\n        # Project to image plane\n        points_image = points_camera @ self.camera_intrinsics.T\n        points_image = points_image[:, :2] / points_image[:, 2:3]  # Normalize by depth\n\n        return points_image, points_camera[:, 2]  # (u, v) coordinates and depths\n\n    def fuse_rgb_depth(self, rgb_image, depth_map, lidar_points, lidar_intensities):\n        """\n        Create a fused RGB-D representation with LIDAR enhancement\n        """\n        # Project LIDAR points to image\n        lidar_uv, lidar_depths = self.project_lidar_to_camera(lidar_points, np.eye(4), np.eye(4))\n\n        # Create fused representation\n        height, width = rgb_image.shape[:2]\n        fused_image = np.zeros((height, width, 4), dtype=np.float32)  # RGB + enhanced depth\n\n        # Fill RGB channels\n        fused_image[:, :, :3] = rgb_image.astype(np.float32)\n\n        # Enhance depth map with LIDAR data\n        for i, (u, v) in enumerate(lidar_uv):\n            if 0 <= int(u) < width and 0 <= int(v) < height:\n                # Use LIDAR depth where available, otherwise use camera depth\n                if depth_map[int(v), int(u)] == 0 or lidar_depths[i] < depth_map[int(v), int(u)]:\n                    fused_image[int(v), int(u), 3] = lidar_depths[i]\n                else:\n                    fused_image[int(v), int(u), 3] = depth_map[int(v), int(u)]\n\n        return fused_image\n'})}),"\n",(0,i.jsx)(n.h4,{id:"late-fusion",children:"Late Fusion"}),"\n",(0,i.jsx)(n.p,{children:"Late fusion combines the outputs of individual sensor processing modules:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class LateFusion:\n    def __init__(self):\n        self.camera_detector = ObjectDetector('rgb')\n        self.lidar_detector = ObjectDetector('lidar')\n        self.fusion_weights = {'camera': 0.6, 'lidar': 0.4}\n\n    def fuse_detections(self, camera_detections, lidar_detections, camera_pose, lidar_pose):\n        \"\"\"\n        Fuse object detections from camera and LIDAR\n        \"\"\"\n        # Transform LIDAR detections to camera coordinate system\n        transformed_detections = []\n        for detection in lidar_detections:\n            # Transform 3D bounding box from LIDAR to camera frame\n            transformed_box = self.transform_3d_bbox(\n                detection['bbox'],\n                lidar_pose,\n                camera_pose\n            )\n            transformed_detections.append({\n                'bbox': transformed_box,\n                'class': detection['class'],\n                'confidence': detection['confidence'] * self.fusion_weights['lidar']\n            })\n\n        # Combine with camera detections\n        all_detections = camera_detections + transformed_detections\n\n        # Apply Non-Maximum Suppression to remove duplicates\n        fused_detections = self.non_max_suppression_3d(all_detections)\n\n        return fused_detections\n\n    def non_max_suppression_3d(self, detections, iou_threshold=0.5):\n        \"\"\"\n        3D Non-Maximum Suppression for fused detections\n        \"\"\"\n        if len(detections) == 0:\n            return []\n\n        # Sort by confidence\n        detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)\n\n        keep = []\n        while len(detections) > 0:\n            # Take the highest confidence detection\n            current = detections[0]\n            keep.append(current)\n\n            # Calculate IoU with remaining detections\n            remaining = []\n            for det in detections[1:]:\n                iou = self.calculate_3d_iou(current['bbox'], det['bbox'])\n                if iou < iou_threshold:\n                    # Adjust confidence based on overlap\n                    det['confidence'] *= (1 - iou)\n                    if det['confidence'] > 0.1:  # minimum confidence threshold\n                        remaining.append(det)\n\n            detections = remaining\n\n        return keep\n"})}),"\n",(0,i.jsx)(n.h4,{id:"deep-fusion",children:"Deep Fusion"}),"\n",(0,i.jsx)(n.p,{children:"Deep fusion uses learned approaches to combine sensor information:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\n\nclass DeepSensorFusion(nn.Module):\n    def __init__(self, num_classes=10):\n        super(DeepSensorFusion, self).__init__()\n\n        # RGB branch\n        self.rgb_branch = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n            nn.ReLU(),\n            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n            nn.ReLU()\n        )\n\n        # LIDAR branch (processed as point cloud or range image)\n        self.lidar_branch = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),  # Intensity\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n            nn.ReLU()\n        )\n\n        # Fusion layer\n        self.fusion_conv = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),  # 256*2 from both branches\n            nn.ReLU(),\n            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n            nn.ReLU()\n        )\n\n        # Task-specific heads\n        self.detection_head = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(256, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes * 5)  # [x, y, w, h, conf] for each class\n        )\n\n        self.segmentation_head = nn.Sequential(\n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, num_classes, kernel_size=1)\n        )\n\n    def forward(self, rgb_input, lidar_input):\n        # Process each modality separately\n        rgb_features = self.rgb_branch(rgb_input)\n        lidar_features = self.lidar_branch(lidar_input)\n\n        # Concatenate features along channel dimension\n        fused_features = torch.cat([rgb_features, lidar_features], dim=1)\n\n        # Apply fusion convolution\n        fused_output = self.fusion_conv(fused_features)\n\n        # Generate task-specific outputs\n        detection_output = self.detection_head(fused_output)\n        segmentation_output = self.segmentation_head(fused_output)\n\n        return {\n            'detection': detection_output,\n            'segmentation': segmentation_output,\n            'fused_features': fused_output\n        }\n"})}),"\n",(0,i.jsx)(n.h2,{id:"visual-perception-stack",children:"Visual Perception Stack"}),"\n",(0,i.jsx)(n.h3,{id:"feature-extraction-and-matching",children:"Feature Extraction and Matching"}),"\n",(0,i.jsx)(n.p,{children:"Modern visual perception stacks use deep learning for feature extraction and matching:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class VisualPerceptionStack:\n    def __init__(self):\n        # Pre-trained feature extractor\n        import torchvision.models as models\n        self.feature_extractor = models.resnet50(pretrained=True)\n        self.feature_extractor.fc = nn.Identity()\n\n        # Descriptor normalization\n        self.normalize = nn.functional.normalize\n\n        # Keypoint detector (traditional approach as fallback)\n        self.keypoint_detector = cv2.SIFT_create()\n\n    def extract_features(self, image):\n        """\n        Extract deep features from image using CNN\n        """\n        # Convert to tensor and normalize\n        image_tensor = torch.from_numpy(image).float().permute(2, 0, 1).unsqueeze(0)\n        image_tensor = image_tensor / 255.0\n        image_tensor = torch.nn.functional.interpolate(\n            image_tensor, size=(224, 224), mode=\'bilinear\'\n        )\n\n        # Extract features\n        with torch.no_grad():\n            features = self.feature_extractor(image_tensor)\n\n        return self.normalize(features, dim=1)\n\n    def match_features(self, desc1, desc2, threshold=0.8):\n        """\n        Match features between two images\n        """\n        # Compute similarity matrix\n        similarity = torch.mm(desc1, desc2.t())\n\n        # Find matches above threshold\n        matches = torch.nonzero(similarity > threshold, as_tuple=True)\n\n        return list(zip(matches[0].cpu().numpy(), matches[1].cpu().numpy()))\n'})}),"\n",(0,i.jsx)(n.h3,{id:"semantic-segmentation-stack",children:"Semantic Segmentation Stack"}),"\n",(0,i.jsx)(n.p,{children:"Semantic segmentation provides pixel-level understanding of the scene:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class SemanticSegmentationStack:\n    def __init__(self, num_classes=20):\n        # Use DeepLabV3 with ResNet backbone\n        self.model = torch.hub.load(\n            \'pytorch/vision:v0.10.0\',\n            \'deeplabv3_resnet50\',\n            pretrained=True\n        )\n        self.model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)\n\n        # Color palette for visualization\n        self.color_palette = self.generate_color_palette(num_classes)\n\n    def generate_color_palette(self, num_classes):\n        """\n        Generate distinct colors for each class\n        """\n        import matplotlib.cm as cm\n        colors = cm.get_cmap(\'tab20\', num_classes)\n        return [(int(r*255), int(g*255), int(b*255))\n                for r, g, b, _ in [colors(i) for i in range(num_classes)]]\n\n    def segment_image(self, image):\n        """\n        Perform semantic segmentation on input image\n        """\n        # Preprocess image\n        input_tensor = torch.from_numpy(image).float().permute(2, 0, 1).unsqueeze(0)\n        input_tensor = input_tensor / 255.0\n\n        # Perform segmentation\n        with torch.no_grad():\n            output = self.model(input_tensor)[\'out\']\n            predictions = torch.argmax(output, dim=1)\n\n        return predictions.squeeze().cpu().numpy()\n\n    def visualize_segmentation(self, image, segmentation_mask):\n        """\n        Create a colorized visualization of segmentation\n        """\n        height, width = segmentation_mask.shape\n        colorized = np.zeros((height, width, 3), dtype=np.uint8)\n\n        for class_id in np.unique(segmentation_mask):\n            mask = segmentation_mask == class_id\n            if class_id < len(self.color_palette):\n                colorized[mask] = self.color_palette[class_id]\n\n        # Blend with original image\n        blended = cv2.addWeighted(image, 0.7, colorized, 0.3, 0)\n\n        return blended\n'})}),"\n",(0,i.jsx)(n.h2,{id:"3d-perception-stack",children:"3D Perception Stack"}),"\n",(0,i.jsx)(n.h3,{id:"point-cloud-processing",children:"Point Cloud Processing"}),"\n",(0,i.jsx)(n.p,{children:"3D perception stacks handle point cloud data from LIDAR and depth sensors:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class PointCloudPerception:\n    def __init__(self):\n        # For point cloud operations, we\'ll use numpy\n        # In practice, you might use Open3D or PCL\n        pass\n\n    def remove_ground_plane(self, points, threshold=0.1):\n        """\n        Remove ground plane using RANSAC algorithm\n        """\n        import random\n\n        best_inliers = []\n        best_plane = None\n\n        for _ in range(100):  # Number of iterations\n            # Randomly sample 3 points\n            sample_indices = random.sample(range(len(points)), 3)\n            sample_points = points[sample_indices]\n\n            # Fit plane to sample points\n            plane = self.fit_plane_to_points(sample_points)\n\n            # Count inliers\n            inliers = []\n            for i, point in enumerate(points):\n                distance = self.point_to_plane_distance(point, plane)\n                if abs(distance) < threshold:\n                    inliers.append(i)\n\n            if len(inliers) > len(best_inliers):\n                best_inliers = inliers\n                best_plane = plane\n\n        # Remove ground plane points\n        non_ground_points = np.delete(points, best_inliers, axis=0)\n\n        return non_ground_points, best_plane\n\n    def fit_plane_to_points(self, points):\n        """\n        Fit a plane to 3 points\n        Plane equation: ax + by + cz + d = 0\n        """\n        p1, p2, p3 = points[:3]\n\n        # Calculate plane normal\n        v1 = p2 - p1\n        v2 = p3 - p1\n        normal = np.cross(v1, v2)\n        normal = normal / np.linalg.norm(normal)\n\n        # Calculate d parameter\n        d = -np.dot(normal, p1)\n\n        return np.append(normal, d)\n\n    def point_to_plane_distance(self, point, plane):\n        """\n        Calculate distance from point to plane\n        """\n        a, b, c, d = plane\n        x, y, z = point\n        return abs(a*x + b*y + c*z + d) / np.sqrt(a**2 + b**2 + c**2)\n\n    def cluster_objects(self, points, eps=0.5, min_points=10):\n        """\n        Cluster points into objects using DBSCAN\n        """\n        from sklearn.cluster import DBSCAN\n\n        clustering = DBSCAN(eps=eps, min_samples=min_points).fit(points)\n        labels = clustering.labels_\n\n        # Group points by cluster\n        clusters = {}\n        for i, label in enumerate(labels):\n            if label not in clusters:\n                clusters[label] = []\n            clusters[label].append(points[i])\n\n        # Convert to numpy arrays\n        for label in clusters:\n            clusters[label] = np.array(clusters[label])\n\n        return clusters\n'})}),"\n",(0,i.jsx)(n.h2,{id:"tactile-perception-stack",children:"Tactile Perception Stack"}),"\n",(0,i.jsx)(n.h3,{id:"force-and-tactile-sensing",children:"Force and Tactile Sensing"}),"\n",(0,i.jsx)(n.p,{children:"For manipulation tasks, tactile perception is crucial:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class TactilePerception:\n    def __init__(self):\n        self.force_threshold = 0.1  # Newtons\n        self.slip_threshold = 0.05  # Change in force over time\n\n    def detect_contact(self, force_data, threshold=None):\n        """\n        Detect contact based on force measurements\n        """\n        if threshold is None:\n            threshold = self.force_threshold\n\n        # Calculate magnitude of force vector\n        force_magnitude = np.linalg.norm(force_data, axis=-1)\n\n        # Detect contact events\n        contact_mask = force_magnitude > threshold\n\n        return contact_mask\n\n    def detect_slip(self, force_sequence, time_window=0.1):\n        """\n        Detect slip based on force changes over time\n        """\n        if len(force_sequence) < 2:\n            return False\n\n        # Calculate force change rate\n        force_changes = np.diff(force_sequence, axis=0)\n        change_rates = np.linalg.norm(force_changes, axis=1)\n\n        # Check for rapid changes indicating slip\n        slip_detected = np.any(change_rates > self.slip_threshold)\n\n        return slip_detected\n\n    def estimate_object_properties(self, tactile_data, contact_points):\n        """\n        Estimate object properties from tactile sensing\n        """\n        if len(contact_points) == 0:\n            return None\n\n        # Estimate object pose from contact points\n        centroid = np.mean(contact_points, axis=0)\n\n        # Estimate object size from contact area\n        if len(contact_points) > 1:\n            covariance = np.cov(contact_points.T)\n            eigenvalues = np.linalg.eigvals(covariance)\n            size_estimate = np.sqrt(eigenvalues)\n        else:\n            size_estimate = np.array([0.01, 0.01, 0.01])  # Default small object\n\n        # Estimate friction from force data\n        # This is a simplified model\n        normal_forces = tactile_data[:, 2]  # Assuming z-axis is normal\n        friction_forces = np.linalg.norm(tactile_data[:, :2], axis=1)  # x,y components\n\n        if len(normal_forces) > 0 and np.mean(normal_forces) > 0:\n            friction_coefficient = np.mean(friction_forces) / np.mean(normal_forces)\n        else:\n            friction_coefficient = 0.5  # Default value\n\n        return {\n            \'position\': centroid,\n            \'size\': size_estimate,\n            \'friction_coefficient\': friction_coefficient,\n            \'contact_points\': contact_points\n        }\n'})}),"\n",(0,i.jsx)(n.h2,{id:"perception-quality-and-uncertainty",children:"Perception Quality and Uncertainty"}),"\n",(0,i.jsx)(n.h3,{id:"uncertainty-quantification",children:"Uncertainty Quantification"}),"\n",(0,i.jsx)(n.p,{children:"Modern perception stacks must quantify uncertainty in their estimates:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class UncertaintyQuantification:\n    def __init__(self):\n        pass\n\n    def estimate_detection_uncertainty(self, detection_output):\n        """\n        Estimate uncertainty for object detections\n        """\n        # For classification, uncertainty can be estimated from softmax outputs\n        # For bounding boxes, uncertainty can come from ensemble methods or dropout\n        batch_size, num_detections, num_classes = detection_output.shape\n\n        # Calculate entropy as uncertainty measure\n        probabilities = torch.softmax(detection_output, dim=-1)\n        entropy = -torch.sum(probabilities * torch.log(probabilities + 1e-8), dim=-1)\n\n        # Convert to confidence (inverse of uncertainty)\n        confidence = 1.0 - entropy / np.log(num_classes)\n\n        return confidence\n\n    def ensemble_uncertainty(self, model_ensemble, input_data, num_samples=10):\n        """\n        Estimate uncertainty using model ensemble\n        """\n        predictions = []\n\n        for model in model_ensemble:\n            with torch.no_grad():\n                pred = model(input_data)\n                predictions.append(pred)\n\n        # Calculate mean and variance across ensemble\n        pred_stack = torch.stack(predictions)\n        mean_pred = torch.mean(pred_stack, dim=0)\n        var_pred = torch.var(pred_stack, dim=0)\n\n        return mean_pred, var_pred\n\n    def bayesian_uncertainty(self, model, input_data, num_forward_passes=10):\n        """\n        Estimate uncertainty using Monte Carlo dropout\n        """\n        model.train()  # Enable dropout\n        predictions = []\n\n        for _ in range(num_forward_passes):\n            pred = model(input_data)\n            predictions.append(pred)\n\n        pred_stack = torch.stack(predictions)\n        mean_pred = torch.mean(pred_stack, dim=0)\n        var_pred = torch.var(pred_stack, dim=0)\n\n        return mean_pred, var_pred\n'})}),"\n",(0,i.jsx)(n.h2,{id:"real-time-performance-optimization",children:"Real-time Performance Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"efficient-processing-pipelines",children:"Efficient Processing Pipelines"}),"\n",(0,i.jsx)(n.p,{children:"Real-time perception requires careful optimization:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import multiprocessing as mp\nfrom queue import Queue\nimport threading\n\nclass RealTimePerceptionPipeline:\n    def __init__(self, config):\n        self.config = config\n        self.input_queue = Queue(maxsize=2)\n        self.output_queue = Queue(maxsize=2)\n\n        # Initialize processing modules\n        self.preprocessor = self.initialize_preprocessor()\n        self.perception_module = self.initialize_perception_module()\n        self.postprocessor = self.initialize_postprocessor()\n\n        # Processing thread\n        self.processing_thread = threading.Thread(target=self._process_loop)\n        self.processing_thread.daemon = True\n        self.running = True\n        self.processing_thread.start()\n\n    def _process_loop(self):\n        """\n        Main processing loop for real-time perception\n        """\n        while self.running:\n            try:\n                # Get input data\n                input_data = self.input_queue.get(timeout=0.01)\n\n                # Preprocess\n                processed_data = self.preprocessor(input_data)\n\n                # Run perception\n                start_time = time.time()\n                perception_output = self.perception_module(processed_data)\n                processing_time = time.time() - start_time\n\n                # Postprocess\n                final_output = self.postprocessor(perception_output)\n\n                # Add to output queue if there\'s space\n                if self.output_queue.qsize() < 2:  # Prevent output queue overflow\n                    self.output_queue.put({\n                        \'result\': final_output,\n                        \'timestamp\': time.time(),\n                        \'processing_time\': processing_time\n                    })\n\n            except Exception as e:\n                continue  # Continue processing even if one frame fails\n\n    def submit_frame(self, frame_data):\n        """\n        Submit a frame for processing\n        """\n        try:\n            if not self.input_queue.full():\n                self.input_queue.put(frame_data)\n                return True\n            else:\n                return False  # Drop frame if input queue is full\n        except Exception:\n            return False\n\n    def get_result(self, block=False):\n        """\n        Get the latest perception result\n        """\n        try:\n            if block:\n                return self.output_queue.get()\n            else:\n                if not self.output_queue.empty():\n                    # Get the latest result, discarding older ones\n                    latest_result = None\n                    while not self.output_queue.empty():\n                        latest_result = self.output_queue.get()\n                    return latest_result\n                else:\n                    return None\n        except Exception:\n            return None\n'})}),"\n",(0,i.jsx)(n.p,{children:"Perception stacks form the sensory foundation of Physical AI systems, enabling robots to understand and interact with their environment. By combining multiple sensing modalities and processing them through carefully designed pipelines, these systems can achieve robust and reliable perception even in challenging real-world conditions."})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);