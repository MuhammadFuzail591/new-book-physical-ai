"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[7101],{8453:(e,n,a)=>{a.d(n,{R:()=>s,x:()=>o});var t=a(6540);const i={},r=t.createContext(i);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(r.Provider,{value:n},e.children)}},9377:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>d});var t=a(4848),i=a(8453);const r={title:"Synthetic Data Generation for AI Perception"},s="Synthetic Data Generation for AI Perception",o={id:"physical-ai/perception-pipelines/synthetic-data",title:"Synthetic Data Generation for AI Perception",description:"Introduction to Synthetic Data in Robotics",source:"@site/docs/physical-ai/perception-pipelines/synthetic-data.mdx",sourceDirName:"physical-ai/perception-pipelines",slug:"/physical-ai/perception-pipelines/synthetic-data",permalink:"/physical-ai-textbook/physical-ai/physical-ai/perception-pipelines/synthetic-data",draft:!1,unlisted:!1,editUrl:"https://github.com/your-username/physical-ai-textbook/tree/main/docs/physical-ai/perception-pipelines/synthetic-data.mdx",tags:[],version:"current",frontMatter:{title:"Synthetic Data Generation for AI Perception"},sidebar:"tutorialSidebar",previous:{title:"Chapter 10 - AI Perception, Synthetic Data & Manipulation Pipelines",permalink:"/physical-ai-textbook/physical-ai/physical-ai/perception-pipelines/"},next:{title:"Perception Stacks - Multi-Modal Sensor Processing",permalink:"/physical-ai-textbook/physical-ai/physical-ai/perception-pipelines/perception-stacks"}},l={},d=[{value:"Introduction to Synthetic Data in Robotics",id:"introduction-to-synthetic-data-in-robotics",level:2},{value:"Physics-Based Rendering for Synthetic Data",id:"physics-based-rendering-for-synthetic-data",level:2},{value:"Photorealistic Scene Generation",id:"photorealistic-scene-generation",level:3},{value:"Material and Texture Synthesis",id:"material-and-texture-synthesis",level:3},{value:"Domain Randomization Techniques",id:"domain-randomization-techniques",level:2},{value:"Synthetic Data Pipeline for Perception Tasks",id:"synthetic-data-pipeline-for-perception-tasks",level:2},{value:"Object Detection Data Generation",id:"object-detection-data-generation",level:3},{value:"Semantic Segmentation Data Generation",id:"semantic-segmentation-data-generation",level:3},{value:"Sim-to-Real Transfer Techniques",id:"sim-to-real-transfer-techniques",level:2},{value:"Domain Adaptation Networks",id:"domain-adaptation-networks",level:3},{value:"Quality Assessment and Validation",id:"quality-assessment-and-validation",level:2},{value:"Synthetic Data Quality Metrics",id:"synthetic-data-quality-metrics",level:3},{value:"Best Practices for Synthetic Data Generation",id:"best-practices-for-synthetic-data-generation",level:2},{value:"1. Progressive Domain Randomization",id:"1-progressive-domain-randomization",level:3},{value:"2. Validation Against Real Data",id:"2-validation-against-real-data",level:3}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"synthetic-data-generation-for-ai-perception",children:"Synthetic Data Generation for AI Perception"}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-synthetic-data-in-robotics",children:"Introduction to Synthetic Data in Robotics"}),"\n",(0,t.jsx)(n.p,{children:"Synthetic data generation has become a cornerstone of modern AI perception systems, particularly in robotics where real-world data collection can be expensive, time-consuming, and sometimes dangerous. Synthetic data refers to artificially created datasets that mimic real-world observations, enabling the training of machine learning models without the need for extensive physical data collection."}),"\n",(0,t.jsx)(n.p,{children:"In robotics and Physical AI applications, synthetic data serves several critical purposes:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Data Augmentation"}),": Expanding limited real-world datasets"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Edge Case Generation"}),": Creating rare or dangerous scenarios safely"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Domain Randomization"}),": Improving model generalization across environments"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor Simulation"}),": Modeling different sensor configurations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Label Generation"}),": Providing perfect ground truth annotations"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"physics-based-rendering-for-synthetic-data",children:"Physics-Based Rendering for Synthetic Data"}),"\n",(0,t.jsx)(n.h3,{id:"photorealistic-scene-generation",children:"Photorealistic Scene Generation"}),"\n",(0,t.jsx)(n.p,{children:"Physics-based rendering engines create synthetic data by simulating the physical processes of light transport, material properties, and sensor characteristics. This approach ensures that synthetic data closely matches real-world physics:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport torch\nimport torch.nn.functional as F\n\nclass PhysicsBasedRenderer:\n    def __init__(self, width=640, height=480):\n        self.width = width\n        self.height = height\n        self.camera_intrinsics = np.array([\n            [500.0, 0.0, width/2],\n            [0.0, 500.0, height/2],\n            [0.0, 0.0, 1.0]\n        ])\n\n    def render_scene(self, objects, lighting, camera_pose):\n        """\n        Render a scene with physically accurate lighting and materials\n        """\n        # Initialize buffers\n        depth_buffer = np.full((self.height, self.width), np.inf)\n        color_buffer = np.zeros((self.height, self.width, 3))\n        normal_buffer = np.zeros((self.height, self.width, 3))\n\n        # For each pixel, calculate ray intersection with scene\n        for y in range(self.height):\n            for x in range(self.width):\n                # Calculate ray direction in world space\n                ray_dir = self.pixel_to_ray(x, y, camera_pose)\n\n                # Find closest intersection\n                closest_obj, closest_dist = self.find_closest_intersection(\n                    camera_pose[:3, 3], ray_dir, objects\n                )\n\n                if closest_obj is not None:\n                    # Calculate surface properties\n                    surface_point = camera_pose[:3, 3] + closest_dist * ray_dir\n                    surface_normal = closest_obj.get_normal_at(surface_point)\n\n                    # Calculate lighting using Phong model\n                    color = self.calculate_phong_lighting(\n                        surface_point, surface_normal, ray_dir, lighting, closest_obj\n                    )\n\n                    # Update buffers\n                    depth_buffer[y, x] = closest_dist\n                    color_buffer[y, x] = color\n                    normal_buffer[y, x] = surface_normal\n\n        return {\n            \'rgb\': color_buffer,\n            \'depth\': depth_buffer,\n            \'normals\': normal_buffer\n        }\n\n    def pixel_to_ray(self, x, y, camera_pose):\n        """\n        Convert pixel coordinates to world-space ray direction\n        """\n        # Convert to normalized device coordinates\n        ndc_x = (x - self.camera_intrinsics[0, 2]) / self.camera_intrinsics[0, 0]\n        ndc_y = (y - self.camera_intrinsics[1, 2]) / self.camera_intrinsics[1, 1]\n\n        # Create ray in camera space\n        ray_cam = np.array([ndc_x, ndc_y, 1.0])\n\n        # Transform to world space\n        ray_world = camera_pose[:3, :3] @ ray_cam\n        return ray_world / np.linalg.norm(ray_world)\n\n    def calculate_phong_lighting(self, point, normal, view_dir, lighting, obj):\n        """\n        Calculate Phong lighting model for surface point\n        """\n        # Ambient component\n        ambient = lighting.ambient_intensity * obj.material.ambient\n\n        # Diffuse and specular components for each light\n        diffuse = np.zeros(3)\n        specular = np.zeros(3)\n\n        for light in lighting.lights:\n            light_dir = light.position - point\n            light_dir = light_dir / np.linalg.norm(light_dir)\n\n            # Diffuse reflection\n            diff = max(0.0, np.dot(normal, light_dir))\n            diffuse += light.color * obj.material.diffuse * diff * light.intensity\n\n            # Specular reflection\n            reflect_dir = 2 * np.dot(normal, light_dir) * normal - light_dir\n            spec = max(0.0, np.dot(view_dir, reflect_dir)) ** obj.material.shininess\n            specular += light.color * obj.material.specular * spec * light.intensity\n\n        return ambient + diffuse + specular\n'})}),"\n",(0,t.jsx)(n.h3,{id:"material-and-texture-synthesis",children:"Material and Texture Synthesis"}),"\n",(0,t.jsx)(n.p,{children:"Creating realistic materials is crucial for synthetic data quality:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class MaterialSynthesizer:\n    def __init__(self):\n        self.texture_generator = self.initialize_texture_generator()\n        self.material_properties = {\n            \'metallic\': self.generate_metallic_map,\n            \'roughness\': self.generate_roughness_map,\n            \'normal\': self.generate_normal_map,\n            \'albedo\': self.generate_albedo_map\n        }\n\n    def generate_procedural_texture(self, size, material_type):\n        """\n        Generate procedural textures for different material types\n        """\n        if material_type == \'wood\':\n            return self.generate_wood_texture(size)\n        elif material_type == \'metal\':\n            return self.generate_metal_texture(size)\n        elif material_type == \'fabric\':\n            return self.generate_fabric_texture(size)\n        else:\n            return self.generate_generic_texture(size)\n\n    def generate_wood_texture(self, size):\n        """\n        Generate wood-like texture with grain patterns\n        """\n        height, width = size\n        texture = np.zeros((height, width, 3))\n\n        # Create base wood color\n        base_color = np.array([0.6, 0.4, 0.2])  # Brown wood color\n\n        # Add grain patterns using noise\n        x_coords = np.linspace(0, 4*np.pi, width)\n        y_coords = np.linspace(0, 4*np.pi, height)\n        X, Y = np.meshgrid(x_coords, y_coords)\n\n        # Create wood grain pattern\n        grain_pattern = 0.1 * np.sin(20 * X) * np.exp(-0.5 * Y**2)\n\n        # Add radial patterns for wood rings\n        center_x, center_y = width/2, height/2\n        radial_dist = np.sqrt((X - center_x)**2 + (Y - center_y)**2)\n        ring_pattern = 0.05 * np.sin(5 * radial_dist)\n\n        # Combine patterns\n        variation = grain_pattern + ring_pattern\n        texture = base_color + variation[..., np.newaxis] * 0.3\n\n        # Clamp to valid range\n        texture = np.clip(texture, 0, 1)\n\n        return texture\n\n    def generate_metal_texture(self, size):\n        """\n        Generate metal-like texture with surface variations\n        """\n        height, width = size\n        texture = np.zeros((height, width, 3))\n\n        # Base metal color (aluminum-like)\n        base_color = np.array([0.9, 0.9, 0.9])\n\n        # Add surface imperfections\n        noise = np.random.normal(0, 0.1, (height, width, 3))\n        scratches = self.generate_scratches_pattern(size)\n\n        texture = base_color + noise + scratches * 0.1\n        texture = np.clip(texture, 0, 1)\n\n        return texture\n\n    def generate_scratches_pattern(self, size):\n        """\n        Generate scratch patterns for metal surfaces\n        """\n        height, width = size\n        pattern = np.zeros((height, width))\n\n        # Add random scratches\n        for _ in range(20):\n            start_x = np.random.randint(0, width)\n            start_y = np.random.randint(0, height)\n            length = np.random.randint(10, 50)\n            angle = np.random.uniform(0, 2*np.pi)\n\n            x_coords = np.linspace(start_x, start_x + length*np.cos(angle), length)\n            y_coords = np.linspace(start_y, start_y + length*np.sin(angle), length)\n\n            x_coords = np.clip(x_coords, 0, width-1).astype(int)\n            y_coords = np.clip(y_coords, 0, height-1).astype(int)\n\n            for x, y in zip(x_coords, y_coords):\n                if 0 <= x < width and 0 <= y < height:\n                    pattern[y, x] = 1.0\n\n        # Apply blur to make scratches more realistic\n        from scipy import ndimage\n        pattern = ndimage.gaussian_filter(pattern, sigma=0.5)\n\n        return pattern\n'})}),"\n",(0,t.jsx)(n.h2,{id:"domain-randomization-techniques",children:"Domain Randomization Techniques"}),"\n",(0,t.jsx)(n.p,{children:"Domain randomization is a key technique for improving the transferability of models trained on synthetic data to real-world scenarios:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import random\n\nclass DomainRandomizer:\n    def __init__(self):\n        self.randomization_ranges = {\n            'lighting': {\n                'intensity': (0.5, 2.0),\n                'color_temperature': (3000, 8000),  # Kelvin\n                'position_variance': 2.0\n            },\n            'materials': {\n                'albedo_variance': 0.3,\n                'roughness_range': (0.1, 0.9),\n                'metallic_range': (0.0, 1.0)\n            },\n            'camera': {\n                'exposure_range': (0.1, 2.0),\n                'white_balance_range': (0.8, 1.2),\n                'noise_std_range': (0.001, 0.01)\n            }\n        }\n\n    def randomize_lighting(self, base_lighting):\n        \"\"\"\n        Randomize lighting conditions\n        \"\"\"\n        randomized_lighting = base_lighting.copy()\n\n        # Randomize intensity\n        intensity_factor = random.uniform(\n            self.randomization_ranges['lighting']['intensity'][0],\n            self.randomization_ranges['lighting']['intensity'][1]\n        )\n        randomized_lighting['intensity'] *= intensity_factor\n\n        # Randomize color temperature\n        color_temp = random.uniform(\n            self.randomization_ranges['lighting']['color_temperature'][0],\n            self.randomization_ranges['lighting']['color_temperature'][1]\n        )\n        randomized_lighting['color'] = self.color_temperature_to_rgb(color_temp)\n\n        # Randomize position\n        pos_variance = self.randomization_ranges['lighting']['position_variance']\n        randomized_lighting['position'] += np.random.uniform(-pos_variance, pos_variance, 3)\n\n        return randomized_lighting\n\n    def randomize_materials(self, materials):\n        \"\"\"\n        Randomize material properties\n        \"\"\"\n        randomized_materials = []\n\n        for material in materials:\n            new_material = material.copy()\n\n            # Randomize albedo\n            albedo_variance = self.randomization_ranges['materials']['albedo_variance']\n            new_material['albedo'] += np.random.uniform(-albedo_variance, albedo_variance, 3)\n            new_material['albedo'] = np.clip(new_material['albedo'], 0, 1)\n\n            # Randomize roughness\n            roughness_range = self.randomization_ranges['materials']['roughness_range']\n            new_material['roughness'] = random.uniform(roughness_range[0], roughness_range[1])\n\n            # Randomize metallic\n            metallic_range = self.randomization_ranges['materials']['metallic_range']\n            new_material['metallic'] = random.uniform(metallic_range[0], metallic_range[1])\n\n            randomized_materials.append(new_material)\n\n        return randomized_materials\n\n    def randomize_camera(self, base_camera_params):\n        \"\"\"\n        Randomize camera parameters to simulate different sensors\n        \"\"\"\n        randomized_params = base_camera_params.copy()\n\n        # Randomize exposure\n        exposure_range = self.randomization_ranges['camera']['exposure_range']\n        exposure_factor = random.uniform(exposure_range[0], exposure_range[1])\n        randomized_params['exposure'] *= exposure_factor\n\n        # Randomize white balance\n        wb_range = self.randomization_ranges['camera']['white_balance_range']\n        randomized_params['white_balance'] *= random.uniform(wb_range[0], wb_range[1])\n\n        # Add noise\n        noise_std_range = self.randomization_ranges['camera']['noise_std_range']\n        noise_std = random.uniform(noise_std_range[0], noise_std_range[1])\n        randomized_params['noise_std'] = noise_std\n\n        return randomized_params\n\n    def color_temperature_to_rgb(self, color_temp):\n        \"\"\"\n        Convert color temperature in Kelvin to RGB values\n        \"\"\"\n        temp = color_temp / 100.0\n\n        if temp <= 66:\n            red = 255\n            green = temp\n            green = 99.4708025861 * np.log(green) - 161.1195681661\n        else:\n            red = temp - 60\n            red = 329.698727446 * (red ** -0.1332047592)\n            green = temp - 60\n            green = 288.1221695283 * (green ** -0.0755148492)\n\n        if temp >= 66:\n            blue = 255\n        elif temp <= 19:\n            blue = 0\n        else:\n            blue = temp - 10\n            blue = 138.5177312231 * np.log(blue) - 305.0447927307\n\n        return np.clip([red, green, blue] / 255.0, 0, 1)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"synthetic-data-pipeline-for-perception-tasks",children:"Synthetic Data Pipeline for Perception Tasks"}),"\n",(0,t.jsx)(n.h3,{id:"object-detection-data-generation",children:"Object Detection Data Generation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class SyntheticObjectDetectionGenerator:\n    def __init__(self, object_library, scene_generator):\n        self.object_library = object_library\n        self.scene_generator = scene_generator\n        self.domain_randomizer = DomainRandomizer()\n\n    def generate_detection_dataset(self, num_samples, output_dir):\n        \"\"\"\n        Generate synthetic dataset for object detection\n        \"\"\"\n        import os\n        import json\n        from PIL import Image\n\n        os.makedirs(output_dir, exist_ok=True)\n        os.makedirs(os.path.join(output_dir, 'images'), exist_ok=True)\n        os.makedirs(os.path.join(output_dir, 'labels'), exist_ok=True)\n\n        annotations = []\n\n        for i in range(num_samples):\n            # Randomize scene parameters\n            randomized_params = self.domain_randomizer.randomize_lighting({})\n            randomized_materials = self.domain_randomizer.randomize_materials([])\n            randomized_camera = self.domain_randomizer.randomize_camera({})\n\n            # Generate random scene\n            scene = self.scene_generator.generate_random_scene(\n                objects=self.select_random_objects(),\n                lighting=randomized_params,\n                camera=randomized_camera\n            )\n\n            # Render the scene\n            render_result = self.scene_generator.render_scene(scene)\n\n            # Extract bounding boxes and annotations\n            bboxes = self.extract_bounding_boxes(scene['objects'], scene['camera'])\n\n            # Save image\n            img_path = os.path.join(output_dir, 'images', f'{i:06d}.png')\n            Image.fromarray((render_result['rgb'] * 255).astype(np.uint8)).save(img_path)\n\n            # Save annotations\n            annotation = {\n                'image_id': i,\n                'image_path': img_path,\n                'width': render_result['rgb'].shape[1],\n                'height': render_result['rgb'].shape[0],\n                'objects': []\n            }\n\n            for j, (bbox, obj_class) in enumerate(zip(bboxes, scene['object_classes'])):\n                annotation['objects'].append({\n                    'id': j,\n                    'bbox': bbox.tolist(),  # [x, y, width, height]\n                    'class': obj_class,\n                    'occluded': self.is_occluded(bbox, bboxes)\n                })\n\n            annotations.append(annotation)\n\n            # Save individual annotation file\n            annot_path = os.path.join(output_dir, 'labels', f'{i:06d}.json')\n            with open(annot_path, 'w') as f:\n                json.dump(annotation, f)\n\n        # Save overall dataset annotation\n        with open(os.path.join(output_dir, 'annotations.json'), 'w') as f:\n            json.dump(annotations, f)\n\n        return annotations\n\n    def select_random_objects(self):\n        \"\"\"\n        Select random objects from the library\n        \"\"\"\n        num_objects = np.random.randint(1, 5)  # 1-4 objects per scene\n        selected_objects = []\n\n        for _ in range(num_objects):\n            obj = random.choice(self.object_library)\n            obj_instance = {\n                'model': obj['model'],\n                'class': obj['class'],\n                'scale': random.uniform(0.5, 2.0),\n                'position': np.random.uniform(-2, 2, 3),\n                'rotation': np.random.uniform(0, 2*np.pi, 3)\n            }\n            selected_objects.append(obj_instance)\n\n        return selected_objects\n\n    def extract_bounding_boxes(self, objects, camera):\n        \"\"\"\n        Extract 2D bounding boxes from 3D objects in camera view\n        \"\"\"\n        bboxes = []\n\n        for obj in objects:\n            # Project 3D bounding box to 2D\n            corners_3d = self.get_object_bounding_box_3d(obj)\n            corners_2d = self.project_3d_to_2d(corners_3d, camera)\n\n            # Calculate 2D bounding box\n            min_x = np.min(corners_2d[:, 0])\n            max_x = np.max(corners_2d[:, 0])\n            min_y = np.min(corners_2d[:, 1])\n            max_y = np.max(corners_2d[:, 1])\n\n            bbox = np.array([min_x, min_y, max_x - min_x, max_y - min_y])\n            bboxes.append(bbox)\n\n        return np.array(bboxes)\n\n    def is_occluded(self, bbox, all_bboxes, threshold=0.5):\n        \"\"\"\n        Check if a bounding box is significantly occluded by others\n        \"\"\"\n        bbox_area = bbox[2] * bbox[3]\n        if bbox_area == 0:\n            return False\n\n        total_occlusion = 0\n        for other_bbox in all_bboxes:\n            if not np.array_equal(bbox, other_bbox):\n                # Calculate intersection\n                x1 = max(bbox[0], other_bbox[0])\n                y1 = max(bbox[1], other_bbox[1])\n                x2 = min(bbox[0] + bbox[2], other_bbox[0] + other_bbox[2])\n                y2 = min(bbox[1] + bbox[3], other_bbox[1] + other_bbox[3])\n\n                if x2 > x1 and y2 > y1:\n                    intersection_area = (x2 - x1) * (y2 - y1)\n                    total_occlusion += intersection_area\n\n        occlusion_ratio = total_occlusion / bbox_area\n        return occlusion_ratio > threshold\n"})}),"\n",(0,t.jsx)(n.h3,{id:"semantic-segmentation-data-generation",children:"Semantic Segmentation Data Generation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class SyntheticSegmentationGenerator:\n    def __init__(self, object_library, scene_generator):\n        self.object_library = object_library\n        self.scene_generator = scene_generator\n        self.domain_randomizer = DomainRandomizer()\n\n        # Define class mapping\n        self.class_mapping = {\n            'background': 0,\n            'person': 1,\n            'chair': 2,\n            'table': 3,\n            'monitor': 4,\n            'keyboard': 5,\n            'mouse': 6,\n            'cup': 7,\n            'book': 8,\n            'plant': 9\n        }\n\n    def generate_segmentation_dataset(self, num_samples, output_dir):\n        \"\"\"\n        Generate synthetic dataset for semantic segmentation\n        \"\"\"\n        import os\n        from PIL import Image\n\n        os.makedirs(output_dir, exist_ok=True)\n        os.makedirs(os.path.join(output_dir, 'images'), exist_ok=True)\n        os.makedirs(os.path.join(output_dir, 'masks'), exist_ok=True)\n\n        for i in range(num_samples):\n            # Generate scene with domain randomization\n            randomized_params = self.domain_randomizer.randomize_lighting({})\n            randomized_materials = self.domain_randomizer.randomize_materials([])\n            randomized_camera = self.domain_randomizer.randomize_camera({})\n\n            scene = self.scene_generator.generate_random_scene(\n                objects=self.select_random_objects(),\n                lighting=randomized_params,\n                camera=randomized_camera\n            )\n\n            # Render RGB and segmentation\n            render_result = self.scene_generator.render_scene(scene)\n            segmentation_mask = self.render_segmentation_mask(scene)\n\n            # Save RGB image\n            img_path = os.path.join(output_dir, 'images', f'{i:06d}.png')\n            Image.fromarray((render_result['rgb'] * 255).astype(np.uint8)).save(img_path)\n\n            # Save segmentation mask\n            mask_path = os.path.join(output_dir, 'masks', f'{i:06d}.png')\n            Image.fromarray(segmentation_mask.astype(np.uint8)).save(mask_path)\n\n        return f\"Generated {num_samples} segmentation samples in {output_dir}\"\n\n    def render_segmentation_mask(self, scene):\n        \"\"\"\n        Render semantic segmentation mask for the scene\n        \"\"\"\n        height, width = scene['camera']['resolution']\n        mask = np.zeros((height, width), dtype=np.uint8)\n\n        # Render each object with its class ID\n        for obj in scene['objects']:\n            class_id = self.class_mapping.get(obj['class'], 0)\n\n            # Project object mesh to get pixel mask\n            obj_mask = self.project_object_to_image(obj, scene['camera'])\n            mask[obj_mask > 0] = class_id\n\n        return mask\n\n    def project_object_to_image(self, obj, camera):\n        \"\"\"\n        Project object mesh to 2D image to create pixel mask\n        \"\"\"\n        # This is a simplified version - in practice, you'd use a proper\n        # 3D projection with depth testing\n        height, width = camera['resolution']\n\n        # Get object bounding box in 2D\n        corners_3d = self.get_object_bounding_box_3d(obj)\n        corners_2d = self.project_3d_to_2d(corners_3d, camera)\n\n        # Create mask for bounding box region\n        min_x = max(0, int(np.min(corners_2d[:, 0])))\n        max_x = min(width, int(np.max(corners_2d[:, 0])) + 1)\n        min_y = max(0, int(np.min(corners_2d[:, 1])))\n        max_y = min(height, int(np.max(corners_2d[:, 1])) + 1)\n\n        obj_mask = np.zeros((height, width), dtype=np.uint8)\n        obj_mask[min_y:max_y, min_x:max_x] = 1\n\n        return obj_mask\n"})}),"\n",(0,t.jsx)(n.h2,{id:"sim-to-real-transfer-techniques",children:"Sim-to-Real Transfer Techniques"}),"\n",(0,t.jsx)(n.h3,{id:"domain-adaptation-networks",children:"Domain Adaptation Networks"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DomainAdaptationNetwork(nn.Module):\n    def __init__(self, num_classes=10):\n        super(DomainAdaptationNetwork, self).__init__()\n\n        # Feature extractor (shared between domains)\n        self.feature_extractor = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((8, 8))\n        )\n\n        # Label prediction head (for semantic task)\n        self.label_predictor = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(256 * 8 * 8, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)\n        )\n\n        # Domain prediction head (for domain adaptation)\n        self.domain_predictor = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(256 * 8 * 8, 256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, 2)  # 2 domains: synthetic and real\n        )\n\n        # Gradient reversal layer\n        self.grad_reverse = GradientReversalLayer()\n\n    def forward(self, x, alpha=1.0):\n        features = self.feature_extractor(x)\n\n        # Label prediction (classification)\n        label_pred = self.label_predictor(features)\n\n        # Domain prediction with gradient reversal\n        reversed_features = self.grad_reverse(features, alpha)\n        domain_pred = self.domain_predictor(reversed_features)\n\n        return label_pred, domain_pred\n\nclass GradientReversalLayer(torch.autograd.Function):\n    """\n    Gradient Reversal Layer for Domain Adversarial Training\n    """\n    @staticmethod\n    def forward(ctx, input, alpha):\n        ctx.alpha = alpha\n        return input\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        output = grad_output.neg() * ctx.alpha\n        return output, None\n\ndef train_domain_adaptation(model, synthetic_loader, real_loader, num_epochs=10):\n    """\n    Train model with domain adaptation using adversarial loss\n    """\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    label_criterion = nn.CrossEntropyLoss()\n    domain_criterion = nn.CrossEntropyLoss()\n\n    for epoch in range(num_epochs):\n        for (synth_batch, real_batch) in zip(synthetic_loader, real_loader):\n            optimizer.zero_grad()\n\n            # Combine synthetic and real data\n            combined_data = torch.cat([synth_batch[0], real_batch[0]], dim=0)\n            combined_domains = torch.cat([\n                torch.zeros(synth_batch[0].size(0)),  # Synthetic domain: 0\n                torch.ones(real_batch[0].size(0))     # Real domain: 1\n            ]).long()\n\n            # Forward pass\n            label_pred, domain_pred = model(combined_data)\n\n            # Label prediction loss (only on synthetic data)\n            synth_labels = synth_batch[1]  # Assuming synthetic data has labels\n            label_loss = label_criterion(\n                label_pred[:synth_batch[0].size(0)],\n                synth_labels\n            )\n\n            # Domain classification loss (try to fool domain classifier)\n            domain_loss = domain_criterion(domain_pred, combined_domains)\n\n            # Total loss\n            total_loss = label_loss + domain_loss\n\n            total_loss.backward()\n            optimizer.step()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"quality-assessment-and-validation",children:"Quality Assessment and Validation"}),"\n",(0,t.jsx)(n.h3,{id:"synthetic-data-quality-metrics",children:"Synthetic Data Quality Metrics"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class SyntheticDataQualityAssessment:\n    def __init__(self):\n        pass\n\n    def assess_realism_score(self, synthetic_img, real_img_distribution):\n        """\n        Assess how realistic synthetic images appear compared to real images\n        """\n        # Calculate statistical similarity between synthetic and real image distributions\n        synth_features = self.extract_image_features(synthetic_img)\n        real_features_mean = real_img_distribution[\'mean_features\']\n        real_features_std = real_img_distribution[\'std_features\']\n\n        # Calculate z-score for feature similarity\n        z_scores = (synth_features - real_features_mean) / real_features_std\n        realism_score = 1.0 / (1.0 + np.mean(np.abs(z_scores)))\n\n        return realism_score\n\n    def assess_diversity_score(self, synthetic_dataset):\n        """\n        Assess the diversity of synthetic data samples\n        """\n        features_list = []\n        for img in synthetic_dataset:\n            features = self.extract_image_features(img)\n            features_list.append(features)\n\n        features_array = np.array(features_list)\n\n        # Calculate pairwise distances\n        distances = []\n        for i in range(len(features_array)):\n            for j in range(i+1, len(features_array)):\n                dist = np.linalg.norm(features_array[i] - features_array[j])\n                distances.append(dist)\n\n        # Diversity is the average distance between samples\n        diversity_score = np.mean(distances)\n        return diversity_score\n\n    def assess_task_performance(self, model, synthetic_data, real_data, task=\'classification\'):\n        """\n        Assess synthetic data quality by comparing model performance\n        on synthetic-trained vs real-trained models\n        """\n        # Train model on synthetic data\n        synthetic_model = self.train_model(model, synthetic_data, task)\n        synth_performance = self.evaluate_model(synthetic_model, real_data)\n\n        # Train model on real data (if available)\n        real_model = self.train_model(model, real_data, task)\n        real_performance = self.evaluate_model(real_model, real_data)\n\n        # Calculate sim-to-real gap\n        performance_gap = real_performance - synth_performance\n\n        return {\n            \'synthetic_performance\': synth_performance,\n            \'real_performance\': real_performance,\n            \'performance_gap\': performance_gap,\n            \'transfer_score\': max(0, 1 - performance_gap / real_performance)\n        }\n\n    def extract_image_features(self, img):\n        """\n        Extract perceptually relevant features from image\n        """\n        # Convert to grayscale\n        if len(img.shape) == 3:\n            gray = np.dot(img[...,:3], [0.299, 0.587, 0.114])\n        else:\n            gray = img\n\n        # Extract texture features using Local Binary Patterns (simplified)\n        texture_features = self.extract_lbp_features(gray)\n\n        # Extract color histogram features (if color image)\n        if len(img.shape) == 3:\n            color_features = self.extract_color_histogram(img)\n        else:\n            color_features = np.array([])\n\n        # Combine features\n        features = np.concatenate([texture_features, color_features])\n\n        return features\n\n    def extract_lbp_features(self, img, num_points=8, radius=1):\n        """\n        Extract Local Binary Pattern features (simplified)\n        """\n        # This is a simplified version - in practice, use scikit-image or OpenCV\n        features = []\n\n        # Calculate gradients as a simple texture measure\n        grad_x = np.gradient(img, axis=1)\n        grad_y = np.gradient(img, axis=0)\n        gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n\n        # Histogram of gradient magnitudes\n        hist, _ = np.histogram(gradient_magnitude, bins=10, range=(0, np.max(gradient_magnitude)))\n        features.extend(hist)\n\n        return np.array(features)\n\n    def extract_color_histogram(self, img, bins=8):\n        """\n        Extract color histogram features\n        """\n        hists = []\n        for channel in range(img.shape[2]):\n            hist, _ = np.histogram(img[:,:,channel], bins=bins, range=(0, 1))\n            hists.extend(hist)\n\n        return np.array(hists)\n\n    def train_model(self, base_model, data, task):\n        """\n        Train a model on given data (simplified)\n        """\n        # In practice, implement proper training loop\n        return base_model\n\n    def evaluate_model(self, model, data):\n        """\n        Evaluate model performance (simplified)\n        """\n        # In practice, implement proper evaluation\n        return 0.85  # Placeholder accuracy\n'})}),"\n",(0,t.jsx)(n.h2,{id:"best-practices-for-synthetic-data-generation",children:"Best Practices for Synthetic Data Generation"}),"\n",(0,t.jsx)(n.h3,{id:"1-progressive-domain-randomization",children:"1. Progressive Domain Randomization"}),"\n",(0,t.jsx)(n.p,{children:"Start with limited randomization and gradually increase complexity:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class ProgressiveDomainRandomizer:\n    def __init__(self):\n        self.stages = [\n            {'lighting': 0.1, 'textures': 0.1, 'camera': 0.05},\n            {'lighting': 0.3, 'textures': 0.2, 'camera': 0.1},\n            {'lighting': 0.6, 'textures': 0.4, 'camera': 0.2},\n            {'lighting': 1.0, 'textures': 0.8, 'camera': 0.3}\n        ]\n        self.current_stage = 0\n\n    def get_randomization_params(self):\n        \"\"\"Get randomization parameters for current stage\"\"\"\n        return self.stages[self.current_stage]\n\n    def advance_stage(self):\n        \"\"\"Advance to next randomization stage\"\"\"\n        if self.current_stage < len(self.stages) - 1:\n            self.current_stage += 1\n"})}),"\n",(0,t.jsx)(n.h3,{id:"2-validation-against-real-data",children:"2. Validation Against Real Data"}),"\n",(0,t.jsx)(n.p,{children:"Always validate synthetic data quality against real-world distributions:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def validate_synthetic_data_quality(synthetic_data, real_data_samples):\n    \"\"\"\n    Validate synthetic data quality against real data\n    \"\"\"\n    quality_assessment = SyntheticDataQualityAssessment()\n\n    # Assess realism\n    avg_realism = 0\n    for synth_img in synthetic_data[:10]:  # Sample for efficiency\n        realism = quality_assessment.assess_realism_score(\n            synth_img,\n            {'mean_features': np.mean(real_data_samples, axis=0),\n             'std_features': np.std(real_data_samples, axis=0)}\n        )\n        avg_realism += realism\n\n    avg_realism /= min(10, len(synthetic_data))\n\n    # Assess diversity\n    diversity = quality_assessment.assess_diversity_score(synthetic_data)\n\n    return {\n        'realism_score': avg_realism,\n        'diversity_score': diversity,\n        'quality_ok': avg_realism > 0.5 and diversity > 0.1  # Thresholds are example values\n    }\n"})}),"\n",(0,t.jsx)(n.p,{children:"Synthetic data generation is a powerful technique for training AI perception systems in robotics. By combining physics-based rendering, domain randomization, and careful validation, we can create synthetic datasets that enable effective sim-to-real transfer for Physical AI applications. The key is to balance realism with diversity while ensuring that models trained on synthetic data can generalize to real-world scenarios."})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}}}]);