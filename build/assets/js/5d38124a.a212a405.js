"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[7053],{3640:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>a,toc:()=>l});var t=o(4848),i=o(8453);const s={title:"Chapter 12 - Voice-to-Action Robotics with Whisper & ROS 2"},r="Chapter 12: Voice-to-Action Robotics with Whisper & ROS 2",a={id:"physical-ai/voice-robotics/index",title:"Chapter 12 - Voice-to-Action Robotics with Whisper & ROS 2",description:"Chapter Overview",source:"@site/docs/physical-ai/voice-robotics/index.mdx",sourceDirName:"physical-ai/voice-robotics",slug:"/physical-ai/voice-robotics/",permalink:"/physical-ai-textbook/physical-ai/physical-ai/voice-robotics/",draft:!1,unlisted:!1,editUrl:"https://github.com/your-username/physical-ai-textbook/tree/main/docs/physical-ai/voice-robotics/index.mdx",tags:[],version:"current",frontMatter:{title:"Chapter 12 - Voice-to-Action Robotics with Whisper & ROS 2"},sidebar:"tutorialSidebar",previous:{title:"Sim-to-Real Transfer Techniques in Robotics",permalink:"/physical-ai-textbook/physical-ai/physical-ai/navigation-systems/sim-to-real"},next:{title:"whisper-integration",permalink:"/physical-ai-textbook/physical-ai/physical-ai/voice-robotics/whisper-integration"}},c={},l=[{value:"Chapter Overview",id:"chapter-overview",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Introduction to Voice-to-Action Robotics",id:"introduction-to-voice-to-action-robotics",level:2},{value:"The Evolution of Human-Robot Interaction",id:"the-evolution-of-human-robot-interaction",level:3},{value:"Whisper Model in Robotics Context",id:"whisper-model-in-robotics-context",level:3},{value:"Voice Command Architecture",id:"voice-command-architecture",level:3},{value:"Technical Foundation: Whisper Integration with ROS 2",id:"technical-foundation-whisper-integration-with-ros-2",level:2},{value:"Setting Up Whisper for Robotics",id:"setting-up-whisper-for-robotics",level:3},{value:"Advanced Natural Language Processing",id:"advanced-natural-language-processing",level:3},{value:"Real-time Voice Processing Optimization",id:"real-time-voice-processing-optimization",level:2},{value:"Streaming Audio Processing",id:"streaming-audio-processing",level:3},{value:"Integration with Robot Control Systems",id:"integration-with-robot-control-systems",level:2},{value:"Command Execution Pipeline",id:"command-execution-pipeline",level:3},{value:"Performance Optimization and Accuracy Enhancement",id:"performance-optimization-and-accuracy-enhancement",level:2},{value:"Model Optimization for Robotics",id:"model-optimization-for-robotics",level:3},{value:"Error Handling and Robustness",id:"error-handling-and-robustness",level:2},{value:"Voice Command Error Recovery",id:"voice-command-error-recovery",level:3},{value:"Looking Forward",id:"looking-forward",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"chapter-12-voice-to-action-robotics-with-whisper--ros-2",children:"Chapter 12: Voice-to-Action Robotics with Whisper & ROS 2"}),"\n",(0,t.jsx)(n.h2,{id:"chapter-overview",children:"Chapter Overview"}),"\n",(0,t.jsx)(n.p,{children:"This chapter explores the revolutionary integration of advanced speech recognition technology with robotic systems, specifically focusing on OpenAI's Whisper model and its integration with ROS 2 (Robot Operating System 2). Voice-to-action robotics represents a paradigm shift in human-robot interaction, enabling intuitive, natural communication between humans and robotic systems through spoken language. This technology is particularly crucial for Physical AI applications where robots need to understand and execute complex verbal commands in real-time."}),"\n",(0,t.jsx)(n.p,{children:"The chapter covers the technical foundations of speech recognition in robotics, the integration of Whisper's state-of-the-art transcription capabilities with ROS 2's communication framework, and the implementation of natural language processing pipelines that translate spoken commands into robotic actions. We'll examine how modern AI speech recognition can be leveraged to create more intuitive and accessible robotic interfaces."}),"\n",(0,t.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement Whisper-based speech recognition in robotic systems using ROS 2"}),"\n",(0,t.jsx)(n.li,{children:"Design voice command interpretation systems that translate speech to robot actions"}),"\n",(0,t.jsx)(n.li,{children:"Integrate real-time speech processing with robotic control pipelines"}),"\n",(0,t.jsx)(n.li,{children:"Develop natural language understanding modules for robot command execution"}),"\n",(0,t.jsx)(n.li,{children:"Optimize voice-to-action systems for real-time performance and accuracy"}),"\n",(0,t.jsx)(n.li,{children:"Create robust voice interfaces that handle environmental noise and ambiguity"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-voice-to-action-robotics",children:"Introduction to Voice-to-Action Robotics"}),"\n",(0,t.jsx)(n.h3,{id:"the-evolution-of-human-robot-interaction",children:"The Evolution of Human-Robot Interaction"}),"\n",(0,t.jsx)(n.p,{children:"Traditional human-robot interaction has relied primarily on graphical user interfaces, dedicated control panels, or pre-programmed gesture recognition. Voice-to-action robotics introduces a more natural and intuitive interface that mirrors human-to-human communication patterns. This approach is particularly beneficial for:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Accessibility"}),": Enabling interaction for users with limited mobility or visual impairments"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hands-free Operation"}),": Allowing humans to control robots while performing other tasks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Intuitive Communication"}),": Using natural language that doesn't require specialized training"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multimodal Integration"}),": Combining voice commands with visual and tactile feedback"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"whisper-model-in-robotics-context",children:"Whisper Model in Robotics Context"}),"\n",(0,t.jsx)(n.p,{children:"OpenAI's Whisper model represents a breakthrough in speech recognition technology, offering exceptional accuracy across multiple languages and robustness to various acoustic conditions. In robotics applications, Whisper's capabilities are particularly valuable due to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multilingual Support"}),": Understanding commands in multiple languages"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Noise Robustness"}),": Maintaining accuracy in noisy environments"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time Processing"}),": Capable of near real-time transcription with appropriate optimization"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Context Understanding"}),": Ability to maintain context across multiple utterances"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Customization Potential"}),": Capability to fine-tune for domain-specific vocabularies"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"voice-command-architecture",children:"Voice Command Architecture"}),"\n",(0,t.jsx)(n.p,{children:"The voice-to-action pipeline in robotics typically follows this architecture:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Audio Input \u2192 Speech Recognition \u2192 Natural Language Processing \u2192 Action Planning \u2192 Robot Execution\n"})}),"\n",(0,t.jsx)(n.p,{children:"Each stage must operate efficiently to maintain responsive interaction while ensuring accuracy in command interpretation."}),"\n",(0,t.jsx)(n.h2,{id:"technical-foundation-whisper-integration-with-ros-2",children:"Technical Foundation: Whisper Integration with ROS 2"}),"\n",(0,t.jsx)(n.h3,{id:"setting-up-whisper-for-robotics",children:"Setting Up Whisper for Robotics"}),"\n",(0,t.jsx)(n.p,{children:"To integrate Whisper with ROS 2 systems, we need to establish a robust audio processing pipeline:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rospy\nimport whisper\nimport torch\nimport numpy as np\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import Joy\n\nclass WhisperROSNode:\n    def __init__(self):\n        # Initialize ROS node\n        rospy.init_node(\'whisper_voice_control\', anonymous=True)\n\n        # Load Whisper model (choose appropriate size based on computational resources)\n        self.model_size = rospy.get_param(\'~model_size\', \'base\')\n        self.model = whisper.load_model(self.model_size)\n\n        # Audio buffer for continuous processing\n        self.audio_buffer = []\n        self.buffer_size = rospy.get_param(\'~buffer_size\', 16000 * 2)  # 2 seconds of audio\n\n        # Publishers and subscribers\n        self.voice_command_pub = rospy.Publisher(\'/voice/command\', String, queue_size=10)\n        self.robot_cmd_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=10)\n        self.audio_sub = rospy.Subscriber(\'/audio/audio\', AudioData, self.audio_callback)\n\n        # Configuration parameters\n        self.silence_threshold = rospy.get_param(\'~silence_threshold\', 0.01)\n        self.vad_threshold = rospy.get_param(\'~vad_threshold\', 0.3)  # Voice activity detection\n        self.language = rospy.get_param(\'~language\', \'en\')\n\n        # Command mapping dictionary\n        self.command_map = {\n            \'move forward\': self.move_forward,\n            \'move backward\': self.move_backward,\n            \'turn left\': self.turn_left,\n            \'turn right\': self.turn_right,\n            \'stop\': self.stop_robot,\n            \'come here\': self.go_to_user,\n            \'follow me\': self.follow_user,\n            \'pick up object\': self.pick_object,\n            \'place object\': self.place_object,\n        }\n\n        rospy.loginfo("Whisper voice control node initialized")\n\n    def audio_callback(self, audio_msg):\n        """Handle incoming audio data"""\n        # Convert audio data to numpy array\n        audio_array = np.frombuffer(audio_msg.data, dtype=np.int16).astype(np.float32) / 32768.0\n\n        # Add to buffer\n        self.audio_buffer.extend(audio_array)\n\n        # Process if buffer is full\n        if len(self.audio_buffer) >= self.buffer_size:\n            self.process_audio_buffer()\n            # Keep some overlap for continuity\n            self.audio_buffer = self.audio_buffer[-int(self.buffer_size/4):]\n\n    def process_audio_buffer(self):\n        """Process the accumulated audio buffer"""\n        if len(self.audio_buffer) == 0:\n            return\n\n        # Convert to appropriate format for Whisper\n        audio_data = np.array(self.audio_buffer)\n\n        # Check for voice activity (simplified approach)\n        if np.max(np.abs(audio_data)) < self.silence_threshold:\n            return  # Skip silent segments\n\n        try:\n            # Transcribe audio using Whisper\n            result = self.model.transcribe(\n                audio_data,\n                language=self.language,\n                task=\'transcribe\'\n            )\n\n            transcription = result[\'text\'].strip().lower()\n\n            if transcription:  # Only process non-empty transcriptions\n                rospy.loginfo(f"Transcribed: {transcription}")\n                self.process_command(transcription)\n\n        except Exception as e:\n            rospy.logerr(f"Error in Whisper transcription: {e}")\n\n    def process_command(self, command_text):\n        """Process the transcribed command and execute appropriate action"""\n        # Publish the recognized command\n        cmd_msg = String()\n        cmd_msg.data = command_text\n        self.voice_command_pub.publish(cmd_msg)\n\n        # Try to match command with known actions\n        matched = False\n        for keyword, action_func in self.command_map.items():\n            if keyword in command_text:\n                rospy.loginfo(f"Executing command: {keyword}")\n                action_func()\n                matched = True\n                break\n\n        if not matched:\n            # Try fuzzy matching for similar commands\n            best_match = self.find_best_command_match(command_text)\n            if best_match:\n                rospy.loginfo(f"Fuzzy match: executing {best_match}")\n                self.command_map[best_match]()\n            else:\n                rospy.loginfo(f"Unknown command: {command_text}")\n\n    def find_best_command_match(self, input_text):\n        """Find the best matching command using fuzzy logic"""\n        from difflib import SequenceMatcher\n\n        best_match = None\n        best_ratio = 0.0\n        threshold = 0.6  # Minimum similarity ratio\n\n        for command in self.command_map.keys():\n            ratio = SequenceMatcher(None, input_text, command).ratio()\n            if ratio > best_ratio and ratio > threshold:\n                best_ratio = ratio\n                best_match = command\n\n        return best_match\n\n    # Robot action implementations\n    def move_forward(self):\n        cmd = Twist()\n        cmd.linear.x = 0.5  # Adjust speed as needed\n        self.robot_cmd_pub.publish(cmd)\n\n    def move_backward(self):\n        cmd = Twist()\n        cmd.linear.x = -0.5\n        self.robot_cmd_pub.publish(cmd)\n\n    def turn_left(self):\n        cmd = Twist()\n        cmd.angular.z = 0.5\n        self.robot_cmd_pub.publish(cmd)\n\n    def turn_right(self):\n        cmd = Twist()\n        cmd.angular.z = -0.5\n        self.robot_cmd_pub.publish(cmd)\n\n    def stop_robot(self):\n        cmd = Twist()\n        self.robot_cmd_pub.publish(cmd)\n\n    def go_to_user(self):\n        # This would involve more complex navigation logic\n        rospy.loginfo("Initiating navigation to user")\n        # Implementation would involve person detection and navigation\n\n    def follow_user(self):\n        # Follow-behavior implementation\n        rospy.loginfo("Starting follow behavior")\n        # Implementation would involve person tracking and following\n\n    def pick_object(self):\n        # Manipulation command implementation\n        rospy.loginfo("Attempting to pick up object")\n        # Implementation would involve perception and manipulation\n\n    def place_object(self):\n        # Manipulation command implementation\n        rospy.loginfo("Attempting to place object")\n        # Implementation would involve manipulation planning\n\n    def spin(self):\n        """Main loop"""\n        rospy.spin()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"advanced-natural-language-processing",children:"Advanced Natural Language Processing"}),"\n",(0,t.jsx)(n.p,{children:"For more sophisticated command interpretation, we can enhance our system with advanced NLP techniques:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import spacy\nimport transformers\nfrom transformers import pipeline\nfrom typing import Dict, List, Tuple\n\nclass AdvancedVoiceCommandProcessor:\n    def __init__(self):\n        # Load spaCy model for linguistic analysis\n        try:\n            self.nlp = spacy.load(\"en_core_web_sm\")\n        except OSError:\n            rospy.logwarn(\"spaCy English model not found. Install with: python -m spacy download en_core_web_sm\")\n            self.nlp = None\n\n        # Load a question-answering model for command disambiguation\n        self.qa_pipeline = pipeline(\n            \"question-answering\",\n            model=\"distilbert-base-cased-distilled-squad\"\n        )\n\n        # Define command templates and entities\n        self.command_templates = {\n            'navigation': {\n                'patterns': [\n                    'go to the {location}',\n                    'move to {location}',\n                    'navigate to {location}',\n                    'go to {location} room',\n                ],\n                'entities': ['location']\n            },\n            'manipulation': {\n                'patterns': [\n                    'pick up the {object}',\n                    'grab the {object}',\n                    'take the {object}',\n                    'move the {object} to {destination}',\n                ],\n                'entities': ['object', 'destination']\n            },\n            'interaction': {\n                'patterns': [\n                    'talk to me',\n                    'tell me about yourself',\n                    'introduce yourself',\n                    'what can you do',\n                ],\n                'entities': []\n            }\n        }\n\n    def parse_command_advanced(self, text: str) -> Dict:\n        \"\"\"Advanced command parsing using NLP techniques\"\"\"\n        if self.nlp:\n            doc = self.nlp(text)\n\n            # Extract entities\n            entities = {}\n            for ent in doc.ents:\n                entities[ent.label_] = ent.text\n\n            # Analyze dependencies and parts of speech\n            action_verb = None\n            for token in doc:\n                if token.pos_ == \"VERB\":\n                    action_verb = token.lemma_\n                    break\n\n            # Determine command type based on patterns\n            command_type = self.classify_command_type(text)\n\n            return {\n                'command_type': command_type,\n                'entities': entities,\n                'action_verb': action_verb,\n                'original_text': text,\n                'parsed': True\n            }\n        else:\n            # Fallback to simple keyword matching\n            return self.simple_parse_command(text)\n\n    def classify_command_type(self, text: str) -> str:\n        \"\"\"Classify command into predefined categories\"\"\"\n        text_lower = text.lower()\n\n        for cmd_type, template_info in self.command_templates.items():\n            for pattern in template_info['patterns']:\n                # Simple pattern matching (could be enhanced with regex)\n                if any(keyword in text_lower for keyword in pattern.split() if '{' not in keyword):\n                    return cmd_type\n\n        return 'unknown'\n\n    def simple_parse_command(self, text: str) -> Dict:\n        \"\"\"Fallback command parsing without advanced NLP\"\"\"\n        # Simple keyword-based parsing\n        entities = {}\n\n        if 'room' in text:\n            entities['location'] = text.split('room')[0].split()[-1] + ' room'\n        elif 'kitchen' in text or 'bedroom' in text or 'living room' in text:\n            for room_type in ['kitchen', 'bedroom', 'living room', 'bathroom']:\n                if room_type in text:\n                    entities['location'] = room_type\n                    break\n\n        if 'object' in text or 'thing' in text or 'item' in text:\n            # Extract object mentions\n            for word in text.split():\n                if word.lower() in ['ball', 'cup', 'book', 'box', 'toy']:\n                    entities['object'] = word\n                    break\n\n        return {\n            'command_type': self.classify_command_type(text),\n            'entities': entities,\n            'action_verb': None,\n            'original_text': text,\n            'parsed': False\n        }\n"})}),"\n",(0,t.jsx)(n.h2,{id:"real-time-voice-processing-optimization",children:"Real-time Voice Processing Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"streaming-audio-processing",children:"Streaming Audio Processing"}),"\n",(0,t.jsx)(n.p,{children:"For real-time applications, we need to implement efficient streaming audio processing:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import pyaudio\nimport threading\nimport queue\nimport time\n\nclass RealTimeVoiceProcessor:\n    def __init__(self, model_size=\'base\'):\n        self.model = whisper.load_model(model_size)\n\n        # Audio configuration\n        self.rate = 16000  # Sampling rate\n        self.chunk = 1024  # Audio chunk size\n        self.format = pyaudio.paInt16\n        self.channels = 1\n\n        # Processing buffers\n        self.audio_queue = queue.Queue()\n        self.transcription_queue = queue.Queue()\n\n        # Voice activity detection parameters\n        self.energy_threshold = 0.01\n        self.silence_duration = 1.0  # Seconds of silence to trigger processing\n        self.continuous_audio = np.array([], dtype=np.float32)\n\n        # Threading\n        self.processing_thread = threading.Thread(target=self.process_audio_stream)\n        self.processing_thread.daemon = True\n        self.running = True\n\n        # Callback function for command processing\n        self.command_callback = None\n\n    def start_audio_capture(self):\n        """Start capturing audio from microphone"""\n        self.audio = pyaudio.PyAudio()\n\n        self.stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk,\n            stream_callback=self.audio_callback\n        )\n\n        self.processing_thread.start()\n        self.stream.start_stream()\n\n    def audio_callback(self, in_data, frame_count, time_info, status):\n        """Callback for real-time audio capture"""\n        audio_data = np.frombuffer(in_data, dtype=np.int16).astype(np.float32) / 32768.0\n\n        # Add to continuous buffer\n        self.continuous_audio = np.concatenate([self.continuous_audio, audio_data])\n\n        # Keep only the last 5 seconds to manage memory\n        max_samples = self.rate * 5\n        if len(self.continuous_audio) > max_samples:\n            self.continuous_audio = self.continuous_audio[-max_samples:]\n\n        return (in_data, pyaudio.paContinue)\n\n    def process_audio_stream(self):\n        """Continuously process audio for voice commands"""\n        last_voice_time = time.time()\n\n        while self.running:\n            current_time = time.time()\n\n            # Check if we have enough audio and sufficient silence has passed\n            if (len(self.continuous_audio) > self.rate * 0.5 and  # At least 0.5 seconds\n                current_time - last_voice_time > self.silence_duration):\n\n                # Check for voice activity in the last segment\n                recent_audio = self.continuous_audio[-int(self.rate * 2):]  # Last 2 seconds\n                if np.max(np.abs(recent_audio)) > self.energy_threshold:\n                    # Process this segment\n                    self.process_segment(recent_audio)\n                    last_voice_time = current_time\n                    # Clear the processed portion to manage buffer size\n                    self.continuous_audio = self.continuous_audio[:-int(self.rate * 2)]\n\n            time.sleep(0.1)  # Small delay to prevent busy waiting\n\n    def process_segment(self, audio_segment):\n        """Process a segment of audio for speech recognition"""\n        try:\n            result = self.model.transcribe(\n                audio_segment,\n                language=\'en\',\n                task=\'transcribe\',\n                temperature=0.0  # More deterministic for commands\n            )\n\n            if result[\'text\'].strip():  # If we got a transcription\n                transcription = result[\'text\'].strip()\n                rospy.loginfo(f"Real-time transcription: {transcription}")\n\n                # Process the command if callback is set\n                if self.command_callback:\n                    self.command_callback(transcription)\n\n        except Exception as e:\n            rospy.logerr(f"Error processing audio segment: {e}")\n\n    def set_command_callback(self, callback_func):\n        """Set callback function for processed commands"""\n        self.command_callback = callback_func\n\n    def stop(self):\n        """Stop audio processing"""\n        self.running = False\n        if hasattr(self, \'stream\'):\n            self.stream.stop_stream()\n            self.stream.close()\n        if hasattr(self, \'audio\'):\n            self.audio.terminate()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"integration-with-robot-control-systems",children:"Integration with Robot Control Systems"}),"\n",(0,t.jsx)(n.h3,{id:"command-execution-pipeline",children:"Command Execution Pipeline"}),"\n",(0,t.jsx)(n.p,{children:"The voice-to-action system needs to be tightly integrated with the robot's control architecture:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import actionlib\nfrom move_base_msgs.msg import MoveBaseAction, MoveBaseGoal\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import String\n\nclass VoiceCommandExecutor:\n    def __init__(self):\n        # Initialize ROS components\n        self.nav_client = actionlib.SimpleActionClient(\'move_base\', MoveBaseAction)\n        self.manipulation_client = actionlib.SimpleActionClient(\'manipulation_server\', ManipulationAction)\n\n        # Wait for servers\n        rospy.loginfo("Waiting for move_base action server...")\n        self.nav_client.wait_for_server(rospy.Duration(10.0))\n\n        rospy.loginfo("Waiting for manipulation action server...")\n        self.manipulation_client.wait_for_server(rospy.Duration(10.0))\n\n        # Publishers for different robot systems\n        self.cmd_vel_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=10)\n        self.speech_pub = rospy.Publisher(\'/tts/input\', String, queue_size=10)\n\n        # Location mapping\n        self.location_map = {\n            \'kitchen\': self.get_kitchen_pose(),\n            \'bedroom\': self.get_bedroom_pose(),\n            \'living room\': self.get_living_room_pose(),\n            \'office\': self.get_office_pose(),\n            \'dining room\': self.get_dining_room_pose()\n        }\n\n    def execute_navigation_command(self, destination):\n        """Execute navigation command to specified destination"""\n        if destination.lower() in self.location_map:\n            goal_pose = self.location_map[destination.lower()]\n            goal = MoveBaseGoal()\n            goal.target_pose = goal_pose\n\n            rospy.loginfo(f"Navigating to {destination}")\n            self.nav_client.send_goal(goal)\n\n            # Wait for result with timeout\n            finished_within_time = self.nav_client.wait_for_result(rospy.Duration(60.0))\n\n            if not finished_within_time:\n                self.nav_client.cancel_goal()\n                rospy.loginfo("Navigation timeout")\n                return False\n            else:\n                state = self.nav_client.get_state()\n                if state == actionlib.GoalStatus.SUCCEEDED:\n                    rospy.loginfo(f"Successfully reached {destination}")\n                    self.speak_response(f"I have reached the {destination}")\n                    return True\n                else:\n                    rospy.loginfo(f"Failed to reach {destination}")\n                    self.speak_response(f"Sorry, I couldn\'t reach the {destination}")\n                    return False\n        else:\n            rospy.loginfo(f"Unknown destination: {destination}")\n            self.speak_response(f"Sorry, I don\'t know where the {destination} is")\n            return False\n\n    def execute_manipulation_command(self, object_name, action=\'pick\'):\n        """Execute manipulation command for specified object"""\n        goal = ManipulationGoal()\n        goal.object_name = object_name\n        goal.action = action  # \'pick\', \'place\', \'move\'\n\n        rospy.loginfo(f"Attempting to {action} {object_name}")\n        self.manipulation_client.send_goal(goal)\n\n        finished_within_time = self.manipulation_client.wait_for_result(rospy.Duration(30.0))\n\n        if not finished_within_time:\n            self.manipulation_client.cancel_goal()\n            rospy.loginfo("Manipulation timeout")\n            self.speak_response(f"Sorry, I couldn\'t {action} the {object_name}")\n            return False\n        else:\n            state = self.manipulation_client.get_state()\n            if state == actionlib.GoalStatus.SUCCEEDED:\n                rospy.loginfo(f"Successfully {action}ed {object_name}")\n                self.speak_response(f"I have successfully {action}ed the {object_name}")\n                return True\n            else:\n                rospy.loginfo(f"Failed to {action} {object_name}")\n                self.speak_response(f"Sorry, I couldn\'t {action} the {object_name}")\n                return False\n\n    def speak_response(self, text):\n        """Publish text-to-speech response"""\n        msg = String()\n        msg.data = text\n        self.speech_pub.publish(msg)\n\n    def get_kitchen_pose(self):\n        """Get predefined kitchen pose"""\n        pose = PoseStamped()\n        pose.header.frame_id = "map"\n        pose.pose.position.x = 2.0\n        pose.pose.position.y = 1.0\n        pose.pose.orientation.w = 1.0\n        return pose\n\n    def get_bedroom_pose(self):\n        """Get predefined bedroom pose"""\n        pose = PoseStamped()\n        pose.header.frame_id = "map"\n        pose.pose.position.x = -1.0\n        pose.pose.position.y = -2.0\n        pose.pose.orientation.w = 1.0\n        return pose\n\n    def get_living_room_pose(self):\n        """Get predefined living room pose"""\n        pose = PoseStamped()\n        pose.header.frame_id = "map"\n        pose.pose.position.x = 0.0\n        pose.pose.position.y = 0.0\n        pose.pose.orientation.w = 1.0\n        return pose\n\n    def get_office_pose(self):\n        """Get predefined office pose"""\n        pose = PoseStamped()\n        pose.header.frame_id = "map"\n        pose.pose.position.x = 3.0\n        pose.pose.position.y = -1.0\n        pose.pose.orientation.w = 1.0\n        return pose\n\n    def get_dining_room_pose(self):\n        """Get predefined dining room pose"""\n        pose = PoseStamped()\n        pose.header.frame_id = "map"\n        pose.pose.position.x = 1.0\n        pose.pose.position.y = 2.0\n        pose.pose.orientation.w = 1.0\n        return pose\n'})}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization-and-accuracy-enhancement",children:"Performance Optimization and Accuracy Enhancement"}),"\n",(0,t.jsx)(n.h3,{id:"model-optimization-for-robotics",children:"Model Optimization for Robotics"}),"\n",(0,t.jsx)(n.p,{children:"For robotics applications, we need to optimize Whisper for real-time performance:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch\nfrom transformers import WhisperForConditionalGeneration, WhisperProcessor\nimport time\n\nclass OptimizedWhisperProcessor:\n    def __init__(self, model_name=\"openai/whisper-base\"):\n        # Use CUDA if available\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        # Load model and processor\n        self.model = WhisperForConditionalGeneration.from_pretrained(model_name)\n        self.processor = WhisperProcessor.from_pretrained(model_name)\n\n        # Move model to device\n        self.model.to(self.device)\n\n        # Set model to evaluation mode\n        self.model.eval()\n\n        # Performance statistics\n        self.processing_times = []\n        self.accuracy_stats = []\n\n    def transcribe_optimized(self, audio_input, language=\"en\", task=\"transcribe\"):\n        \"\"\"Optimized transcription with performance monitoring\"\"\"\n        start_time = time.time()\n\n        try:\n            # Process audio input\n            if isinstance(audio_input, np.ndarray):\n                # Direct audio array\n                input_features = self.processor(\n                    audio_input,\n                    sampling_rate=16000,\n                    return_tensors=\"pt\"\n                ).input_features\n            else:\n                # File path or other input\n                input_features = self.processor(\n                    audio_input,\n                    return_tensors=\"pt\"\n                ).input_features\n\n            # Move to device\n            input_features = input_features.to(self.device)\n\n            # Generate token ids\n            generated_ids = self.model.generate(\n                input_features,\n                language=language,\n                task=task,\n                # Optimization parameters\n                max_new_tokens=128,  # Limit output length\n                do_sample=False,     # Deterministic output for commands\n                temperature=0.0,     # No randomness\n            )\n\n            # Decode transcription\n            transcription = self.processor.batch_decode(\n                generated_ids,\n                skip_special_tokens=True\n            )[0]\n\n            # Record performance\n            processing_time = time.time() - start_time\n            self.processing_times.append(processing_time)\n\n            rospy.loginfo(f\"Transcription took {processing_time:.3f}s: {transcription}\")\n\n            return {\n                'text': transcription.strip(),\n                'processing_time': processing_time,\n                'confidence': self.estimate_confidence(transcription)\n            }\n\n        except Exception as e:\n            rospy.logerr(f\"Error in optimized transcription: {e}\")\n            return {\n                'text': '',\n                'processing_time': time.time() - start_time,\n                'error': str(e)\n            }\n\n    def estimate_confidence(self, transcription):\n        \"\"\"Estimate confidence in transcription\"\"\"\n        # Simple confidence estimation based on various factors\n        if not transcription or len(transcription.strip()) == 0:\n            return 0.0\n\n        # Length-based confidence (very short transcriptions might be unreliable)\n        length_confidence = min(len(transcription) / 10.0, 1.0)  # Up to 10 characters = full confidence\n\n        # Contains common words confidence\n        common_words = ['the', 'and', 'is', 'are', 'to', 'for', 'on', 'in', 'at', 'by']\n        word_count = len(transcription.split())\n        common_word_ratio = sum(1 for word in transcription.lower().split() if word in common_words) / max(word_count, 1)\n\n        # Combine confidences\n        overall_confidence = (length_confidence * 0.6 + common_word_ratio * 0.4)\n\n        return min(overall_confidence, 1.0)\n\n    def get_performance_stats(self):\n        \"\"\"Get performance statistics\"\"\"\n        if not self.processing_times:\n            return {'avg_processing_time': 0, 'call_count': 0}\n\n        return {\n            'avg_processing_time': np.mean(self.processing_times),\n            'min_processing_time': np.min(self.processing_times),\n            'max_processing_time': np.max(self.processing_times),\n            'call_count': len(self.processing_times)\n        }\n"})}),"\n",(0,t.jsx)(n.h2,{id:"error-handling-and-robustness",children:"Error Handling and Robustness"}),"\n",(0,t.jsx)(n.h3,{id:"voice-command-error-recovery",children:"Voice Command Error Recovery"}),"\n",(0,t.jsx)(n.p,{children:"Robust voice-to-action systems need comprehensive error handling:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class VoiceCommandErrorRecovery:\n    def __init__(self, command_executor):\n        self.executor = command_executor\n        self.command_history = []\n        self.error_recovery_enabled = True\n\n        # Context for disambiguation\n        self.last_known_context = {}\n\n    def execute_command_with_recovery(self, command_text, parsed_command=None):\n        \"\"\"Execute command with error recovery capabilities\"\"\"\n        if not self.error_recovery_enabled:\n            return self.executor.execute_simple_command(command_text)\n\n        try:\n            # Store command in history\n            command_entry = {\n                'text': command_text,\n                'timestamp': rospy.Time.now(),\n                'attempt_count': 0,\n                'status': 'pending'\n            }\n            self.command_history.append(command_entry)\n\n            # Attempt execution\n            result = self.attempt_command_execution(command_text, parsed_command)\n\n            if result['success']:\n                command_entry['status'] = 'success'\n                return result\n            else:\n                # Try recovery strategies\n                recovery_result = self.attempt_error_recovery(command_text, result['error'])\n\n                if recovery_result['success']:\n                    command_entry['status'] = 'recovered'\n                    return recovery_result\n                else:\n                    command_entry['status'] = 'failed'\n                    self.handle_persistent_failure(command_text, recovery_result['error'])\n                    return recovery_result\n\n        except Exception as e:\n            rospy.logerr(f\"Error in command execution with recovery: {e}\")\n            return {'success': False, 'error': str(e)}\n\n    def attempt_command_execution(self, command_text, parsed_command=None):\n        \"\"\"Attempt to execute a single command\"\"\"\n        if parsed_command is None:\n            parsed_command = self.parse_command(command_text)\n\n        try:\n            if parsed_command['command_type'] == 'navigation':\n                success = self.executor.execute_navigation_command(parsed_command['entities'].get('location', ''))\n            elif parsed_command['command_type'] == 'manipulation':\n                object_name = parsed_command['entities'].get('object', '')\n                action = self.determine_manipulation_action(command_text)\n                success = self.executor.execute_manipulation_command(object_name, action)\n            else:\n                # Handle other command types\n                success = self.handle_other_command_types(parsed_command)\n\n            return {'success': success, 'command': parsed_command}\n\n        except Exception as e:\n            return {'success': False, 'error': str(e)}\n\n    def attempt_error_recovery(self, command_text, error):\n        \"\"\"Attempt various recovery strategies\"\"\"\n        recovery_strategies = [\n            self.recovery_clarification_request,\n            self.recovery_alternative_interpretation,\n            self.recovery_context_aware_retry,\n            self.recovery_simplification\n        ]\n\n        for strategy in recovery_strategies:\n            try:\n                result = strategy(command_text, error)\n                if result['success']:\n                    return result\n            except Exception as e:\n                rospy.logwarn(f\"Recovery strategy failed: {e}\")\n                continue\n\n        # If all recovery strategies fail\n        return {'success': False, 'error': f\"All recovery strategies failed: {error}\"}\n\n    def recovery_clarification_request(self, command_text, error):\n        \"\"\"Ask for clarification when command is ambiguous\"\"\"\n        # Check if error is related to ambiguity\n        if \"ambiguous\" in error.lower() or \"unclear\" in error.lower():\n            # Ask for clarification\n            clarification_prompt = f\"I didn't understand your command '{command_text}'. Could you please repeat or rephrase it?\"\n            self.executor.speak_response(clarification_prompt)\n\n            # In a real system, you would wait for a follow-up command\n            # For now, return a recovery indication\n            return {'success': False, 'needs_clarification': True, 'message': clarification_prompt}\n\n        return {'success': False, 'error': error}\n\n    def recovery_alternative_interpretation(self, command_text, error):\n        \"\"\"Try alternative interpretations of the command\"\"\"\n        # Try different parsing approaches\n        alternative_parses = [\n            self.simple_keyword_parse(command_text),\n            self.partial_match_parse(command_text),\n            self.semantic_similarity_parse(command_text)\n        ]\n\n        for alt_parse in alternative_parses:\n            if alt_parse and alt_parse.get('command_type'):\n                try:\n                    result = self.attempt_command_execution(command_text, alt_parse)\n                    if result['success']:\n                        return result\n                except:\n                    continue\n\n        return {'success': False, 'error': error}\n\n    def handle_persistent_failure(self, command_text, error):\n        \"\"\"Handle cases where command consistently fails\"\"\"\n        rospy.logerr(f\"Persistent failure for command '{command_text}': {error}\")\n\n        # Log the failure for analysis\n        self.log_failure(command_text, error)\n\n        # Provide user feedback\n        feedback_msg = f\"Sorry, I couldn't execute '{command_text}'. The command failed with error: {error}\"\n        self.executor.speak_response(feedback_msg)\n\n    def log_failure(self, command_text, error):\n        \"\"\"Log command failures for system improvement\"\"\"\n        failure_log = {\n            'timestamp': rospy.Time.now(),\n            'command': command_text,\n            'error': error,\n            'context': self.last_known_context.copy()\n        }\n\n        # In a real system, you might save this to a database or file\n        rospy.loginfo(f\"Logged failure: {failure_log}\")\n"})}),"\n",(0,t.jsx)(n.h2,{id:"looking-forward",children:"Looking Forward"}),"\n",(0,t.jsx)(n.p,{children:"Voice-to-action robotics represents a significant advancement in human-robot interaction, making robots more accessible and intuitive to use. The integration of Whisper's advanced speech recognition with ROS 2's robust communication framework enables sophisticated voice-controlled robotic systems that can understand and execute complex commands in real-time."}),"\n",(0,t.jsx)(n.p,{children:"As this technology continues to evolve, we can expect even more natural and sophisticated voice interfaces that will further blur the line between human and robot interaction, making robots truly collaborative partners in various applications from household assistance to industrial automation."}),"\n",(0,t.jsx)(n.p,{children:"The techniques covered in this chapter provide the foundation for building robust, responsive voice-controlled robotic systems that can operate effectively in real-world environments with varying acoustic conditions and complex command requirements."})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(m,{...e})}):m(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>a});var t=o(6540);const i={},s=t.createContext(i);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);