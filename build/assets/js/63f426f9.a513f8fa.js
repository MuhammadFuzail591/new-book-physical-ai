"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[5279],{901:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>m,frontMatter:()=>a,metadata:()=>r,toc:()=>l});var s=t(4848),o=t(8453);const a={title:"LLM Integration with ROS 2 for Cognitive Robotics"},i="LLM Integration with ROS 2 for Cognitive Robotics",r={id:"physical-ai/cognitive-planning/llm-integration",title:"LLM Integration with ROS 2 for Cognitive Robotics",description:"Introduction to LLM Integration in Robotics",source:"@site/docs/physical-ai/cognitive-planning/02-llm-integration.mdx",sourceDirName:"physical-ai/cognitive-planning",slug:"/physical-ai/cognitive-planning/llm-integration",permalink:"/physical-ai-textbook/physical-ai/physical-ai/cognitive-planning/llm-integration",draft:!1,unlisted:!1,editUrl:"https://github.com/your-username/physical-ai-textbook/tree/main/docs/physical-ai/cognitive-planning/02-llm-integration.mdx",tags:[],version:"current",sidebarPosition:2,frontMatter:{title:"LLM Integration with ROS 2 for Cognitive Robotics"},sidebar:"tutorialSidebar",previous:{title:"Vision-Language-Action Systems in Robotics",permalink:"/physical-ai-textbook/physical-ai/physical-ai/cognitive-planning/vla-systems"},next:{title:"chapter-summary",permalink:"/physical-ai-textbook/physical-ai/physical-ai/cognitive-planning/chapter-summary"}},c={},l=[{value:"Introduction to LLM Integration in Robotics",id:"introduction-to-llm-integration-in-robotics",level:2},{value:"LLM Architectures for Robotics",id:"llm-architectures-for-robotics",level:2},{value:"Transformer-Based Models",id:"transformer-based-models",level:3},{value:"Vision-Language Models",id:"vision-language-models",level:3},{value:"ROS 2 Integration Patterns",id:"ros-2-integration-patterns",level:2},{value:"Cognitive Node Architecture",id:"cognitive-node-architecture",level:3},{value:"Context Management and Memory",id:"context-management-and-memory",level:3},{value:"Task Planning and Execution",id:"task-planning-and-execution",level:2},{value:"Hierarchical Task Networks",id:"hierarchical-task-networks",level:3},{value:"Safety and Validation",id:"safety-and-validation",level:2},{value:"Confidence Assessment and Validation",id:"confidence-assessment-and-validation",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Caching and Efficient Processing",id:"caching-and-efficient-processing",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"llm-integration-with-ros-2-for-cognitive-robotics",children:"LLM Integration with ROS 2 for Cognitive Robotics"}),"\n",(0,s.jsx)(n.h2,{id:"introduction-to-llm-integration-in-robotics",children:"Introduction to LLM Integration in Robotics"}),"\n",(0,s.jsx)(n.p,{children:"Large Language Models (LLMs) have emerged as powerful tools for cognitive robotics, enabling robots to understand natural language commands, reason about complex tasks, and generate executable action sequences. The integration of LLMs with ROS 2 (Robot Operating System 2) creates cognitive robotic systems that can bridge the gap between high-level human instructions and low-level robot control, facilitating more natural and intuitive human-robot interaction."}),"\n",(0,s.jsx)(n.p,{children:'Traditional robotic systems rely on pre-programmed behaviors and structured command interfaces, limiting their flexibility and requiring specialized knowledge to operate. LLM integration allows robots to interpret natural language instructions such as "bring me the red mug from the kitchen" and translate them into sequences of ROS 2 actions and service calls. This integration requires careful consideration of real-time constraints, safety mechanisms, and the distributed nature of ROS 2 systems.'}),"\n",(0,s.jsx)(n.p,{children:"The integration process involves several key components: natural language understanding, task planning, action generation, and execution monitoring. Each component must be designed to work seamlessly within the ROS 2 ecosystem while maintaining the responsiveness and reliability required for robotic applications."}),"\n",(0,s.jsx)(n.h2,{id:"llm-architectures-for-robotics",children:"LLM Architectures for Robotics"}),"\n",(0,s.jsx)(n.h3,{id:"transformer-based-models",children:"Transformer-Based Models"}),"\n",(0,s.jsx)(n.p,{children:"Transformer-based LLMs form the foundation of most robotic language understanding systems. These models excel at processing natural language and can be adapted for robotic tasks through fine-tuning or prompt engineering:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nfrom transformers import AutoModel, AutoTokenizer\nfrom typing import Dict, List, Optional, Any\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\n\nclass RobotLanguageModel(nn.Module):\n    """Transformer-based language model adapted for robotics tasks"""\n\n    def __init__(self, model_name: str = "microsoft/DialoGPT-medium"):\n        super().__init__()\n\n        # Base transformer model\n        self.transformer = AutoModel.from_pretrained(model_name)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n        # Add special tokens for robotics domain\n        special_tokens = {\n            "additional_special_tokens": [\n                "<robot>", "<action>", "<object>", "<location>",\n                "<navigation>", "<manipulation>", "<perception>"\n            ]\n        }\n        self.tokenizer.add_special_tokens(special_tokens)\n\n        # Task-specific heads\n        self.intent_classifier = nn.Linear(self.transformer.config.hidden_size, 10)  # 10 robot intents\n        self.action_generator = nn.Linear(self.transformer.config.hidden_size, 128)  # Action space\n        self.location_predictor = nn.Linear(self.transformer.config.hidden_size, 64)  # Location space\n\n        # Action vocabulary for robotics\n        self.robot_actions = [\n            "move_to", "pick_up", "place", "grasp", "release",\n            "navigate", "inspect", "follow", "stop", "wait"\n        ]\n\n    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Dict[str, torch.Tensor]:\n        """Process input and generate robot-specific outputs"""\n        # Get transformer outputs\n        transformer_outputs = self.transformer(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n\n        # Use the last hidden state for predictions\n        last_hidden_state = transformer_outputs.last_hidden_state[:, -1, :]  # [batch, hidden_size]\n\n        # Predict intent\n        intent_logits = self.intent_classifier(last_hidden_state)\n\n        # Generate action representation\n        action_repr = self.action_generator(last_hidden_state)\n\n        # Predict location/context\n        location_repr = self.location_predictor(last_hidden_state)\n\n        return {\n            \'intent_logits\': intent_logits,\n            \'action_repr\': action_repr,\n            \'location_repr\': location_repr,\n            \'hidden_states\': transformer_outputs.last_hidden_state\n        }\n\n    def parse_command(self, command: str) -> Dict[str, Any]:\n        """Parse a natural language command into structured components"""\n        # Tokenize the command\n        inputs = self.tokenizer(\n            command,\n            return_tensors="pt",\n            padding=True,\n            truncation=True,\n            max_length=512\n        )\n\n        # Get model outputs\n        outputs = self.forward(inputs[\'input_ids\'], inputs[\'attention_mask\'])\n\n        # Extract intent\n        intent_probs = torch.softmax(outputs[\'intent_logits\'], dim=-1)\n        predicted_intent = torch.argmax(intent_probs, dim=-1).item()\n\n        # Generate action sequence\n        action_sequence = self.generate_action_sequence(\n            outputs[\'action_repr\'],\n            command\n        )\n\n        return {\n            \'intent\': predicted_intent,\n            \'action_sequence\': action_sequence,\n            \'confidence\': intent_probs[0][predicted_intent].item()\n        }\n\n    def generate_action_sequence(self, action_repr: torch.Tensor, command: str) -> List[Dict[str, Any]]:\n        """Generate a sequence of ROS 2 actions from command and representation"""\n        # This is a simplified example - in practice, this would be more sophisticated\n        actions = []\n\n        if "move" in command.lower() or "go" in command.lower() or "navigate" in command.lower():\n            actions.append({\n                \'action\': \'move_base\',\n                \'target\': self.extract_location(command),\n                \'parameters\': {}\n            })\n        elif "pick" in command.lower() or "grasp" in command.lower():\n            actions.append({\n                \'action\': \'pick_object\',\n                \'target\': self.extract_object(command),\n                \'parameters\': {}\n            })\n        elif "place" in command.lower() or "put" in command.lower():\n            actions.append({\n                \'action\': \'place_object\',\n                \'target\': self.extract_location(command),\n                \'parameters\': {}\n            })\n\n        return actions\n\n    def extract_location(self, command: str) -> str:\n        """Extract location information from command"""\n        # Simple keyword-based extraction (in practice, use more sophisticated NLP)\n        locations = ["kitchen", "living room", "bedroom", "office", "dining room", "bathroom"]\n        for loc in locations:\n            if loc in command.lower():\n                return loc\n        return "unknown_location"\n\n    def extract_object(self, command: str) -> str:\n        """Extract object information from command"""\n        # Simple keyword-based extraction\n        objects = ["cup", "mug", "bottle", "book", "phone", "keys", "plate", "fork"]\n        for obj in objects:\n            if obj in command.lower():\n                # Extract color if present\n                colors = ["red", "blue", "green", "yellow", "black", "white"]\n                for color in colors:\n                    if color in command.lower():\n                        return f"{color} {obj}"\n                return obj\n        return "unknown_object"\n'})}),"\n",(0,s.jsx)(n.h3,{id:"vision-language-models",children:"Vision-Language Models"}),"\n",(0,s.jsx)(n.p,{children:"For more sophisticated robotic applications, vision-language models combine visual perception with language understanding:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from transformers import VisionEncoderDecoderModel, ViTImageProcessor\nimport cv2\nimport numpy as np\n\nclass VisionLanguageRobotModel(nn.Module):\n    """Vision-language model for robotic applications"""\n\n    def __init__(self, vision_model_name: str = "google/vit-base-patch16-224",\n                 text_model_name: str = "gpt2"):\n        super().__init__()\n\n        # Vision encoder\n        self.vision_encoder = VisionEncoderDecoderModel.from_pretrained(\n            "nlpconnect/vit-gpt2-image-captioning"\n        )\n        self.image_processor = ViTImageProcessor.from_pretrained(vision_model_name)\n\n        # Language model for command interpretation\n        self.language_model = RobotLanguageModel(text_model_name)\n\n        # Cross-modal attention for vision-language fusion\n        self.cross_attention = nn.MultiheadAttention(\n            embed_dim=768,  # Typical hidden size\n            num_heads=8,\n            batch_first=True\n        )\n\n        # Fusion layer\n        self.fusion_layer = nn.Linear(768 * 2, 768)\n\n    def forward(self, images: torch.Tensor, commands: List[str]) -> Dict[str, torch.Tensor]:\n        """Process visual and linguistic inputs jointly"""\n        batch_size = images.size(0)\n\n        # Process images\n        visual_features = self.vision_encoder.encoder(images)  # Simplified\n\n        # Process commands\n        command_features = []\n        for cmd in commands:\n            inputs = self.language_model.tokenizer(\n                cmd, return_tensors="pt", padding=True, truncation=True\n            )\n            cmd_outputs = self.language_model.transformer(**inputs)\n            command_features.append(cmd_outputs.last_hidden_state[:, -1, :])  # [hidden_size]\n\n        command_features = torch.stack(command_features)  # [batch, hidden_size]\n\n        # Apply cross-attention between visual and language features\n        attended_features, attention_weights = self.cross_attention(\n            query=command_features.unsqueeze(1),  # [batch, 1, hidden_size]\n            key=visual_features,  # [batch, seq_len, hidden_size]\n            value=visual_features\n        )\n\n        # Fuse features\n        fused_features = torch.cat([\n            command_features,\n            attended_features.squeeze(1)\n        ], dim=-1)  # [batch, 2*hidden_size]\n\n        fused_output = self.fusion_layer(fused_features)  # [batch, hidden_size]\n\n        return {\n            \'fused_features\': fused_output,\n            \'visual_features\': visual_features,\n            \'language_features\': command_features,\n            \'attention_weights\': attention_weights\n        }\n\n    def process_robot_task(self, image: np.ndarray, command: str) -> Dict[str, Any]:\n        """Process a robot task with both visual and linguistic inputs"""\n        # Preprocess image\n        image_tensor = self.preprocess_image(image)\n\n        # Get fused representation\n        outputs = self.forward(image_tensor.unsqueeze(0), [command])\n\n        # Parse command using fused representation\n        action_sequence = self.language_model.generate_action_sequence(\n            outputs[\'fused_features\'], command\n        )\n\n        return {\n            \'action_sequence\': action_sequence,\n            \'visual_attention\': outputs[\'attention_weights\'],\n            \'confidence\': 0.9  # Placeholder confidence\n        }\n\n    def preprocess_image(self, image: np.ndarray) -> torch.Tensor:\n        """Preprocess image for the vision model"""\n        # Convert BGR to RGB if needed\n        if len(image.shape) == 3 and image.shape[2] == 3:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # Resize and normalize\n        image_resized = cv2.resize(image, (224, 224))\n        image_normalized = image_resized.astype(np.float32) / 255.0\n        image_tensor = torch.from_numpy(image_normalized).permute(2, 0, 1).unsqueeze(0)\n\n        return image_tensor\n'})}),"\n",(0,s.jsx)(n.h2,{id:"ros-2-integration-patterns",children:"ROS 2 Integration Patterns"}),"\n",(0,s.jsx)(n.h3,{id:"cognitive-node-architecture",children:"Cognitive Node Architecture"}),"\n",(0,s.jsx)(n.p,{children:"The cognitive node architecture provides the foundation for integrating LLMs with ROS 2 systems:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class CognitiveNode(Node):\n    """ROS 2 node for LLM-based cognitive processing"""\n\n    def __init__(self):\n        super().__init__(\'cognitive_node\')\n\n        # Initialize LLM model\n        self.llm_model = RobotLanguageModel()\n\n        # ROS 2 interfaces\n        self.command_subscriber = self.create_subscription(\n            String,\n            \'robot_commands\',\n            self.command_callback,\n            10\n        )\n\n        self.response_publisher = self.create_publisher(\n            String,\n            \'cognitive_response\',\n            10\n        )\n\n        self.action_publisher = self.create_publisher(\n            String,  # In practice, use action-specific message types\n            \'cognitive_actions\',\n            10\n        )\n\n        # Service server for complex queries\n        self.query_service = self.create_service(\n            String,\n            \'cognitive_query\',\n            self.query_callback\n        )\n\n        # Action clients for robot execution\n        self.action_clients = {}\n\n        # Internal state\n        self.current_task = None\n        self.task_queue = []\n        self.context_history = []\n\n        # Non-blocking processing timer\n        self.process_timer = self.create_timer(0.1, self.process_tasks)\n\n        self.get_logger().info(\'Cognitive node initialized\')\n\n    def command_callback(self, msg: String):\n        """Handle incoming natural language commands"""\n        command = msg.data\n        self.get_logger().info(f\'Received command: {command}\')\n\n        # Add to processing queue\n        self.task_queue.append({\n            \'command\': command,\n            \'timestamp\': self.get_clock().now(),\n            \'source\': \'subscriber\'\n        })\n\n    def query_callback(self, request: String, response):\n        """Handle cognitive query service requests"""\n        try:\n            # Process the query with LLM\n            result = self.llm_model.parse_command(request.data)\n\n            # Format response\n            response.data = f"Intent: {result[\'intent\']}, Actions: {result[\'action_sequence\']}"\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing query: {e}\')\n            response.data = f"Error: {str(e)}"\n\n        return response\n\n    def process_tasks(self):\n        """Process tasks in the queue"""\n        if self.task_queue:\n            task = self.task_queue.pop(0)\n\n            try:\n                # Process command with LLM\n                result = self.llm_model.parse_command(task[\'command\'])\n\n                if result[\'confidence\'] > 0.7:  # Confidence threshold\n                    # Publish action sequence\n                    action_msg = String()\n                    action_msg.data = str(result[\'action_sequence\'])\n                    self.action_publisher.publish(action_msg)\n\n                    # Publish response\n                    response_msg = String()\n                    response_msg.data = f"Processing: {task[\'command\']}"\n                    self.response_publisher.publish(response_msg)\n\n                    self.get_logger().info(f\'Executed actions: {result["action_sequence"]}\')\n                else:\n                    self.get_logger().warn(f\'Low confidence for command: {task["command"]}\')\n\n            except Exception as e:\n                self.get_logger().error(f\'Error processing task: {e}\')\n                error_msg = String()\n                error_msg.data = f"Error: {str(e)}"\n                self.response_publisher.publish(error_msg)\n\n    def execute_action_sequence(self, actions: List[Dict[str, Any]]):\n        """Execute a sequence of actions"""\n        for action in actions:\n            try:\n                self.execute_single_action(action)\n            except Exception as e:\n                self.get_logger().error(f\'Error executing action {action}: {e}\')\n                break\n\n    def execute_single_action(self, action: Dict[str, Any]):\n        """Execute a single action"""\n        action_type = action[\'action\']\n\n        if action_type == \'move_base\':\n            self.execute_navigation_action(action)\n        elif action_type == \'pick_object\':\n            self.execute_manipulation_action(action)\n        elif action_type == \'place_object\':\n            self.execute_manipulation_action(action)\n        else:\n            self.get_logger().warn(f\'Unknown action type: {action_type}\')\n\n    def execute_navigation_action(self, action: Dict[str, Any]):\n        """Execute navigation-related actions"""\n        # Implementation would use navigation2 stack\n        pass\n\n    def execute_manipulation_action(self, action: Dict[str, Any]):\n        """Execute manipulation-related actions"""\n        # Implementation would use moveit2 or similar\n        pass\n'})}),"\n",(0,s.jsx)(n.h3,{id:"context-management-and-memory",children:"Context Management and Memory"}),"\n",(0,s.jsx)(n.p,{children:"Cognitive robots need to maintain context and memory across interactions:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class ContextManager:\n    """Manage context and memory for cognitive robots"""\n\n    def __init__(self, max_history: int = 100):\n        self.max_history = max_history\n        self.conversation_history = []\n        self.object_locations = {}  # Object -> location mapping\n        self.robot_state = {}  # Current robot state\n        self.user_preferences = {}  # User preferences and habits\n\n    def add_interaction(self, user_input: str, robot_response: str, timestamp: float = None):\n        """Add an interaction to the conversation history"""\n        if timestamp is None:\n            import time\n            timestamp = time.time()\n\n        interaction = {\n            \'timestamp\': timestamp,\n            \'user_input\': user_input,\n            \'robot_response\': robot_response,\n            \'entities\': self.extract_entities(user_input)\n        }\n\n        self.conversation_history.append(interaction)\n\n        # Trim history if needed\n        if len(self.conversation_history) > self.max_history:\n            self.conversation_history = self.conversation_history[-self.max_history:]\n\n    def extract_entities(self, text: str) -> Dict[str, List[str]]:\n        """Extract named entities from text"""\n        entities = {\n            \'objects\': [],\n            \'locations\': [],\n            \'people\': [],\n            \'times\': []\n        }\n\n        # Simple keyword-based extraction (in practice, use NER models)\n        objects = ["cup", "mug", "bottle", "book", "phone", "keys", "plate", "fork", "spoon"]\n        locations = ["kitchen", "living room", "bedroom", "office", "dining room", "bathroom"]\n\n        for obj in objects:\n            if obj in text.lower():\n                entities[\'objects\'].append(obj)\n\n        for loc in locations:\n            if loc in text.lower():\n                entities[\'locations\'].append(loc)\n\n        return entities\n\n    def update_object_location(self, obj: str, location: str, confidence: float = 1.0):\n        """Update the known location of an object"""\n        self.object_locations[obj] = {\n            \'location\': location,\n            \'confidence\': confidence,\n            \'timestamp\': time.time()\n        }\n\n    def get_context_prompt(self, current_command: str) -> str:\n        """Generate context prompt for LLM"""\n        # Build context from recent interactions\n        recent_interactions = self.conversation_history[-5:]  # Last 5 interactions\n\n        context_parts = []\n\n        # Add recent conversation history\n        if recent_interactions:\n            context_parts.append("Recent conversation:")\n            for interaction in recent_interactions:\n                context_parts.append(f"User: {interaction[\'user_input\']}")\n                context_parts.append(f"Robot: {interaction[\'robot_response\']}")\n\n        # Add known object locations\n        if self.object_locations:\n            context_parts.append("\\nKnown object locations:")\n            for obj, info in self.object_locations.items():\n                if time.time() - info[\'timestamp\'] < 3600:  # Less than 1 hour old\n                    context_parts.append(f"- {obj} is in {info[\'location\']} (confidence: {info[\'confidence\']:.2f})")\n\n        # Add current command\n        context_parts.append(f"\\nCurrent command: {current_command}")\n        context_parts.append("Please generate appropriate robot actions based on the context above.")\n\n        return "\\n".join(context_parts)\n\n    def get_relevant_context(self, query: str) -> str:\n        """Get context relevant to a specific query"""\n        # Simple keyword-based relevance (in practice, use semantic search)\n        relevant_parts = []\n\n        # Check conversation history for relevant objects/locations\n        keywords = query.lower().split()\n        for interaction in self.conversation_history[-10:]:  # Check last 10 interactions\n            interaction_text = f"{interaction[\'user_input\']} {interaction[\'robot_response\']}".lower()\n            if any(keyword in interaction_text for keyword in keywords):\n                relevant_parts.append(f"Context: {interaction[\'user_input\']} -> {interaction[\'robot_response\']}")\n\n        return "\\n".join(relevant_parts) if relevant_parts else "No relevant context found."\n\nclass MemoryEnhancedCognitiveNode(CognitiveNode):\n    """Cognitive node with enhanced memory and context management"""\n\n    def __init__(self):\n        super().__init__()\n\n        # Initialize context manager\n        self.context_manager = ContextManager()\n\n        # Service for context queries\n        self.context_service = self.create_service(\n            String,\n            \'context_query\',\n            self.context_callback\n        )\n\n    def command_callback(self, msg: String):\n        """Handle commands with context"""\n        command = msg.data\n\n        # Get relevant context\n        context_prompt = self.context_manager.get_context_prompt(command)\n\n        # Process with context\n        full_prompt = f"{context_prompt}\\n\\nCommand: {command}"\n\n        # For now, just pass to parent method, but in practice you\'d use the context\n        super().command_callback(msg)\n\n        # Add to context history\n        self.context_manager.add_interaction(command, "Processing...")\n\n    def context_callback(self, request: String, response):\n        """Handle context queries"""\n        context = self.context_manager.get_relevant_context(request.data)\n        response.data = context\n        return response\n'})}),"\n",(0,s.jsx)(n.h2,{id:"task-planning-and-execution",children:"Task Planning and Execution"}),"\n",(0,s.jsx)(n.h3,{id:"hierarchical-task-networks",children:"Hierarchical Task Networks"}),"\n",(0,s.jsx)(n.p,{children:"LLM integration enables sophisticated task planning using hierarchical structures:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class HierarchicalTaskPlanner:\n    """Hierarchical task planner using LLM for decomposition"""\n\n    def __init__(self, llm_model: RobotLanguageModel):\n        self.llm_model = llm_model\n        self.task_library = self.initialize_task_library()\n\n    def initialize_task_library(self) -> Dict[str, Any]:\n        """Initialize the library of known tasks and their decompositions"""\n        return {\n            \'fetch_object\': {\n                \'description\': \'Fetch an object from a location\',\n                \'subtasks\': [\'navigate_to_location\', \'identify_object\', \'grasp_object\', \'navigate_to_destination\', \'place_object\']\n            },\n            \'room_cleanup\': {\n                \'description\': \'Clean up a room by putting objects in their proper places\',\n                \'subtasks\': [\'scan_room\', \'identify_misplaced_objects\', \'classify_objects\', \'plan_collection_sequence\', \'collect_and_store_objects\']\n            },\n            \'guided_tour\': {\n                \'description\': \'Give a guided tour of a location\',\n                \'subtasks\': [\'plan_route\', \'navigate_to_point_of_interest\', \'provide_information\', \'wait_for_attention\', \'move_to_next_point\']\n            }\n        }\n\n    def decompose_task(self, high_level_task: str) -> List[Dict[str, Any]]:\n        """Decompose a high-level task into executable subtasks"""\n        # Check if task is in library\n        if high_level_task in self.task_library:\n            subtasks = self.task_library[high_level_task][\'subtasks\']\n            return [{\'task\': subtask, \'parameters\': {}} for subtask in subtasks]\n\n        # Use LLM to decompose novel tasks\n        return self.llm_decompose_task(high_level_task)\n\n    def llm_decompose_task(self, task_description: str) -> List[Dict[str, Any]]:\n        """Use LLM to decompose a task description into subtasks"""\n        prompt = f"""\n        Decompose the following high-level task into specific, executable subtasks for a robot:\n        Task: {task_description}\n\n        Provide the subtasks as a list of dictionaries with \'task\' and \'parameters\' keys.\n        Example format:\n        [\n            {{"task": "navigate_to", "parameters": {{"location": "kitchen"}}}},\n            {{"task": "detect_object", "parameters": {{"object_type": "cup"}}}},\n            {{"task": "grasp_object", "parameters": {{"object_id": "detected_cup"}}}}\n        ]\n\n        Return only the JSON list, no other text.\n        """\n\n        # In practice, this would call the LLM\n        # For this example, return a simple decomposition\n        if "fetch" in task_description.lower() or "bring" in task_description.lower():\n            return [\n                {"task": "navigate_to", "parameters": {"location": self.extract_location(task_description)}},\n                {"task": "detect_object", "parameters": {"object_type": self.extract_object(task_description)}},\n                {"task": "grasp_object", "parameters": {"object_id": "detected_object"}},\n                {"task": "navigate_to", "parameters": {"location": "delivery_location"}},\n                {"task": "place_object", "parameters": {"placement_location": "delivery_surface"}}\n            ]\n        else:\n            return [{"task": "unknown_task", "parameters": {"description": task_description}}]\n\n    def extract_location(self, task: str) -> str:\n        """Extract location from task description"""\n        # Simple extraction (in practice, use NLP)\n        locations = ["kitchen", "living room", "bedroom", "office", "dining room"]\n        for loc in locations:\n            if loc in task.lower():\n                return loc\n        return "unknown"\n\n    def extract_object(self, task: str) -> str:\n        """Extract object from task description"""\n        # Simple extraction (in practice, use NLP)\n        objects = ["cup", "mug", "bottle", "book", "phone", "keys"]\n        for obj in objects:\n            if obj in task.lower():\n                return obj\n        return "unknown"\n\nclass TaskExecutionManager:\n    """Manage the execution of hierarchical tasks"""\n\n    def __init__(self, ros_node: Node):\n        self.ros_node = ros_node\n        self.planner = HierarchicalTaskPlanner(None)  # Will be initialized with actual model\n        self.active_tasks = []\n        self.task_status = {}  # task_id -> status\n\n    def execute_task(self, task_description: str) -> str:\n        """Execute a high-level task by decomposing and running subtasks"""\n        task_id = f"task_{int(time.time())}"\n\n        # Decompose task\n        subtasks = self.planner.decompose_task(task_description)\n\n        # Execute subtasks sequentially\n        for i, subtask in enumerate(subtasks):\n            self.ros_node.get_logger().info(f\'Executing subtask {i+1}/{len(subtasks)}: {subtask["task"]}\')\n\n            success = self.execute_subtask(subtask)\n\n            if not success:\n                self.ros_node.get_logger().error(f\'Subtask failed: {subtask}\')\n                return f"Task failed at subtask {i+1}: {subtask[\'task\']}"\n\n        return f"Task completed successfully: {task_description}"\n\n    def execute_subtask(self, subtask: Dict[str, Any]) -> bool:\n        """Execute a single subtask"""\n        task_type = subtask[\'task\']\n        params = subtask[\'parameters\']\n\n        try:\n            if task_type == \'navigate_to\':\n                return self.execute_navigation(params)\n            elif task_type == \'detect_object\':\n                return self.execute_detection(params)\n            elif task_type == \'grasp_object\':\n                return self.execute_grasping(params)\n            elif task_type == \'place_object\':\n                return self.execute_placement(params)\n            else:\n                self.ros_node.get_logger().warn(f\'Unknown subtask type: {task_type}\')\n                return False\n        except Exception as e:\n            self.ros_node.get_logger().error(f\'Error executing subtask {subtask}: {e}\')\n            return False\n\n    def execute_navigation(self, params: Dict[str, Any]) -> bool:\n        """Execute navigation subtask"""\n        # Implementation would use navigation2\n        location = params.get(\'location\', \'unknown\')\n        self.ros_node.get_logger().info(f\'Navigating to {location}\')\n        # In practice, send goal to navigation system\n        return True\n\n    def execute_detection(self, params: Dict[str, Any]) -> bool:\n        """Execute object detection subtask"""\n        obj_type = params.get(\'object_type\', \'unknown\')\n        self.ros_node.get_logger().info(f\'Detecting {obj_type}\')\n        # In practice, use perception system\n        return True\n\n    def execute_grasping(self, params: Dict[str, Any]) -> bool:\n        """Execute grasping subtask"""\n        obj_id = params.get(\'object_id\', \'unknown\')\n        self.ros_node.get_logger().info(f\'Grasping {obj_id}\')\n        # In practice, use manipulation system\n        return True\n\n    def execute_placement(self, params: Dict[str, Any]) -> bool:\n        """Execute placement subtask"""\n        location = params.get(\'placement_location\', \'default\')\n        self.ros_node.get_logger().info(f\'Placing object at {location}\')\n        # In practice, use manipulation system\n        return True\n'})}),"\n",(0,s.jsx)(n.h2,{id:"safety-and-validation",children:"Safety and Validation"}),"\n",(0,s.jsx)(n.h3,{id:"confidence-assessment-and-validation",children:"Confidence Assessment and Validation"}),"\n",(0,s.jsx)(n.p,{children:"LLM outputs must be validated before execution to ensure safety:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class ConfidenceAssessor:\n    \"\"\"Assess confidence in LLM outputs for robotic execution\"\"\"\n\n    def __init__(self, min_confidence: float = 0.7):\n        self.min_confidence = min_confidence\n        self.action_validator = ActionValidator()\n\n    def assess_command(self, command: str, llm_output: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Assess confidence and safety of LLM command interpretation\"\"\"\n        results = {\n            'confidence': self.calculate_confidence(command, llm_output),\n            'safe_to_execute': False,\n            'validation_errors': [],\n            'suggested_corrections': []\n        }\n\n        # Validate action sequence\n        validation_result = self.action_validator.validate_action_sequence(\n            llm_output.get('action_sequence', [])\n        )\n\n        results['validation_errors'] = validation_result['errors']\n        results['suggested_corrections'] = validation_result['corrections']\n\n        # Check if command is safe to execute\n        results['safe_to_execute'] = (\n            results['confidence'] >= self.min_confidence and\n            not validation_result['has_critical_errors']\n        )\n\n        return results\n\n    def calculate_confidence(self, command: str, llm_output: Dict[str, Any]) -> float:\n        \"\"\"Calculate confidence score for command interpretation\"\"\"\n        # Multiple factors contribute to confidence:\n\n        # 1. LLM's own confidence (if provided)\n        llm_confidence = llm_output.get('confidence', 0.5)\n\n        # 2. Command clarity (length, keywords, structure)\n        clarity_score = self.assess_command_clarity(command)\n\n        # 3. Action feasibility\n        feasibility_score = self.assess_action_feasibility(llm_output.get('action_sequence', []))\n\n        # Weighted average\n        confidence = 0.5 * llm_confidence + 0.3 * clarity_score + 0.2 * feasibility_score\n\n        return min(confidence, 1.0)  # Cap at 1.0\n\n    def assess_command_clarity(self, command: str) -> float:\n        \"\"\"Assess how clear and unambiguous the command is\"\"\"\n        score = 0.5  # Base score\n\n        # Positive factors\n        if len(command.split()) >= 3:  # At least 3 words\n            score += 0.2\n        if any(word in command.lower() for word in ['the', 'a', 'an']):  # Definite articles\n            score += 0.1\n        if any(word in command.lower() for word in ['please', 'could you', 'would you']):  # Politeness\n            score += 0.1\n\n        # Negative factors\n        if '?' in command:  # Questions might not be commands\n            score -= 0.1\n        if command.lower().strip().endswith('?'):\n            score -= 0.1\n\n        return max(0.0, min(1.0, score))\n\n    def assess_action_feasibility(self, action_sequence: List[Dict[str, Any]]) -> float:\n        \"\"\"Assess whether the action sequence is feasible\"\"\"\n        if not action_sequence:\n            return 0.1  # Very low for empty sequences\n\n        feasible_actions = 0\n        total_actions = len(action_sequence)\n\n        for action in action_sequence:\n            if self.is_action_feasible(action):\n                feasible_actions += 1\n\n        return feasible_actions / total_actions if total_actions > 0 else 0.0\n\n    def is_action_feasible(self, action: Dict[str, Any]) -> bool:\n        \"\"\"Check if a single action is feasible\"\"\"\n        action_type = action.get('action', '')\n\n        # Check for obviously invalid actions\n        invalid_actions = ['destroy', 'break', 'damage', 'harm', 'attack']\n        if any(invalid in action_type.lower() for invalid in invalid_actions):\n            return False\n\n        # Check for dangerous locations\n        params = action.get('parameters', {})\n        dangerous_locations = ['cliff', 'lava', 'fire', 'dangerous_area']\n        if 'location' in params:\n            if any(danger in params['location'].lower() for danger in dangerous_locations):\n                return False\n\n        return True\n\nclass ActionValidator:\n    \"\"\"Validate robot actions for safety and feasibility\"\"\"\n\n    def __init__(self):\n        self.known_actions = {\n            'move_base', 'pick_object', 'place_object', 'grasp', 'release',\n            'navigate', 'inspect', 'follow', 'stop', 'wait', 'speak'\n        }\n        self.dangerous_objects = {'knife', 'blade', 'sharp', 'fire', 'hot'}\n        self.restricted_areas = {'restricted', 'danger', 'unsafe'}\n\n    def validate_action_sequence(self, action_sequence: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"Validate a sequence of actions\"\"\"\n        errors = []\n        corrections = []\n        critical_errors = False\n\n        for i, action in enumerate(action_sequence):\n            action_errors, action_corrections, is_critical = self.validate_action(action, i)\n            errors.extend(action_errors)\n            corrections.extend(action_corrections)\n            if is_critical:\n                critical_errors = True\n\n        return {\n            'errors': errors,\n            'corrections': corrections,\n            'has_critical_errors': critical_errors\n        }\n\n    def validate_action(self, action: Dict[str, Any], index: int) -> tuple:\n        \"\"\"Validate a single action\"\"\"\n        errors = []\n        corrections = []\n        is_critical = False\n\n        # Check if action type is known\n        action_type = action.get('action', '').lower()\n        if action_type not in self.known_actions:\n            errors.append(f\"Unknown action type '{action_type}' at index {index}\")\n            is_critical = True\n\n        # Check parameters\n        params = action.get('parameters', {})\n\n        # Validate object safety\n        obj = params.get('object', '').lower()\n        if any(danger in obj for danger in self.dangerous_objects):\n            errors.append(f\"Potentially dangerous object '{obj}' at index {index}\")\n            is_critical = True\n\n        # Validate location safety\n        location = params.get('location', '').lower()\n        if any(restricted in location for restricted in self.restricted_areas):\n            errors.append(f\"Restricted location '{location}' at index {index}\")\n            is_critical = True\n\n        # Check for required parameters\n        if action_type in ['move_base', 'navigate']:\n            if 'location' not in params:\n                errors.append(f\"Missing location parameter for {action_type} at index {index}\")\n                is_critical = True\n\n        if action_type in ['pick_object', 'grasp']:\n            if 'object' not in params and 'object_id' not in params:\n                errors.append(f\"Missing object parameter for {action_type} at index {index}\")\n                is_critical = True\n\n        return errors, corrections, is_critical\n\nclass SafeCognitiveNode(CognitiveNode):\n    \"\"\"Cognitive node with safety validation\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n        # Initialize safety components\n        self.confidence_assessor = ConfidenceAssessor(min_confidence=0.7)\n        self.task_executor = TaskExecutionManager(self)\n\n        # Safety service\n        self.safety_service = self.create_service(\n            String,\n            'safety_check',\n            self.safety_callback\n        )\n\n    def command_callback(self, msg: String):\n        \"\"\"Handle commands with safety validation\"\"\"\n        command = msg.data\n\n        try:\n            # Parse command with LLM\n            result = self.llm_model.parse_command(command)\n\n            # Assess confidence and safety\n            safety_check = self.confidence_assessor.assess_command(command, result)\n\n            if safety_check['safe_to_execute']:\n                # Execute the task\n                task_result = self.task_executor.execute_task(command)\n\n                # Publish success\n                response_msg = String()\n                response_msg.data = f\"Success: {task_result}\"\n                self.response_publisher.publish(response_msg)\n\n                self.get_logger().info(f'Executed command safely: {command}')\n            else:\n                # Report safety issues\n                error_msg = String()\n                error_msg.data = f\"Safety validation failed: {safety_check['validation_errors']}\"\n                self.response_publisher.publish(error_msg)\n\n                self.get_logger().warn(f'Safety validation failed for command: {command}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing command safely: {e}')\n            error_msg = String()\n            error_msg.data = f\"Error: {str(e)}\"\n            self.response_publisher.publish(error_msg)\n\n    def safety_callback(self, request: String, response):\n        \"\"\"Handle safety check requests\"\"\"\n        try:\n            result = self.llm_model.parse_command(request.data)\n            safety_check = self.confidence_assessor.assess_command(request.data, result)\n\n            response.data = str(safety_check)\n        except Exception as e:\n            response.data = f\"Error in safety check: {str(e)}\"\n\n        return response\n"})}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"caching-and-efficient-processing",children:"Caching and Efficient Processing"}),"\n",(0,s.jsx)(n.p,{children:"To maintain real-time performance, cognitive systems need efficient processing mechanisms:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from functools import lru_cache\nimport threading\nfrom queue import Queue, Empty\nimport asyncio\n\nclass OptimizedCognitiveNode(SafeCognitiveNode):\n    """Cognitive node with performance optimizations"""\n\n    def __init__(self):\n        super().__init__()\n\n        # LLM response cache\n        self.response_cache = {}\n        self.cache_lock = threading.Lock()\n\n        # Async processing queue\n        self.processing_queue = Queue()\n        self.result_queue = Queue()\n\n        # Start processing thread\n        self.processing_thread = threading.Thread(target=self.process_commands_async, daemon=True)\n        self.processing_thread.start()\n\n    @lru_cache(maxsize=128)\n    def cached_llm_process(self, command: str) -> Dict[str, Any]:\n        """Cached LLM processing for repeated commands"""\n        return self.llm_model.parse_command(command)\n\n    def command_callback(self, msg: String):\n        """Handle commands with caching and async processing"""\n        command = msg.data\n\n        # Check cache first\n        with self.cache_lock:\n            if command in self.response_cache:\n                cached_result = self.response_cache[command]\n                # Validate cache freshness if needed\n                if time.time() - cached_result[\'timestamp\'] < 300:  # 5 minutes\n                    self.handle_cached_result(command, cached_result[\'result\'])\n                    return\n\n        # Add to processing queue\n        self.processing_queue.put({\n            \'command\': command,\n            \'timestamp\': time.time()\n        })\n\n    def process_commands_async(self):\n        """Process commands asynchronously"""\n        while rclpy.ok():\n            try:\n                task = self.processing_queue.get(timeout=0.1)\n                command = task[\'command\']\n\n                # Process with LLM\n                result = self.cached_llm_process(command)\n\n                # Add to response queue\n                self.result_queue.put({\n                    \'command\': command,\n                    \'result\': result,\n                    \'timestamp\': task[\'timestamp\']\n                })\n\n            except Empty:\n                continue\n            except Exception as e:\n                self.get_logger().error(f\'Error in async processing: {e}\')\n\n    def handle_cached_result(self, command: str, result: Dict[str, Any]):\n        """Handle a cached LLM result"""\n        safety_check = self.confidence_assessor.assess_command(command, result)\n\n        if safety_check[\'safe_to_execute\']:\n            # Execute the task\n            task_result = self.task_executor.execute_task(command)\n\n            response_msg = String()\n            response_msg.data = f"Success: {task_result}"\n            self.response_publisher.publish(response_msg)\n        else:\n            error_msg = String()\n            error_msg.data = f"Safety validation failed: {safety_check[\'validation_errors\']}"\n            self.response_publisher.publish(error_msg)\n\nclass BatchProcessingCognitiveNode(OptimizedCognitiveNode):\n    """Cognitive node with batch processing capabilities"""\n\n    def __init__(self):\n        super().__init__()\n\n        # Batch processing parameters\n        self.batch_size = 5\n        self.batch_timeout = 1.0  # seconds\n        self.command_batch = []\n        self.batch_timer = None\n\n    def command_callback(self, msg: String):\n        """Collect commands for batch processing"""\n        command = msg.data\n        self.command_batch.append({\n            \'command\': command,\n            \'msg\': msg,\n            \'timestamp\': time.time()\n        })\n\n        # Start timer if first command in batch\n        if len(self.command_batch) == 1:\n            self.batch_timer = self.create_timer(\n                self.batch_timeout,\n                self.process_batch\n            )\n\n        # Process batch if it reaches the limit\n        if len(self.command_batch) >= self.batch_size:\n            self.process_batch()\n\n    def process_batch(self):\n        """Process a batch of commands"""\n        if self.batch_timer:\n            self.destroy_timer(self.batch_timer)\n            self.batch_timer = None\n\n        if not self.command_batch:\n            return\n\n        commands = [item[\'command\'] for item in self.command_batch]\n\n        # Process all commands at once (if model supports batching)\n        try:\n            batch_results = self.process_command_batch(commands)\n\n            # Handle each result\n            for i, result in enumerate(batch_results):\n                original_item = self.command_batch[i]\n                self.handle_single_result(original_item[\'command\'], result)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in batch processing: {e}\')\n\n        # Clear batch\n        self.command_batch = []\n\n    def process_command_batch(self, commands: List[str]) -> List[Dict[str, Any]]:\n        """Process a batch of commands (simplified - in practice, use model\'s batch capabilities)"""\n        return [self.llm_model.parse_command(cmd) for cmd in commands]\n'})}),"\n",(0,s.jsx)(n.p,{children:"LLM integration with ROS 2 enables cognitive robotics systems that can understand natural language commands and execute complex tasks. The integration requires careful consideration of safety, real-time performance, and the distributed nature of ROS 2 systems. By implementing proper validation, caching, and safety mechanisms, robots can leverage the power of large language models while maintaining reliability and safety in real-world applications."}),"\n",(0,s.jsx)(n.p,{children:"The key to successful LLM integration lies in creating robust interfaces between high-level language understanding and low-level robot control, with appropriate validation and safety checks at each step of the process."})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>r});var s=t(6540);const o={},a=s.createContext(o);function i(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);