"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[2378],{7676:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>c});var t=i(4848),o=i(8453);const a={title:"Chapter 8 - Unity Visualization & Human-Robot Interaction"},r="Chapter 8: Unity Visualization & Human-Robot Interaction",s={id:"physical-ai/unity-visualization/index",title:"Chapter 8 - Unity Visualization & Human-Robot Interaction",description:"Chapter Overview",source:"@site/docs/physical-ai/unity-visualization/index.mdx",sourceDirName:"physical-ai/unity-visualization",slug:"/physical-ai/unity-visualization/",permalink:"/physical-ai-textbook/physical-ai/physical-ai/unity-visualization/",draft:!1,unlisted:!1,editUrl:"https://github.com/your-username/physical-ai-textbook/tree/main/docs/physical-ai/unity-visualization/index.mdx",tags:[],version:"current",frontMatter:{title:"Chapter 8 - Unity Visualization & Human-Robot Interaction"},sidebar:"tutorialSidebar",previous:{title:"Chapter 7 Summary - Gazebo Simulation: Physics, Sensors & World Building",permalink:"/physical-ai-textbook/physical-ai/physical-ai/gazebo-simulation/chapter-summary"},next:{title:"Chapter 8 - Human-Robot Interaction Design Principles",permalink:"/physical-ai-textbook/physical-ai/physical-ai/unity-visualization/human-robot-interaction"}},l={},c=[{value:"Chapter Overview",id:"chapter-overview",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Introduction to Unity for Robotics",id:"introduction-to-unity-for-robotics",level:2},{value:"Unity in the Robotics Ecosystem",id:"unity-in-the-robotics-ecosystem",level:3},{value:"Unity Robotics Framework",id:"unity-robotics-framework",level:3},{value:"Setting Up Unity for Robotics Applications",id:"setting-up-unity-for-robotics-applications",level:2},{value:"Unity Installation and Configuration",id:"unity-installation-and-configuration",level:3},{value:"ROS 2 Integration Setup",id:"ros-2-integration-setup",level:3},{value:"Unity Visualization Techniques for Robotics",id:"unity-visualization-techniques-for-robotics",level:2},{value:"Robot Model Visualization",id:"robot-model-visualization",level:3},{value:"Sensor Data Visualization",id:"sensor-data-visualization",level:3},{value:"Human-Robot Interaction in Unity",id:"human-robot-interaction-in-unity",level:2},{value:"Interface Design Principles",id:"interface-design-principles",level:3},{value:"Teleoperation Interfaces",id:"teleoperation-interfaces",level:3},{value:"Advanced Visualization Techniques",id:"advanced-visualization-techniques",level:2},{value:"Point Cloud Visualization",id:"point-cloud-visualization",level:3},{value:"Immersive VR/AR Interfaces",id:"immersive-vrar-interfaces",level:3},{value:"Unity-ROS 2 Integration Patterns",id:"unity-ros-2-integration-patterns",level:2},{value:"Publisher and Subscriber Patterns",id:"publisher-and-subscriber-patterns",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Rendering Optimization",id:"rendering-optimization",level:3},{value:"Best Practices for Unity Robotics Applications",id:"best-practices-for-unity-robotics-applications",level:2},{value:"1. Separation of Concerns",id:"1-separation-of-concerns",level:3},{value:"2. Network Resilience",id:"2-network-resilience",level:3}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.h1,{id:"chapter-8-unity-visualization--human-robot-interaction",children:"Chapter 8: Unity Visualization & Human-Robot Interaction"}),"\n",(0,t.jsx)(e.h2,{id:"chapter-overview",children:"Chapter Overview"}),"\n",(0,t.jsx)(e.p,{children:"This chapter explores the integration of Unity 3D with robotics systems for advanced visualization and human-robot interaction (HRI). Unity provides powerful real-time rendering capabilities that enable immersive visualization of robot behaviors, environments, and sensor data. We'll examine how to create compelling user interfaces, implement intuitive interaction paradigms, and leverage Unity's capabilities for teleoperation, monitoring, and collaborative robotics applications in the context of Physical AI and humanoid robots."}),"\n",(0,t.jsx)(e.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,t.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Integrate Unity with ROS 2 systems for real-time robot visualization"}),"\n",(0,t.jsx)(e.li,{children:"Design intuitive human-robot interaction interfaces using Unity"}),"\n",(0,t.jsx)(e.li,{children:"Implement teleoperation systems with Unity-based control interfaces"}),"\n",(0,t.jsx)(e.li,{children:"Create immersive visualization environments for robot monitoring and debugging"}),"\n",(0,t.jsx)(e.li,{children:"Develop collaborative interfaces that enable effective human-robot teamwork"}),"\n",(0,t.jsx)(e.li,{children:"Optimize Unity applications for real-time robotics visualization"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"introduction-to-unity-for-robotics",children:"Introduction to Unity for Robotics"}),"\n",(0,t.jsx)(e.p,{children:"Unity is a powerful cross-platform game engine that has found extensive applications in robotics for visualization, simulation, and human-robot interaction. Its real-time rendering capabilities, extensive asset ecosystem, and cross-platform support make it an ideal choice for creating sophisticated visualization and interaction systems for Physical AI applications."}),"\n",(0,t.jsx)(e.h3,{id:"unity-in-the-robotics-ecosystem",children:"Unity in the Robotics Ecosystem"}),"\n",(0,t.jsx)(e.p,{children:"Unity serves multiple roles in robotics development:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Visualization"}),": Real-time rendering of robot states, sensor data, and environments"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Simulation"}),": High-fidelity physics simulation with advanced graphics"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Human-Robot Interaction"}),": Intuitive interfaces for robot control and monitoring"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Teleoperation"}),": Immersive remote control interfaces"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Training"}),": Virtual environments for robot learning and human training"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"unity-robotics-framework",children:"Unity Robotics Framework"}),"\n",(0,t.jsx)(e.p,{children:"Unity provides the Unity Robotics Framework, which includes tools and packages specifically designed for robotics applications:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"ROS-TCP-Connector"}),": Enables communication between Unity and ROS 2"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Robotics XR Interaction Framework"}),": Tools for creating immersive VR/AR interactions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Unity Perception"}),": Tools for generating synthetic training data"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Unity Simulation"}),": High-fidelity physics simulation capabilities"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"setting-up-unity-for-robotics-applications",children:"Setting Up Unity for Robotics Applications"}),"\n",(0,t.jsx)(e.h3,{id:"unity-installation-and-configuration",children:"Unity Installation and Configuration"}),"\n",(0,t.jsx)(e.p,{children:"To set up Unity for robotics applications, you'll need:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Unity Hub"}),": For managing Unity installations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Unity Editor"}),": Latest LTS version recommended for stability"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Unity Robotics Packages"}),": For ROS integration"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"XR Packages"}),": If developing immersive interfaces"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"ros-2-integration-setup",children:"ROS 2 Integration Setup"}),"\n",(0,t.jsx)(e.p,{children:"The connection between Unity and ROS 2 typically uses TCP/IP communication:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'// Example Unity C# script for ROS 2 communication\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Std;\nusing RosMessageTypes.Geometry;\n\npublic class RobotController : MonoBehaviour\n{\n    ROSConnection ros;\n    string rosIP = "127.0.0.1"; // Default to local host\n    int rosPort = 10000;        // Default port for ROS connection\n\n    // Robot state variables\n    Vector3 robotPosition;\n    Quaternion robotRotation;\n\n    // Start is called before the first frame update\n    void Start()\n    {\n        // Get the ROS connection static instance\n        ros = ROSConnection.instance;\n\n        // Set the IP and port for the ROS connection\n        ros.Initialize(rosIP, rosPort);\n\n        // Subscribe to robot state topic\n        ros.Subscribe<OdometryMsg>("robot/odom", UpdateRobotState);\n    }\n\n    // Update robot state from ROS messages\n    void UpdateRobotState(OdometryMsg odom)\n    {\n        robotPosition = new Vector3((float)odom.pose.pose.position.x,\n                                   (float)odom.pose.pose.position.y,\n                                   (float)odom.pose.pose.position.z);\n        robotRotation = new Quaternion((float)odom.pose.pose.orientation.x,\n                                      (float)odom.pose.pose.orientation.y,\n                                      (float)odom.pose.pose.orientation.z,\n                                      (float)odom.pose.pose.orientation.w);\n\n        // Update Unity object position and rotation\n        transform.position = robotPosition;\n        transform.rotation = robotRotation;\n    }\n\n    // Send commands to robot\n    public void SendVelocityCommand(float linearX, float angularZ)\n    {\n        var twist = new TwistMsg();\n        twist.linear.x = linearX;\n        twist.angular.z = angularZ;\n\n        ros.Send("robot/cmd_vel", twist);\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h2,{id:"unity-visualization-techniques-for-robotics",children:"Unity Visualization Techniques for Robotics"}),"\n",(0,t.jsx)(e.h3,{id:"robot-model-visualization",children:"Robot Model Visualization"}),"\n",(0,t.jsx)(e.p,{children:"Creating accurate and informative robot visualizations in Unity involves several key techniques:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\n\npublic class RobotVisualizer : MonoBehaviour\n{\n    [Header("Robot Configuration")]\n    public GameObject robotModel;\n    public Transform[] jointTransforms;\n    public float[] jointPositions;\n\n    [Header("Visualization Settings")]\n    public bool showTrajectory = true;\n    public bool showSensorData = true;\n    public Color trajectoryColor = Color.blue;\n\n    private LineRenderer trajectoryLine;\n    private List<Vector3> trajectoryPoints = new List<Vector3>();\n\n    void Start()\n    {\n        SetupTrajectoryRenderer();\n    }\n\n    void SetupTrajectoryRenderer()\n    {\n        trajectoryLine = gameObject.AddComponent<LineRenderer>();\n        trajectoryLine.material = new Material(Shader.Find("Sprites/Default"));\n        trajectoryLine.color = trajectoryColor;\n        trajectoryLine.widthMultiplier = 0.1f;\n    }\n\n    void Update()\n    {\n        UpdateRobotPose();\n        UpdateTrajectory();\n        UpdateSensorVisualization();\n    }\n\n    void UpdateRobotPose()\n    {\n        // Update joint positions based on robot state\n        for (int i = 0; i < jointTransforms.Length; i++)\n        {\n            // Apply joint angles to transforms\n            jointTransforms[i].localEulerAngles = new Vector3(0, 0, jointPositions[i] * Mathf.Rad2Deg);\n        }\n    }\n\n    void UpdateTrajectory()\n    {\n        if (showTrajectory)\n        {\n            trajectoryPoints.Add(transform.position);\n\n            // Limit trajectory length\n            if (trajectoryPoints.Count > 1000)\n            {\n                trajectoryPoints.RemoveAt(0);\n            }\n\n            trajectoryLine.positionCount = trajectoryPoints.Count;\n            trajectoryLine.SetPositions(trajectoryPoints.ToArray());\n        }\n    }\n\n    void UpdateSensorVisualization()\n    {\n        if (showSensorData)\n        {\n            // Visualize sensor data like laser scans, camera feeds, etc.\n            VisualizeLaserScan();\n            VisualizeCameraFeed();\n        }\n    }\n\n    void VisualizeLaserScan()\n    {\n        // Implementation for visualizing laser scan data\n        // This would typically involve creating line objects for each laser beam\n    }\n\n    void VisualizeCameraFeed()\n    {\n        // Implementation for displaying camera feeds\n        // This could involve updating textures or UI elements\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h3,{id:"sensor-data-visualization",children:"Sensor Data Visualization"}),"\n",(0,t.jsx)(e.p,{children:"Visualizing sensor data in Unity provides valuable insights into robot perception:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\n\npublic class SensorDataVisualizer : MonoBehaviour\n{\n    [Header("Laser Scan Visualization")]\n    public GameObject laserBeamPrefab;\n    public int maxLaserPoints = 360;\n    public float maxRange = 30.0f;\n    public Color laserColor = Color.red;\n\n    [Header("Camera Feed")]\n    public Renderer cameraFeedRenderer;\n    public Material cameraMaterial;\n\n    private GameObject[] laserBeams;\n    private Vector3[] laserPoints;\n\n    void Start()\n    {\n        InitializeLaserVisualization();\n    }\n\n    void InitializeLaserVisualization()\n    {\n        laserBeams = new GameObject[maxLaserPoints];\n        laserPoints = new Vector3[maxLaserPoints];\n\n        for (int i = 0; i < maxLaserPoints; i++)\n        {\n            laserBeams[i] = Instantiate(laserBeamPrefab, transform);\n            laserBeams[i].SetActive(false);\n        }\n    }\n\n    public void UpdateLaserScan(float[] ranges, float angleMin, float angleMax)\n    {\n        float angleIncrement = (angleMax - angleMin) / ranges.Length;\n\n        for (int i = 0; i < ranges.Length && i < maxLaserPoints; i++)\n        {\n            float angle = angleMin + i * angleIncrement;\n            float range = ranges[i];\n\n            if (range < maxRange && range > 0.1f)\n            {\n                laserBeams[i].SetActive(true);\n                Vector3 direction = new Vector3(Mathf.Cos(angle), 0, Mathf.Sin(angle));\n                laserBeams[i].transform.position = transform.position;\n                laserBeams[i].transform.rotation = Quaternion.LookRotation(direction);\n                laserBeams[i].transform.localScale = new Vector3(0.05f, 0.05f, range);\n            }\n            else\n            {\n                laserBeams[i].SetActive(false);\n            }\n        }\n    }\n\n    public void UpdateCameraFeed(Texture2D cameraImage)\n    {\n        if (cameraFeedRenderer != null && cameraMaterial != null)\n        {\n            cameraMaterial.mainTexture = cameraImage;\n            cameraFeedRenderer.material = cameraMaterial;\n        }\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h2,{id:"human-robot-interaction-in-unity",children:"Human-Robot Interaction in Unity"}),"\n",(0,t.jsx)(e.h3,{id:"interface-design-principles",children:"Interface Design Principles"}),"\n",(0,t.jsx)(e.p,{children:"Creating effective human-robot interaction interfaces requires understanding both human factors and robotics principles:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Intuitive Controls"}),": Design controls that match human expectations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Clear Feedback"}),": Provide immediate and clear feedback for robot actions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Safety Considerations"}),": Implement safety checks and emergency stops"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Context Awareness"}),": Adapt interface based on robot state and environment"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"teleoperation-interfaces",children:"Teleoperation Interfaces"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing UnityEngine.UI;\n\npublic class TeleoperationInterface : MonoBehaviour\n{\n    [Header("Control Elements")]\n    public Slider linearVelocitySlider;\n    public Slider angularVelocitySlider;\n    public Button emergencyStopButton;\n    public Toggle autonomousModeToggle;\n\n    [Header("Visualization Elements")]\n    public Text velocityDisplay;\n    public Text modeDisplay;\n    public Image joystickArea;\n    public GameObject joystickHandle;\n\n    [Header("Robot Connection")]\n    public RobotController robotController;\n\n    private Vector2 joystickInput;\n    private bool isDragging = false;\n    private Vector2 dragStartPosition;\n\n    void Start()\n    {\n        SetupEventHandlers();\n        SetupJoystick();\n    }\n\n    void SetupEventHandlers()\n    {\n        linearVelocitySlider.onValueChanged.AddListener(OnLinearVelocityChanged);\n        angularVelocitySlider.onValueChanged.AddListener(OnAngularVelocityChanged);\n        emergencyStopButton.onClick.AddListener(OnEmergencyStop);\n        autonomousModeToggle.onValueChanged.AddListener(OnAutonomousModeChanged);\n    }\n\n    void SetupJoystick()\n    {\n        // Set up joystick drag events\n        EventTrigger joystickTrigger = joystickArea.GetComponent<EventTrigger>();\n        if (joystickTrigger == null)\n        {\n            joystickTrigger = joystickArea.gameObject.AddComponent<EventTrigger>();\n        }\n\n        EventTrigger.Entry entryDown = new EventTrigger.Entry();\n        entryDown.eventID = EventTriggerType.PointerDown;\n        entryDown.callback.AddListener((data) => { OnJoystickDown((PointerEventData)data); });\n        joystickTrigger.triggers.Add(entryDown);\n\n        EventTrigger.Entry entryDrag = new EventTrigger.Entry();\n        entryDrag.eventID = EventTriggerType.Drag;\n        entryDrag.callback.AddListener((data) => { OnJoystickDrag((PointerEventData)data); });\n        joystickTrigger.triggers.Add(entryDrag);\n\n        EventTrigger.Entry entryUp = new EventTrigger.Entry();\n        entryUp.eventID = EventTriggerType.PointerUp;\n        entryUp.callback.AddListener((data) => { OnJoystickUp((PointerEventData)data); });\n        joystickTrigger.triggers.Add(entryUp);\n    }\n\n    void OnLinearVelocityChanged(float value)\n    {\n        UpdateVelocityDisplay();\n        if (robotController != null && !autonomousModeToggle.isOn)\n        {\n            robotController.SendVelocityCommand(value, angularVelocitySlider.value);\n        }\n    }\n\n    void OnAngularVelocityChanged(float value)\n    {\n        UpdateVelocityDisplay();\n        if (robotController != null && !autonomousModeToggle.isOn)\n        {\n            robotController.SendVelocityCommand(linearVelocitySlider.value, value);\n        }\n    }\n\n    void OnJoystickDown(PointerEventData data)\n    {\n        isDragging = true;\n        dragStartPosition = data.position;\n        UpdateJoystickPosition(data.position);\n    }\n\n    void OnJoystickDrag(PointerEventData data)\n    {\n        if (isDragging)\n        {\n            UpdateJoystickPosition(data.position);\n            CalculateVelocityFromJoystick();\n        }\n    }\n\n    void OnJoystickUp(PointerEventData data)\n    {\n        isDragging = false;\n        joystickHandle.anchoredPosition = Vector2.zero;\n        ResetVelocity();\n    }\n\n    void UpdateJoystickPosition(Vector2 position)\n    {\n        Vector2 localPoint;\n        RectTransformUtility.ScreenPointToLocalPointInRectangle(\n            joystickArea.rectTransform, position, null, out localPoint);\n\n        // Clamp to joystick area\n        localPoint.x = Mathf.Clamp(localPoint.x, -joystickArea.rectTransform.rect.width / 2,\n                                  joystickArea.rectTransform.rect.width / 2);\n        localPoint.y = Mathf.Clamp(localPoint.y, -joystickArea.rectTransform.rect.height / 2,\n                                  joystickArea.rectTransform.rect.height / 2);\n\n        joystickHandle.anchoredPosition = localPoint;\n\n        // Update sliders based on joystick position\n        linearVelocitySlider.value = localPoint.y / (joystickArea.rectTransform.rect.height / 2) * linearVelocitySlider.maxValue;\n        angularVelocitySlider.value = localPoint.x / (joystickArea.rectTransform.rect.width / 2) * angularVelocitySlider.maxValue;\n    }\n\n    void CalculateVelocityFromJoystick()\n    {\n        Vector2 normalizedPos = new Vector2(\n            joystickHandle.anchoredPosition.x / (joystickArea.rectTransform.rect.width / 2),\n            joystickHandle.anchoredPosition.y / (joystickArea.rectTransform.rect.height / 2)\n        );\n\n        if (robotController != null && !autonomousModeToggle.isOn)\n        {\n            robotController.SendVelocityCommand(\n                normalizedPos.y * linearVelocitySlider.maxValue,\n                normalizedPos.x * angularVelocitySlider.maxValue\n            );\n        }\n\n        UpdateVelocityDisplay();\n    }\n\n    void UpdateVelocityDisplay()\n    {\n        velocityDisplay.text = $"Linear: {linearVelocitySlider.value:F2}, Angular: {angularVelocitySlider.value:F2}";\n        modeDisplay.text = autonomousModeToggle.isOn ? "Autonomous" : "Manual";\n    }\n\n    void ResetVelocity()\n    {\n        linearVelocitySlider.value = 0f;\n        angularVelocitySlider.value = 0f;\n        if (robotController != null)\n        {\n            robotController.SendVelocityCommand(0f, 0f);\n        }\n    }\n\n    public void OnEmergencyStop()\n    {\n        ResetVelocity();\n        autonomousModeToggle.isOn = false;\n        Debug.Log("Emergency stop activated!");\n    }\n\n    void OnAutonomousModeChanged(bool isAutonomous)\n    {\n        linearVelocitySlider.interactable = !isAutonomous;\n        angularVelocitySlider.interactable = !isAutonomous;\n        joystickArea.enabled = !isAutonomous;\n\n        modeDisplay.text = isAutonomous ? "Autonomous" : "Manual";\n\n        if (isAutonomous)\n        {\n            ResetVelocity();\n        }\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h2,{id:"advanced-visualization-techniques",children:"Advanced Visualization Techniques"}),"\n",(0,t.jsx)(e.h3,{id:"point-cloud-visualization",children:"Point Cloud Visualization"}),"\n",(0,t.jsx)(e.p,{children:"For 3D sensor data like LiDAR or depth cameras:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\n\n[RequireComponent(typeof(PointCloudRenderer))]\npublic class PointCloudVisualizer : MonoBehaviour\n{\n    [Header("Point Cloud Settings")]\n    public int maxPoints = 100000;\n    public float pointSize = 0.01f;\n    public Color pointColor = Color.white;\n    public bool useIntensityColor = true;\n\n    private PointCloudRenderer pointCloudRenderer;\n    private Material pointMaterial;\n    private ComputeShader pointComputeShader;\n\n    private Vector3[] points;\n    private float[] intensities;\n    private ComputeBuffer pointBuffer;\n    private ComputeBuffer intensityBuffer;\n\n    void Start()\n    {\n        InitializePointCloud();\n    }\n\n    void InitializePointCloud()\n    {\n        points = new Vector3[maxPoints];\n        intensities = new float[maxPoints];\n\n        // Create compute buffers for GPU processing\n        pointBuffer = new ComputeBuffer(maxPoints, sizeof(float) * 3);\n        intensityBuffer = new ComputeBuffer(maxPoints, sizeof(float));\n\n        // Create material for point rendering\n        pointMaterial = new Material(Shader.Find("Custom/PointCloudShader"));\n        pointMaterial.SetBuffer("Points", pointBuffer);\n        pointMaterial.SetBuffer("Intensities", intensityBuffer);\n        pointMaterial.SetFloat("PointSize", pointSize);\n        pointMaterial.SetColor("PointColor", pointColor);\n\n        // Setup renderer\n        pointCloudRenderer = GetComponent<PointCloudRenderer>();\n    }\n\n    public void UpdatePointCloud(Vector3[] newPoints, float[] newIntensities = null)\n    {\n        int pointCount = Mathf.Min(newPoints.Length, maxPoints);\n\n        // Copy points\n        for (int i = 0; i < pointCount; i++)\n        {\n            points[i] = newPoints[i];\n        }\n\n        // Copy intensities if provided\n        if (newIntensities != null)\n        {\n            for (int i = 0; i < pointCount && i < newIntensities.Length; i++)\n            {\n                intensities[i] = newIntensities[i];\n            }\n        }\n\n        // Update compute buffers\n        pointBuffer.SetData(points, 0, 0, pointCount);\n        if (newIntensities != null)\n        {\n            intensityBuffer.SetData(intensities, 0, 0, pointCount);\n        }\n\n        // Update material properties\n        pointMaterial.SetInt("PointCount", pointCount);\n        pointMaterial.SetInt("UseIntensityColor", useIntensityColor ? 1 : 0);\n    }\n\n    void OnRenderObject()\n    {\n        if (pointMaterial != null)\n        {\n            pointMaterial.SetPass(0);\n            // Render points using graphics calls\n            // This would typically involve custom rendering\n        }\n    }\n\n    void OnDestroy()\n    {\n        if (pointBuffer != null) pointBuffer.Release();\n        if (intensityBuffer != null) intensityBuffer.Release();\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h3,{id:"immersive-vrar-interfaces",children:"Immersive VR/AR Interfaces"}),"\n",(0,t.jsx)(e.p,{children:"For creating immersive human-robot interaction experiences:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'#if UNITY_HAS_VR || UNITY_HAS_AR\nusing UnityEngine.XR;\nusing UnityEngine.XR.ARFoundation;\nusing UnityEngine.XR.ARSubsystems;\n\npublic class ImmersiveHRI : MonoBehaviour\n{\n    [Header("VR/AR Settings")]\n    public bool enableVR = true;\n    public bool enableAR = false;\n    public Transform robotModel;\n    public Transform robotController;\n\n    [Header("Interaction Settings")]\n    public float interactionDistance = 3.0f;\n    public LayerMask interactionLayers;\n\n    private XRInputSubsystem xrInputSubsystem;\n    private List<InputDevice> inputDevices = new List<InputDevice>();\n\n    void Start()\n    {\n        SetupXRSystems();\n    }\n\n    void SetupXRSystems()\n    {\n        if (enableVR)\n        {\n            SetupVR();\n        }\n        else if (enableAR)\n        {\n            SetupAR();\n        }\n    }\n\n    void SetupVR()\n    {\n        // Initialize VR-specific components\n        var xrSubsystems = new List<XRInputSubsystem>();\n        SubsystemManager.GetInstances(xrSubsystems);\n\n        foreach (var subsystem in xrSubsystems)\n        {\n            if (subsystem != null)\n            {\n                xrInputSubsystem = subsystem;\n                break;\n            }\n        }\n    }\n\n    void SetupAR()\n    {\n        // Initialize AR-specific components\n        // This would typically involve AR Foundation setup\n    }\n\n    void Update()\n    {\n        HandleXRInput();\n        UpdateRobotInteraction();\n    }\n\n    void HandleXRInput()\n    {\n        if (xrInputSubsystem != null)\n        {\n            InputDeviceCharacteristics desiredCharacteristics = InputDeviceCharacteristics.Controller;\n            InputDevices.GetDevicesWithCharacteristics(desiredCharacteristics, inputDevices);\n\n            foreach (var device in inputDevices)\n            {\n                // Handle controller input\n                if (device.TryGetFeatureValue(CommonUsages.triggerButton, out bool triggerPressed) && triggerPressed)\n                {\n                    HandleGripInteraction();\n                }\n\n                if (device.TryGetFeatureValue(CommonUsages.gripButton, out bool gripPressed) && gripPressed)\n                {\n                    HandleTriggerInteraction();\n                }\n            }\n        }\n    }\n\n    void HandleGripInteraction()\n    {\n        // Handle grip button interaction\n        RaycastHit hit;\n        Ray ray = new Ray(robotController.position, robotController.forward);\n\n        if (Physics.Raycast(ray, out hit, interactionDistance, interactionLayers))\n        {\n            // Process interaction with hit object\n            ProcessRobotInteraction(hit.collider.gameObject);\n        }\n    }\n\n    void HandleTriggerInteraction()\n    {\n        // Handle trigger button interaction\n        // This could be for commanding robot actions\n    }\n\n    void ProcessRobotInteraction(GameObject interactedObject)\n    {\n        // Process interaction with the robot or environment\n        Debug.Log($"Interacting with: {interactedObject.name}");\n    }\n\n    void UpdateRobotInteraction()\n    {\n        // Update robot based on VR/AR input\n        if (robotModel != null && robotController != null)\n        {\n            // Update robot position and orientation based on controller\n            robotModel.position = robotController.position;\n            robotModel.rotation = robotController.rotation;\n        }\n    }\n}\n#endif\n'})}),"\n",(0,t.jsx)(e.h2,{id:"unity-ros-2-integration-patterns",children:"Unity-ROS 2 Integration Patterns"}),"\n",(0,t.jsx)(e.h3,{id:"publisher-and-subscriber-patterns",children:"Publisher and Subscriber Patterns"}),"\n",(0,t.jsx)(e.p,{children:"Implementing ROS 2 communication patterns in Unity:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'using System;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Std;\nusing RosMessageTypes.Sensor;\nusing UnityEngine;\n\npublic class UnityROSPublisher : MonoBehaviour\n{\n    [Header("ROS Topics")]\n    public string robotStateTopic = "robot/state";\n    public string sensorDataTopic = "sensor/data";\n    public string visualizationTopic = "visualization";\n\n    private ROSConnection ros;\n\n    void Start()\n    {\n        ros = ROSConnection.instance;\n\n        // Start publishing data at regular intervals\n        InvokeRepeating("PublishRobotState", 0.0f, 0.1f); // 10 Hz\n        InvokeRepeating("PublishSensorData", 0.0f, 0.033f); // ~30 Hz\n    }\n\n    void PublishRobotState()\n    {\n        // Create and publish robot state message\n        var stateMsg = new StringMsg();\n        stateMsg.data = $"Robot pose: {transform.position}, {transform.rotation.eulerAngles}";\n\n        ros.Send(robotStateTopic, stateMsg);\n    }\n\n    void PublishSensorData()\n    {\n        // Create and publish sensor data message\n        var sensorMsg = new JointStateMsg();\n        sensorMsg.name = new string[] { "joint1", "joint2", "joint3" };\n        sensorMsg.position = new double[] {\n            transform.eulerAngles.x * Mathf.Deg2Rad,\n            transform.eulerAngles.y * Mathf.Deg2Rad,\n            transform.eulerAngles.z * Mathf.Deg2Rad\n        };\n        sensorMsg.header.stamp = new TimeStamp(ROSConnection.GetNodeTime());\n\n        ros.Send(sensorDataTopic, sensorMsg);\n    }\n}\n\npublic class UnityROSSubscriber : MonoBehaviour\n{\n    [Header("ROS Topics")]\n    public string commandTopic = "robot/command";\n    public string sensorTopic = "sensor/feedback";\n\n    private ROSConnection ros;\n\n    void Start()\n    {\n        ros = ROSConnection.instance;\n\n        // Subscribe to ROS topics\n        ros.Subscribe<StringMsg>(commandTopic, OnCommandReceived);\n        ros.Subscribe<JointStateMsg>(sensorTopic, OnSensorDataReceived);\n    }\n\n    void OnCommandReceived(StringMsg command)\n    {\n        // Process command from ROS\n        Debug.Log($"Received command: {command.data}");\n\n        // Execute command in Unity\n        ProcessCommand(command.data);\n    }\n\n    void OnSensorDataReceived(JointStateMsg sensorData)\n    {\n        // Process sensor feedback from ROS\n        if (sensorData.position.Length >= 3)\n        {\n            // Update Unity visualization based on sensor data\n            transform.eulerAngles = new Vector3(\n                (float)sensorData.position[0] * Mathf.Rad2Deg,\n                (float)sensorData.position[1] * Mathf.Rad2Deg,\n                (float)sensorData.position[2] * Mathf.Rad2Deg\n            );\n        }\n    }\n\n    void ProcessCommand(string command)\n    {\n        // Parse and execute command\n        switch (command)\n        {\n            case "move_forward":\n                transform.Translate(Vector3.forward * Time.deltaTime);\n                break;\n            case "turn_left":\n                transform.Rotate(Vector3.up, -90 * Time.deltaTime);\n                break;\n            case "turn_right":\n                transform.Rotate(Vector3.up, 90 * Time.deltaTime);\n                break;\n            default:\n                Debug.LogWarning($"Unknown command: {command}");\n                break;\n        }\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(e.h3,{id:"rendering-optimization",children:"Rendering Optimization"}),"\n",(0,t.jsx)(e.p,{children:"Optimizing Unity applications for real-time robotics visualization:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\n\npublic class VisualizationOptimizer : MonoBehaviour\n{\n    [Header("LOD Settings")]\n    public int maxLODLevel = 3;\n    public float lodDistance = 10.0f;\n\n    [Header("Culling Settings")]\n    public float maxRenderDistance = 50.0f;\n    public bool enableOcclusionCulling = true;\n\n    [Header("Quality Settings")]\n    public bool dynamicQualityAdjustment = true;\n    public float targetFrameRate = 60.0f;\n\n    [Header("Visualization Toggles")]\n    public bool showTrajectory = true;\n    public bool showSensorData = true;\n    public bool showDebugInfo = false;\n\n    private LODGroup lodGroup;\n    private Camera mainCamera;\n\n    void Start()\n    {\n        SetupOptimization();\n    }\n\n    void SetupOptimization()\n    {\n        mainCamera = Camera.main;\n        SetupLOD();\n        SetupQualitySettings();\n    }\n\n    void SetupLOD()\n    {\n        lodGroup = GetComponent<LODGroup>();\n        if (lodGroup != null)\n        {\n            LOD[] lods = new LOD[maxLODLevel];\n\n            for (int i = 0; i < maxLODLevel; i++)\n            {\n                float screenRelativeTransitionHeight = (lodDistance * (i + 1)) / maxRenderDistance;\n                lods[i] = new LOD(screenRelativeTransitionHeight, GetRenderersForLOD(i));\n            }\n\n            lodGroup.SetLODs(lods);\n        }\n    }\n\n    Renderer[] GetRenderersForLOD(int lodLevel)\n    {\n        // Return appropriate renderers for each LOD level\n        // This would typically involve getting renderers for different detail levels\n        return new Renderer[0]; // Placeholder\n    }\n\n    void SetupQualitySettings()\n    {\n        Application.targetFrameRate = Mathf.RoundToInt(targetFrameRate);\n\n        if (dynamicQualityAdjustment)\n        {\n            QualitySettings.vSyncCount = 0; // Disable VSync for consistent frame rate\n        }\n    }\n\n    void Update()\n    {\n        OptimizeBasedOnPerformance();\n        ToggleVisualizations();\n    }\n\n    void OptimizeBasedOnPerformance()\n    {\n        // Adjust visualization quality based on performance\n        float currentFrameRate = 1.0f / Time.deltaTime;\n\n        if (currentFrameRate < targetFrameRate * 0.8f)\n        {\n            // Reduce quality if frame rate is too low\n            showSensorData = false;\n            showTrajectory = false;\n        }\n        else if (currentFrameRate > targetFrameRate * 0.9f)\n        {\n            // Increase quality if frame rate is good\n            showSensorData = true;\n            showTrajectory = true;\n        }\n    }\n\n    void ToggleVisualizations()\n    {\n        // Toggle visualization elements based on settings\n        if (GetComponent<RobotVisualizer>() != null)\n        {\n            GetComponent<RobotVisualizer>().showTrajectory = showTrajectory;\n            GetComponent<RobotVisualizer>().showSensorData = showSensorData;\n        }\n    }\n\n    void OnBecameVisible()\n    {\n        // Enable rendering when object becomes visible\n        EnableRendering();\n    }\n\n    void OnBecameInvisible()\n    {\n        // Disable rendering when object becomes invisible\n        DisableRendering();\n    }\n\n    void EnableRendering()\n    {\n        // Enable rendering components\n        Renderer[] renderers = GetComponentsInChildren<Renderer>();\n        foreach (Renderer renderer in renderers)\n        {\n            renderer.enabled = true;\n        }\n    }\n\n    void DisableRendering()\n    {\n        // Disable rendering components to save performance\n        Renderer[] renderers = GetComponentsInChildren<Renderer>();\n        foreach (Renderer renderer in renderers)\n        {\n            renderer.enabled = false;\n        }\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h2,{id:"best-practices-for-unity-robotics-applications",children:"Best Practices for Unity Robotics Applications"}),"\n",(0,t.jsx)(e.h3,{id:"1-separation-of-concerns",children:"1. Separation of Concerns"}),"\n",(0,t.jsx)(e.p,{children:"Keep visualization logic separate from robot control logic:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:"// Robot controller handles robot logic\npublic class RobotController : MonoBehaviour\n{\n    public void MoveTo(Vector3 target)\n    {\n        // Robot movement logic\n    }\n\n    public RobotState GetState()\n    {\n        // Return robot state\n        return new RobotState();\n    }\n}\n\n// Visualization component handles visualization\npublic class RobotVisualizer : MonoBehaviour\n{\n    public void UpdateVisualization(RobotState state)\n    {\n        // Update visualization based on robot state\n    }\n}\n"})}),"\n",(0,t.jsx)(e.h3,{id:"2-network-resilience",children:"2. Network Resilience"}),"\n",(0,t.jsx)(e.p,{children:"Handle network interruptions gracefully:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'public class NetworkResilientROSConnection : MonoBehaviour\n{\n    private ROSConnection ros;\n    private bool isConnected = false;\n    private float lastMessageTime;\n    private float connectionTimeout = 5.0f;\n\n    void Start()\n    {\n        ros = ROSConnection.instance;\n        ros.OnConnected += OnROSConnected;\n        ros.OnDisconnected += OnROSDisconnected;\n    }\n\n    void Update()\n    {\n        CheckConnectionHealth();\n    }\n\n    void CheckConnectionHealth()\n    {\n        if (isConnected && (Time.time - lastMessageTime) > connectionTimeout)\n        {\n            Debug.LogWarning("ROS connection timeout detected");\n            isConnected = false;\n            AttemptReconnection();\n        }\n    }\n\n    void AttemptReconnection()\n    {\n        Debug.Log("Attempting to reconnect to ROS...");\n        ros.Initialize("127.0.0.1", 10000);\n    }\n\n    void OnROSConnected()\n    {\n        isConnected = true;\n        lastMessageTime = Time.time;\n        Debug.Log("Connected to ROS");\n    }\n\n    void OnROSDisconnected()\n    {\n        isConnected = false;\n        Debug.LogWarning("Disconnected from ROS");\n        Invoke("AttemptReconnection", 1.0f);\n    }\n}\n'})}),"\n",(0,t.jsx)(e.p,{children:"Unity provides powerful capabilities for creating sophisticated visualization and interaction systems for robotics applications. By properly integrating Unity with ROS 2 and following best practices for performance and user experience, you can create compelling interfaces that enhance human-robot collaboration and enable effective robot monitoring and control in Physical AI applications."})]})}function u(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>r,x:()=>s});var t=i(6540);const o={},a=t.createContext(o);function r(n){const e=t.useContext(a);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:r(n.components),t.createElement(a.Provider,{value:e},n.children)}}}]);