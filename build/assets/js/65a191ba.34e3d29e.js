"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[6567],{573:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});var o=n(4848),t=n(8453);const a={title:"Glossary of Physical AI & Humanoid Robotics Terms"},r="Glossary of Physical AI & Humanoid Robotics Terms",s={id:"physical-ai/glossary",title:"Glossary of Physical AI & Humanoid Robotics Terms",description:"This comprehensive glossary contains technical terminology used throughout the Physical AI & Humanoid Robotics textbook. Each term is defined with its specific meaning in the context of Physical AI, robotics, and embodied intelligence.",source:"@site/docs/physical-ai/glossary.mdx",sourceDirName:"physical-ai",slug:"/physical-ai/glossary",permalink:"/physical-ai-textbook/physical-ai/physical-ai/glossary",draft:!1,unlisted:!1,editUrl:"https://github.com/your-username/physical-ai-textbook/tree/main/docs/physical-ai/glossary.mdx",tags:[],version:"current",frontMatter:{title:"Glossary of Physical AI & Humanoid Robotics Terms"},sidebar:"tutorialSidebar",previous:{title:"conclusion",permalink:"/physical-ai-textbook/physical-ai/physical-ai/capstone-project/conclusion"}},l={},c=[{value:"A",id:"a",level:2},{value:"Actuator",id:"actuator",level:3},{value:"Artificial Intelligence (AI)",id:"artificial-intelligence-ai",level:3},{value:"Autonomous System",id:"autonomous-system",level:3},{value:"B",id:"b",level:2},{value:"Behavior Tree",id:"behavior-tree",level:3},{value:"Bidirectional Encoder Representations from Transformers (BERT)",id:"bidirectional-encoder-representations-from-transformers-bert",level:3},{value:"C",id:"c",level:2},{value:"Computer Vision",id:"computer-vision",level:3},{value:"Control Theory",id:"control-theory",level:3},{value:"Convolutional Neural Network (CNN)",id:"convolutional-neural-network-cnn",level:3},{value:"D",id:"d",level:2},{value:"Deep Learning",id:"deep-learning",level:3},{value:"Degrees of Freedom (DOF)",id:"degrees-of-freedom-dof",level:3},{value:"Dynamic Movement Primitives (DMP)",id:"dynamic-movement-primitives-dmp",level:3},{value:"E",id:"e",level:2},{value:"Embodied Intelligence",id:"embodied-intelligence",level:3},{value:"End Effector",id:"end-effector",level:3},{value:"F",id:"f",level:2},{value:"Forward Kinematics",id:"forward-kinematics",level:3},{value:"Force Control",id:"force-control",level:3},{value:"G",id:"g",level:2},{value:"Gazebo",id:"gazebo",level:3},{value:"Generalized Robot Description Format (URDF/SDF)",id:"generalized-robot-description-format-urdfsdf",level:3},{value:"Generative Adversarial Network (GAN)",id:"generative-adversarial-network-gan",level:3},{value:"H",id:"h",level:2},{value:"Human-Robot Interaction (HRI)",id:"human-robot-interaction-hri",level:3},{value:"I",id:"i",level:2},{value:"Inverse Kinematics",id:"inverse-kinematics",level:3},{value:"Inverse Dynamics",id:"inverse-dynamics",level:3},{value:"J",id:"j",level:2},{value:"Jacobian Matrix",id:"jacobian-matrix",level:3},{value:"K",id:"k",level:2},{value:"Kinematics",id:"kinematics",level:3},{value:"Kinodynamic Planning",id:"kinodynamic-planning",level:3},{value:"L",id:"l",level:2},{value:"LIDAR",id:"lidar",level:3},{value:"Localization",id:"localization",level:3},{value:"M",id:"m",level:2},{value:"Manipulator",id:"manipulator",level:3},{value:"Manipulation",id:"manipulation",level:3},{value:"Motion Planning",id:"motion-planning",level:3},{value:"N",id:"n",level:2},{value:"Neural Network",id:"neural-network",level:3},{value:"NVIDIA Isaac",id:"nvidia-isaac",level:3},{value:"O",id:"o",level:2},{value:"Occupancy Grid",id:"occupancy-grid",level:3},{value:"P",id:"p",level:2},{value:"Path Planning",id:"path-planning",level:3},{value:"Physical AI",id:"physical-ai",level:3},{value:"Point Cloud",id:"point-cloud",level:3},{value:"Proportional-Integral-Derivative (PID) Controller",id:"proportional-integral-derivative-pid-controller",level:3},{value:"R",id:"r",level:2},{value:"Robot Operating System (ROS)",id:"robot-operating-system-ros",level:3},{value:"ROS 2",id:"ros-2",level:3},{value:"Reactive Navigation",id:"reactive-navigation",level:3},{value:"S",id:"s",level:2},{value:"Sensor Fusion",id:"sensor-fusion",level:3},{value:"Simultaneous Localization and Mapping (SLAM)",id:"simultaneous-localization-and-mapping-slam",level:3},{value:"State Estimation",id:"state-estimation",level:3},{value:"Simulation-to-Reality Transfer (Sim-to-Real)",id:"simulation-to-reality-transfer-sim-to-real",level:3},{value:"T",id:"t",level:2},{value:"Trajectory Planning",id:"trajectory-planning",level:3},{value:"Teleoperation",id:"teleoperation",level:3},{value:"V",id:"v",level:2},{value:"Vision-Language-Action (VLA)",id:"vision-language-action-vla",level:3},{value:"Visual Servoing",id:"visual-servoing",level:3},{value:"W",id:"w",level:2},{value:"Waypoint",id:"waypoint",level:3},{value:"Y",id:"y",level:2},{value:"Yaw",id:"yaw",level:3},{value:"Z",id:"z",level:2},{value:"Zero Moment Point (ZMP)",id:"zero-moment-point-zmp",level:3}];function d(e){const i={h1:"h1",h2:"h2",h3:"h3",p:"p",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(i.h1,{id:"glossary-of-physical-ai--humanoid-robotics-terms",children:"Glossary of Physical AI & Humanoid Robotics Terms"}),"\n",(0,o.jsx)(i.p,{children:"This comprehensive glossary contains technical terminology used throughout the Physical AI & Humanoid Robotics textbook. Each term is defined with its specific meaning in the context of Physical AI, robotics, and embodied intelligence."}),"\n",(0,o.jsx)(i.h2,{id:"a",children:"A"}),"\n",(0,o.jsx)(i.h3,{id:"actuator",children:"Actuator"}),"\n",(0,o.jsx)(i.p,{children:"A component of a robot that converts control signals into physical motion. In humanoid robotics, actuators control joint movements and provide the mechanical power needed for locomotion and manipulation."}),"\n",(0,o.jsx)(i.h3,{id:"artificial-intelligence-ai",children:"Artificial Intelligence (AI)"}),"\n",(0,o.jsx)(i.p,{children:"The simulation of human intelligence processes by machines, especially computer systems. In Physical AI, AI systems must operate in real-time within physical constraints and interact directly with the environment."}),"\n",(0,o.jsx)(i.h3,{id:"autonomous-system",children:"Autonomous System"}),"\n",(0,o.jsx)(i.p,{children:"A system that operates independently without human intervention. In robotics, autonomous systems can perceive their environment, make decisions, and execute actions based on sensor data and pre-programmed behaviors."}),"\n",(0,o.jsx)(i.h2,{id:"b",children:"B"}),"\n",(0,o.jsx)(i.h3,{id:"behavior-tree",children:"Behavior Tree"}),"\n",(0,o.jsx)(i.p,{children:"A hierarchical structure used to organize and control the execution of tasks in robotics. Behavior trees provide a way to model complex robot behaviors by breaking them down into smaller, manageable tasks."}),"\n",(0,o.jsx)(i.h3,{id:"bidirectional-encoder-representations-from-transformers-bert",children:"Bidirectional Encoder Representations from Transformers (BERT)"}),"\n",(0,o.jsx)(i.p,{children:"A transformer-based machine learning technique for natural language processing pre-training developed by Google. In robotics, BERT-like models can be adapted for understanding commands and contextual information."}),"\n",(0,o.jsx)(i.h2,{id:"c",children:"C"}),"\n",(0,o.jsx)(i.h3,{id:"computer-vision",children:"Computer Vision"}),"\n",(0,o.jsx)(i.p,{children:"A field of artificial intelligence that trains computers to interpret and understand the visual world. In robotics, computer vision enables robots to identify objects, navigate spaces, and interact with their environment."}),"\n",(0,o.jsx)(i.h3,{id:"control-theory",children:"Control Theory"}),"\n",(0,o.jsx)(i.p,{children:"An engineering and mathematics field dealing with the dynamic behavior of systems. In robotics, control theory provides the mathematical foundation for designing systems that can maintain desired behaviors despite disturbances."}),"\n",(0,o.jsx)(i.h3,{id:"convolutional-neural-network-cnn",children:"Convolutional Neural Network (CNN)"}),"\n",(0,o.jsx)(i.p,{children:"A class of deep neural networks commonly applied to visual imagery. CNNs are widely used in robotics for object recognition, scene understanding, and visual processing tasks."}),"\n",(0,o.jsx)(i.h2,{id:"d",children:"D"}),"\n",(0,o.jsx)(i.h3,{id:"deep-learning",children:"Deep Learning"}),"\n",(0,o.jsx)(i.p,{children:"A subset of machine learning that uses neural networks with multiple layers. In robotics, deep learning enables complex pattern recognition, sensor data processing, and decision-making capabilities."}),"\n",(0,o.jsx)(i.h3,{id:"degrees-of-freedom-dof",children:"Degrees of Freedom (DOF)"}),"\n",(0,o.jsx)(i.p,{children:"The number of independent movements a mechanical system can make. For humanoid robots, DOF determines the flexibility and range of motion of the robot's joints and limbs."}),"\n",(0,o.jsx)(i.h3,{id:"dynamic-movement-primitives-dmp",children:"Dynamic Movement Primitives (DMP)"}),"\n",(0,o.jsx)(i.p,{children:"A mathematical framework for representing and generating movements in robotics. DMPs allow robots to learn and reproduce complex movement patterns while maintaining stability."}),"\n",(0,o.jsx)(i.h2,{id:"e",children:"E"}),"\n",(0,o.jsx)(i.h3,{id:"embodied-intelligence",children:"Embodied Intelligence"}),"\n",(0,o.jsx)(i.p,{children:"Intelligence that emerges from the interaction between an agent and its environment. In Physical AI, embodied intelligence emphasizes that intelligence is not just computation but arises from the coupling between body, brain, and environment."}),"\n",(0,o.jsx)(i.h3,{id:"end-effector",children:"End Effector"}),"\n",(0,o.jsx)(i.p,{children:"The device at the end of a robotic arm that interacts with the environment. Examples include grippers, tools, or sensors designed for specific tasks."}),"\n",(0,o.jsx)(i.h2,{id:"f",children:"F"}),"\n",(0,o.jsx)(i.h3,{id:"forward-kinematics",children:"Forward Kinematics"}),"\n",(0,o.jsx)(i.p,{children:"The use of joint parameters to compute the Cartesian coordinates of the end effector. Forward kinematics determines where the robot's end effector is positioned based on the current joint angles."}),"\n",(0,o.jsx)(i.h3,{id:"force-control",children:"Force Control"}),"\n",(0,o.jsx)(i.p,{children:"A control strategy in robotics that regulates the forces applied by a robot to its environment. Force control is essential for tasks requiring delicate interactions, such as assembly or manipulation of fragile objects."}),"\n",(0,o.jsx)(i.h2,{id:"g",children:"G"}),"\n",(0,o.jsx)(i.h3,{id:"gazebo",children:"Gazebo"}),"\n",(0,o.jsx)(i.p,{children:"A 3D simulation environment for robotics that provides physics simulation and sensor models. Gazebo allows developers to test robotic algorithms in a realistic virtual environment before deploying them on physical robots."}),"\n",(0,o.jsx)(i.h3,{id:"generalized-robot-description-format-urdfsdf",children:"Generalized Robot Description Format (URDF/SDF)"}),"\n",(0,o.jsx)(i.p,{children:"XML-based formats for representing robots in ROS. URDF describes robot structure and kinematics, while SDF extends this to simulation environments and dynamics."}),"\n",(0,o.jsx)(i.h3,{id:"generative-adversarial-network-gan",children:"Generative Adversarial Network (GAN)"}),"\n",(0,o.jsx)(i.p,{children:"A class of machine learning frameworks where two neural networks compete against each other. In robotics, GANs can be used for generating synthetic training data or creating realistic simulation environments."}),"\n",(0,o.jsx)(i.h2,{id:"h",children:"H"}),"\n",(0,o.jsx)(i.h3,{id:"human-robot-interaction-hri",children:"Human-Robot Interaction (HRI)"}),"\n",(0,o.jsx)(i.p,{children:"The study of interactions between humans and robots. HRI encompasses the design, development, and evaluation of robots intended to interact with humans in various contexts."}),"\n",(0,o.jsx)(i.h2,{id:"i",children:"I"}),"\n",(0,o.jsx)(i.h3,{id:"inverse-kinematics",children:"Inverse Kinematics"}),"\n",(0,o.jsx)(i.p,{children:"The use of Cartesian coordinates to compute joint parameters. Inverse kinematics determines the joint angles required to position the robot's end effector at a desired location."}),"\n",(0,o.jsx)(i.h3,{id:"inverse-dynamics",children:"Inverse Dynamics"}),"\n",(0,o.jsx)(i.p,{children:"The calculation of forces and torques required to generate a specific motion. In robotics, inverse dynamics is used for control and planning of robot movements."}),"\n",(0,o.jsx)(i.h2,{id:"j",children:"J"}),"\n",(0,o.jsx)(i.h3,{id:"jacobian-matrix",children:"Jacobian Matrix"}),"\n",(0,o.jsx)(i.p,{children:"A matrix of partial derivatives that describes the relationship between joint velocities and end-effector velocities. The Jacobian is crucial for motion planning and control of robotic manipulators."}),"\n",(0,o.jsx)(i.h2,{id:"k",children:"K"}),"\n",(0,o.jsx)(i.h3,{id:"kinematics",children:"Kinematics"}),"\n",(0,o.jsx)(i.p,{children:"The study of motion without considering the forces that cause it. In robotics, kinematics describes the relationship between joint positions and the position of the robot's end effector."}),"\n",(0,o.jsx)(i.h3,{id:"kinodynamic-planning",children:"Kinodynamic Planning"}),"\n",(0,o.jsx)(i.p,{children:"A type of motion planning that considers both kinematic and dynamic constraints of a robot. Kinodynamic planning is essential for generating feasible trajectories for robots with complex dynamics."}),"\n",(0,o.jsx)(i.h2,{id:"l",children:"L"}),"\n",(0,o.jsx)(i.h3,{id:"lidar",children:"LIDAR"}),"\n",(0,o.jsx)(i.p,{children:"Light Detection and Ranging - a remote sensing method that uses light in the form of a pulsed laser. LIDAR is widely used in robotics for mapping, localization, and obstacle detection."}),"\n",(0,o.jsx)(i.h3,{id:"localization",children:"Localization"}),"\n",(0,o.jsx)(i.p,{children:"The process of determining a robot's position and orientation in a known or unknown environment. Localization is fundamental for autonomous navigation and task execution."}),"\n",(0,o.jsx)(i.h2,{id:"m",children:"M"}),"\n",(0,o.jsx)(i.h3,{id:"manipulator",children:"Manipulator"}),"\n",(0,o.jsx)(i.p,{children:"A robotic device used to manipulate objects in the environment. Robotic manipulators typically consist of a series of joints and links designed to position an end effector at desired locations."}),"\n",(0,o.jsx)(i.h3,{id:"manipulation",children:"Manipulation"}),"\n",(0,o.jsx)(i.p,{children:"The process of grasping, moving, and repositioning objects in the environment. Robotic manipulation requires precise control, force regulation, and object recognition capabilities."}),"\n",(0,o.jsx)(i.h3,{id:"motion-planning",children:"Motion Planning"}),"\n",(0,o.jsx)(i.p,{children:"The computational problem of finding a valid sequence of movements for a robot to navigate from a start configuration to a goal configuration while avoiding obstacles."}),"\n",(0,o.jsx)(i.h2,{id:"n",children:"N"}),"\n",(0,o.jsx)(i.h3,{id:"neural-network",children:"Neural Network"}),"\n",(0,o.jsx)(i.p,{children:"A computing system inspired by the biological neural networks that constitute animal brains. Neural networks are fundamental to many AI applications in robotics, including perception, control, and decision-making."}),"\n",(0,o.jsx)(i.h3,{id:"nvidia-isaac",children:"NVIDIA Isaac"}),"\n",(0,o.jsx)(i.p,{children:"A robotics platform developed by NVIDIA that provides simulation, navigation, manipulation, and perception capabilities for robotics applications, leveraging GPU computing for AI workloads."}),"\n",(0,o.jsx)(i.h2,{id:"o",children:"O"}),"\n",(0,o.jsx)(i.h3,{id:"occupancy-grid",children:"Occupancy Grid"}),"\n",(0,o.jsx)(i.p,{children:"A probabilistic representation of space used in robotics for mapping and path planning. Occupancy grids divide space into discrete cells that represent the probability of occupancy."}),"\n",(0,o.jsx)(i.h2,{id:"p",children:"P"}),"\n",(0,o.jsx)(i.h3,{id:"path-planning",children:"Path Planning"}),"\n",(0,o.jsx)(i.p,{children:"The process of determining a geometric path between a start and goal position. Path planning focuses on geometric constraints without considering dynamic constraints."}),"\n",(0,o.jsx)(i.h3,{id:"physical-ai",children:"Physical AI"}),"\n",(0,o.jsx)(i.p,{children:"The field of artificial intelligence focused on embodied systems that interact with the physical world. Physical AI combines traditional AI with real-world physics, sensorimotor interaction, and environmental constraints."}),"\n",(0,o.jsx)(i.h3,{id:"point-cloud",children:"Point Cloud"}),"\n",(0,o.jsx)(i.p,{children:"A collection of data points in a three-dimensional coordinate system. Point clouds are typically generated by 3D scanners or depth sensors and are used for mapping and object recognition."}),"\n",(0,o.jsx)(i.h3,{id:"proportional-integral-derivative-pid-controller",children:"Proportional-Integral-Derivative (PID) Controller"}),"\n",(0,o.jsx)(i.p,{children:"A control loop mechanism widely used in robotics and industrial control systems. PID controllers adjust system behavior based on the difference between desired and actual values."}),"\n",(0,o.jsx)(i.h2,{id:"r",children:"R"}),"\n",(0,o.jsx)(i.h3,{id:"robot-operating-system-ros",children:"Robot Operating System (ROS)"}),"\n",(0,o.jsx)(i.p,{children:"A flexible framework for writing robot software that provides services designed for a heterogeneous computer cluster. ROS provides hardware abstraction, device drivers, libraries, and tools for robotics development."}),"\n",(0,o.jsx)(i.h3,{id:"ros-2",children:"ROS 2"}),"\n",(0,o.jsx)(i.p,{children:"The second generation of the Robot Operating System with improved security, real-time support, and multi-robot systems. ROS 2 uses DDS (Data Distribution Service) for communication."}),"\n",(0,o.jsx)(i.h3,{id:"reactive-navigation",children:"Reactive Navigation"}),"\n",(0,o.jsx)(i.p,{children:"A navigation approach where a robot responds directly to sensor inputs without maintaining a global map. Reactive navigation is useful for obstacle avoidance but may not find optimal paths."}),"\n",(0,o.jsx)(i.h2,{id:"s",children:"S"}),"\n",(0,o.jsx)(i.h3,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,o.jsx)(i.p,{children:"The combining of sensory data or data derived from disparate sources such that the resulting information has less uncertainty than would be possible when these sources were used individually. In robotics, sensor fusion combines data from cameras, LIDAR, IMUs, and other sensors."}),"\n",(0,o.jsx)(i.h3,{id:"simultaneous-localization-and-mapping-slam",children:"Simultaneous Localization and Mapping (SLAM)"}),"\n",(0,o.jsx)(i.p,{children:"The computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it. SLAM is fundamental for autonomous robots."}),"\n",(0,o.jsx)(i.h3,{id:"state-estimation",children:"State Estimation"}),"\n",(0,o.jsx)(i.p,{children:"The process of determining the state of a system from noisy and incomplete measurements. In robotics, state estimation is crucial for accurate control and navigation."}),"\n",(0,o.jsx)(i.h3,{id:"simulation-to-reality-transfer-sim-to-real",children:"Simulation-to-Reality Transfer (Sim-to-Real)"}),"\n",(0,o.jsx)(i.p,{children:"The process of transferring skills or policies learned in simulation to real-world robotic systems. Sim-to-real transfer addresses the reality gap between simulated and real environments."}),"\n",(0,o.jsx)(i.h2,{id:"t",children:"T"}),"\n",(0,o.jsx)(i.h3,{id:"trajectory-planning",children:"Trajectory Planning"}),"\n",(0,o.jsx)(i.p,{children:"The process of creating a path that a robot will follow to reach a target position. Trajectory planning considers both geometric and temporal aspects of robot motion."}),"\n",(0,o.jsx)(i.h3,{id:"teleoperation",children:"Teleoperation"}),"\n",(0,o.jsx)(i.p,{children:"The remote operation of a robot by a human operator. Teleoperation systems can range from simple remote control to sophisticated interfaces that provide haptic feedback."}),"\n",(0,o.jsx)(i.h2,{id:"v",children:"V"}),"\n",(0,o.jsx)(i.h3,{id:"vision-language-action-vla",children:"Vision-Language-Action (VLA)"}),"\n",(0,o.jsx)(i.p,{children:"A system that integrates visual perception, natural language understanding, and physical action. VLA systems enable robots to understand and execute complex commands based on visual and linguistic input."}),"\n",(0,o.jsx)(i.h3,{id:"visual-servoing",children:"Visual Servoing"}),"\n",(0,o.jsx)(i.p,{children:"A control strategy that uses visual feedback to control the motion of a robot. Visual servoing can be used for tasks such as object tracking, grasping, and navigation."}),"\n",(0,o.jsx)(i.h2,{id:"w",children:"W"}),"\n",(0,o.jsx)(i.h3,{id:"waypoint",children:"Waypoint"}),"\n",(0,o.jsx)(i.p,{children:"A reference point for navigation that helps define a route or path. Waypoints are used in path planning and navigation to guide robots through complex environments."}),"\n",(0,o.jsx)(i.h2,{id:"y",children:"Y"}),"\n",(0,o.jsx)(i.h3,{id:"yaw",children:"Yaw"}),"\n",(0,o.jsx)(i.p,{children:"The rotation of an aircraft or robot about its vertical axis. Yaw is one of the three primary axes of rotation, along with pitch and roll."}),"\n",(0,o.jsx)(i.h2,{id:"z",children:"Z"}),"\n",(0,o.jsx)(i.h3,{id:"zero-moment-point-zmp",children:"Zero Moment Point (ZMP)"}),"\n",(0,o.jsx)(i.p,{children:"A criterion for determining the stability of legged robots. The ZMP is used in humanoid robotics to ensure that the robot maintains balance during locomotion."})]})}function h(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,o.jsx)(i,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>r,x:()=>s});var o=n(6540);const t={},a=o.createContext(t);function r(e){const i=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function s(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(a.Provider,{value:i},e.children)}}}]);