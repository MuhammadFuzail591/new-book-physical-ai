---
title: LLM Integration with ROS 2 for Cognitive Robotics
---

# LLM Integration with ROS 2 for Cognitive Robotics

## Introduction to LLM Integration in Robotics

Large Language Models (LLMs) have emerged as powerful tools for cognitive robotics, enabling robots to understand natural language commands, reason about complex tasks, and generate executable action sequences. The integration of LLMs with ROS 2 (Robot Operating System 2) creates cognitive robotic systems that can bridge the gap between high-level human instructions and low-level robot control, facilitating more natural and intuitive human-robot interaction.

Traditional robotic systems rely on pre-programmed behaviors and structured command interfaces, limiting their flexibility and requiring specialized knowledge to operate. LLM integration allows robots to interpret natural language instructions such as "bring me the red mug from the kitchen" and translate them into sequences of ROS 2 actions and service calls. This integration requires careful consideration of real-time constraints, safety mechanisms, and the distributed nature of ROS 2 systems.

The integration process involves several key components: natural language understanding, task planning, action generation, and execution monitoring. Each component must be designed to work seamlessly within the ROS 2 ecosystem while maintaining the responsiveness and reliability required for robotic applications.

## LLM Architectures for Robotics

### Transformer-Based Models

Transformer-based LLMs form the foundation of most robotic language understanding systems. These models excel at processing natural language and can be adapted for robotic tasks through fine-tuning or prompt engineering:

```python
import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer
from typing import Dict, List, Optional, Any
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from geometry_msgs.msg import PoseStamped

class RobotLanguageModel(nn.Module):
    """Transformer-based language model adapted for robotics tasks"""

    def __init__(self, model_name: str = "microsoft/DialoGPT-medium"):
        super().__init__()

        # Base transformer model
        self.transformer = AutoModel.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        # Add special tokens for robotics domain
        special_tokens = {
            "additional_special_tokens": [
                "<robot>", "<action>", "<object>", "<location>",
                "<navigation>", "<manipulation>", "<perception>"
            ]
        }
        self.tokenizer.add_special_tokens(special_tokens)

        # Task-specific heads
        self.intent_classifier = nn.Linear(self.transformer.config.hidden_size, 10)  # 10 robot intents
        self.action_generator = nn.Linear(self.transformer.config.hidden_size, 128)  # Action space
        self.location_predictor = nn.Linear(self.transformer.config.hidden_size, 64)  # Location space

        # Action vocabulary for robotics
        self.robot_actions = [
            "move_to", "pick_up", "place", "grasp", "release",
            "navigate", "inspect", "follow", "stop", "wait"
        ]

    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Dict[str, torch.Tensor]:
        """Process input and generate robot-specific outputs"""
        # Get transformer outputs
        transformer_outputs = self.transformer(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        # Use the last hidden state for predictions
        last_hidden_state = transformer_outputs.last_hidden_state[:, -1, :]  # [batch, hidden_size]

        # Predict intent
        intent_logits = self.intent_classifier(last_hidden_state)

        # Generate action representation
        action_repr = self.action_generator(last_hidden_state)

        # Predict location/context
        location_repr = self.location_predictor(last_hidden_state)

        return {
            'intent_logits': intent_logits,
            'action_repr': action_repr,
            'location_repr': location_repr,
            'hidden_states': transformer_outputs.last_hidden_state
        }

    def parse_command(self, command: str) -> Dict[str, Any]:
        """Parse a natural language command into structured components"""
        # Tokenize the command
        inputs = self.tokenizer(
            command,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )

        # Get model outputs
        outputs = self.forward(inputs['input_ids'], inputs['attention_mask'])

        # Extract intent
        intent_probs = torch.softmax(outputs['intent_logits'], dim=-1)
        predicted_intent = torch.argmax(intent_probs, dim=-1).item()

        # Generate action sequence
        action_sequence = self.generate_action_sequence(
            outputs['action_repr'],
            command
        )

        return {
            'intent': predicted_intent,
            'action_sequence': action_sequence,
            'confidence': intent_probs[0][predicted_intent].item()
        }

    def generate_action_sequence(self, action_repr: torch.Tensor, command: str) -> List[Dict[str, Any]]:
        """Generate a sequence of ROS 2 actions from command and representation"""
        # This is a simplified example - in practice, this would be more sophisticated
        actions = []

        if "move" in command.lower() or "go" in command.lower() or "navigate" in command.lower():
            actions.append({
                'action': 'move_base',
                'target': self.extract_location(command),
                'parameters': {}
            })
        elif "pick" in command.lower() or "grasp" in command.lower():
            actions.append({
                'action': 'pick_object',
                'target': self.extract_object(command),
                'parameters': {}
            })
        elif "place" in command.lower() or "put" in command.lower():
            actions.append({
                'action': 'place_object',
                'target': self.extract_location(command),
                'parameters': {}
            })

        return actions

    def extract_location(self, command: str) -> str:
        """Extract location information from command"""
        # Simple keyword-based extraction (in practice, use more sophisticated NLP)
        locations = ["kitchen", "living room", "bedroom", "office", "dining room", "bathroom"]
        for loc in locations:
            if loc in command.lower():
                return loc
        return "unknown_location"

    def extract_object(self, command: str) -> str:
        """Extract object information from command"""
        # Simple keyword-based extraction
        objects = ["cup", "mug", "bottle", "book", "phone", "keys", "plate", "fork"]
        for obj in objects:
            if obj in command.lower():
                # Extract color if present
                colors = ["red", "blue", "green", "yellow", "black", "white"]
                for color in colors:
                    if color in command.lower():
                        return f"{color} {obj}"
                return obj
        return "unknown_object"
```

### Vision-Language Models

For more sophisticated robotic applications, vision-language models combine visual perception with language understanding:

```python
from transformers import VisionEncoderDecoderModel, ViTImageProcessor
import cv2
import numpy as np

class VisionLanguageRobotModel(nn.Module):
    """Vision-language model for robotic applications"""

    def __init__(self, vision_model_name: str = "google/vit-base-patch16-224",
                 text_model_name: str = "gpt2"):
        super().__init__()

        # Vision encoder
        self.vision_encoder = VisionEncoderDecoderModel.from_pretrained(
            "nlpconnect/vit-gpt2-image-captioning"
        )
        self.image_processor = ViTImageProcessor.from_pretrained(vision_model_name)

        # Language model for command interpretation
        self.language_model = RobotLanguageModel(text_model_name)

        # Cross-modal attention for vision-language fusion
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=768,  # Typical hidden size
            num_heads=8,
            batch_first=True
        )

        # Fusion layer
        self.fusion_layer = nn.Linear(768 * 2, 768)

    def forward(self, images: torch.Tensor, commands: List[str]) -> Dict[str, torch.Tensor]:
        """Process visual and linguistic inputs jointly"""
        batch_size = images.size(0)

        # Process images
        visual_features = self.vision_encoder.encoder(images)  # Simplified

        # Process commands
        command_features = []
        for cmd in commands:
            inputs = self.language_model.tokenizer(
                cmd, return_tensors="pt", padding=True, truncation=True
            )
            cmd_outputs = self.language_model.transformer(**inputs)
            command_features.append(cmd_outputs.last_hidden_state[:, -1, :])  # [hidden_size]

        command_features = torch.stack(command_features)  # [batch, hidden_size]

        # Apply cross-attention between visual and language features
        attended_features, attention_weights = self.cross_attention(
            query=command_features.unsqueeze(1),  # [batch, 1, hidden_size]
            key=visual_features,  # [batch, seq_len, hidden_size]
            value=visual_features
        )

        # Fuse features
        fused_features = torch.cat([
            command_features,
            attended_features.squeeze(1)
        ], dim=-1)  # [batch, 2*hidden_size]

        fused_output = self.fusion_layer(fused_features)  # [batch, hidden_size]

        return {
            'fused_features': fused_output,
            'visual_features': visual_features,
            'language_features': command_features,
            'attention_weights': attention_weights
        }

    def process_robot_task(self, image: np.ndarray, command: str) -> Dict[str, Any]:
        """Process a robot task with both visual and linguistic inputs"""
        # Preprocess image
        image_tensor = self.preprocess_image(image)

        # Get fused representation
        outputs = self.forward(image_tensor.unsqueeze(0), [command])

        # Parse command using fused representation
        action_sequence = self.language_model.generate_action_sequence(
            outputs['fused_features'], command
        )

        return {
            'action_sequence': action_sequence,
            'visual_attention': outputs['attention_weights'],
            'confidence': 0.9  # Placeholder confidence
        }

    def preprocess_image(self, image: np.ndarray) -> torch.Tensor:
        """Preprocess image for the vision model"""
        # Convert BGR to RGB if needed
        if len(image.shape) == 3 and image.shape[2] == 3:
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Resize and normalize
        image_resized = cv2.resize(image, (224, 224))
        image_normalized = image_resized.astype(np.float32) / 255.0
        image_tensor = torch.from_numpy(image_normalized).permute(2, 0, 1).unsqueeze(0)

        return image_tensor
```

## ROS 2 Integration Patterns

### Cognitive Node Architecture

The cognitive node architecture provides the foundation for integrating LLMs with ROS 2 systems:

```python
class CognitiveNode(Node):
    """ROS 2 node for LLM-based cognitive processing"""

    def __init__(self):
        super().__init__('cognitive_node')

        # Initialize LLM model
        self.llm_model = RobotLanguageModel()

        # ROS 2 interfaces
        self.command_subscriber = self.create_subscription(
            String,
            'robot_commands',
            self.command_callback,
            10
        )

        self.response_publisher = self.create_publisher(
            String,
            'cognitive_response',
            10
        )

        self.action_publisher = self.create_publisher(
            String,  # In practice, use action-specific message types
            'cognitive_actions',
            10
        )

        # Service server for complex queries
        self.query_service = self.create_service(
            String,
            'cognitive_query',
            self.query_callback
        )

        # Action clients for robot execution
        self.action_clients = {}

        # Internal state
        self.current_task = None
        self.task_queue = []
        self.context_history = []

        # Non-blocking processing timer
        self.process_timer = self.create_timer(0.1, self.process_tasks)

        self.get_logger().info('Cognitive node initialized')

    def command_callback(self, msg: String):
        """Handle incoming natural language commands"""
        command = msg.data
        self.get_logger().info(f'Received command: {command}')

        # Add to processing queue
        self.task_queue.append({
            'command': command,
            'timestamp': self.get_clock().now(),
            'source': 'subscriber'
        })

    def query_callback(self, request: String, response):
        """Handle cognitive query service requests"""
        try:
            # Process the query with LLM
            result = self.llm_model.parse_command(request.data)

            # Format response
            response.data = f"Intent: {result['intent']}, Actions: {result['action_sequence']}"

        except Exception as e:
            self.get_logger().error(f'Error processing query: {e}')
            response.data = f"Error: {str(e)}"

        return response

    def process_tasks(self):
        """Process tasks in the queue"""
        if self.task_queue:
            task = self.task_queue.pop(0)

            try:
                # Process command with LLM
                result = self.llm_model.parse_command(task['command'])

                if result['confidence'] > 0.7:  # Confidence threshold
                    # Publish action sequence
                    action_msg = String()
                    action_msg.data = str(result['action_sequence'])
                    self.action_publisher.publish(action_msg)

                    # Publish response
                    response_msg = String()
                    response_msg.data = f"Processing: {task['command']}"
                    self.response_publisher.publish(response_msg)

                    self.get_logger().info(f'Executed actions: {result["action_sequence"]}')
                else:
                    self.get_logger().warn(f'Low confidence for command: {task["command"]}')

            except Exception as e:
                self.get_logger().error(f'Error processing task: {e}')
                error_msg = String()
                error_msg.data = f"Error: {str(e)}"
                self.response_publisher.publish(error_msg)

    def execute_action_sequence(self, actions: List[Dict[str, Any]]):
        """Execute a sequence of actions"""
        for action in actions:
            try:
                self.execute_single_action(action)
            except Exception as e:
                self.get_logger().error(f'Error executing action {action}: {e}')
                break

    def execute_single_action(self, action: Dict[str, Any]):
        """Execute a single action"""
        action_type = action['action']

        if action_type == 'move_base':
            self.execute_navigation_action(action)
        elif action_type == 'pick_object':
            self.execute_manipulation_action(action)
        elif action_type == 'place_object':
            self.execute_manipulation_action(action)
        else:
            self.get_logger().warn(f'Unknown action type: {action_type}')

    def execute_navigation_action(self, action: Dict[str, Any]):
        """Execute navigation-related actions"""
        # Implementation would use navigation2 stack
        pass

    def execute_manipulation_action(self, action: Dict[str, Any]):
        """Execute manipulation-related actions"""
        # Implementation would use moveit2 or similar
        pass
```

### Context Management and Memory

Cognitive robots need to maintain context and memory across interactions:

```python
class ContextManager:
    """Manage context and memory for cognitive robots"""

    def __init__(self, max_history: int = 100):
        self.max_history = max_history
        self.conversation_history = []
        self.object_locations = {}  # Object -> location mapping
        self.robot_state = {}  # Current robot state
        self.user_preferences = {}  # User preferences and habits

    def add_interaction(self, user_input: str, robot_response: str, timestamp: float = None):
        """Add an interaction to the conversation history"""
        if timestamp is None:
            import time
            timestamp = time.time()

        interaction = {
            'timestamp': timestamp,
            'user_input': user_input,
            'robot_response': robot_response,
            'entities': self.extract_entities(user_input)
        }

        self.conversation_history.append(interaction)

        # Trim history if needed
        if len(self.conversation_history) > self.max_history:
            self.conversation_history = self.conversation_history[-self.max_history:]

    def extract_entities(self, text: str) -> Dict[str, List[str]]:
        """Extract named entities from text"""
        entities = {
            'objects': [],
            'locations': [],
            'people': [],
            'times': []
        }

        # Simple keyword-based extraction (in practice, use NER models)
        objects = ["cup", "mug", "bottle", "book", "phone", "keys", "plate", "fork", "spoon"]
        locations = ["kitchen", "living room", "bedroom", "office", "dining room", "bathroom"]

        for obj in objects:
            if obj in text.lower():
                entities['objects'].append(obj)

        for loc in locations:
            if loc in text.lower():
                entities['locations'].append(loc)

        return entities

    def update_object_location(self, obj: str, location: str, confidence: float = 1.0):
        """Update the known location of an object"""
        self.object_locations[obj] = {
            'location': location,
            'confidence': confidence,
            'timestamp': time.time()
        }

    def get_context_prompt(self, current_command: str) -> str:
        """Generate context prompt for LLM"""
        # Build context from recent interactions
        recent_interactions = self.conversation_history[-5:]  # Last 5 interactions

        context_parts = []

        # Add recent conversation history
        if recent_interactions:
            context_parts.append("Recent conversation:")
            for interaction in recent_interactions:
                context_parts.append(f"User: {interaction['user_input']}")
                context_parts.append(f"Robot: {interaction['robot_response']}")

        # Add known object locations
        if self.object_locations:
            context_parts.append("\nKnown object locations:")
            for obj, info in self.object_locations.items():
                if time.time() - info['timestamp'] < 3600:  # Less than 1 hour old
                    context_parts.append(f"- {obj} is in {info['location']} (confidence: {info['confidence']:.2f})")

        # Add current command
        context_parts.append(f"\nCurrent command: {current_command}")
        context_parts.append("Please generate appropriate robot actions based on the context above.")

        return "\n".join(context_parts)

    def get_relevant_context(self, query: str) -> str:
        """Get context relevant to a specific query"""
        # Simple keyword-based relevance (in practice, use semantic search)
        relevant_parts = []

        # Check conversation history for relevant objects/locations
        keywords = query.lower().split()
        for interaction in self.conversation_history[-10:]:  # Check last 10 interactions
            interaction_text = f"{interaction['user_input']} {interaction['robot_response']}".lower()
            if any(keyword in interaction_text for keyword in keywords):
                relevant_parts.append(f"Context: {interaction['user_input']} -> {interaction['robot_response']}")

        return "\n".join(relevant_parts) if relevant_parts else "No relevant context found."

class MemoryEnhancedCognitiveNode(CognitiveNode):
    """Cognitive node with enhanced memory and context management"""

    def __init__(self):
        super().__init__()

        # Initialize context manager
        self.context_manager = ContextManager()

        # Service for context queries
        self.context_service = self.create_service(
            String,
            'context_query',
            self.context_callback
        )

    def command_callback(self, msg: String):
        """Handle commands with context"""
        command = msg.data

        # Get relevant context
        context_prompt = self.context_manager.get_context_prompt(command)

        # Process with context
        full_prompt = f"{context_prompt}\n\nCommand: {command}"

        # For now, just pass to parent method, but in practice you'd use the context
        super().command_callback(msg)

        # Add to context history
        self.context_manager.add_interaction(command, "Processing...")

    def context_callback(self, request: String, response):
        """Handle context queries"""
        context = self.context_manager.get_relevant_context(request.data)
        response.data = context
        return response
```

## Task Planning and Execution

### Hierarchical Task Networks

LLM integration enables sophisticated task planning using hierarchical structures:

```python
class HierarchicalTaskPlanner:
    """Hierarchical task planner using LLM for decomposition"""

    def __init__(self, llm_model: RobotLanguageModel):
        self.llm_model = llm_model
        self.task_library = self.initialize_task_library()

    def initialize_task_library(self) -> Dict[str, Any]:
        """Initialize the library of known tasks and their decompositions"""
        return {
            'fetch_object': {
                'description': 'Fetch an object from a location',
                'subtasks': ['navigate_to_location', 'identify_object', 'grasp_object', 'navigate_to_destination', 'place_object']
            },
            'room_cleanup': {
                'description': 'Clean up a room by putting objects in their proper places',
                'subtasks': ['scan_room', 'identify_misplaced_objects', 'classify_objects', 'plan_collection_sequence', 'collect_and_store_objects']
            },
            'guided_tour': {
                'description': 'Give a guided tour of a location',
                'subtasks': ['plan_route', 'navigate_to_point_of_interest', 'provide_information', 'wait_for_attention', 'move_to_next_point']
            }
        }

    def decompose_task(self, high_level_task: str) -> List[Dict[str, Any]]:
        """Decompose a high-level task into executable subtasks"""
        # Check if task is in library
        if high_level_task in self.task_library:
            subtasks = self.task_library[high_level_task]['subtasks']
            return [{'task': subtask, 'parameters': {}} for subtask in subtasks]

        # Use LLM to decompose novel tasks
        return self.llm_decompose_task(high_level_task)

    def llm_decompose_task(self, task_description: str) -> List[Dict[str, Any]]:
        """Use LLM to decompose a task description into subtasks"""
        prompt = f"""
        Decompose the following high-level task into specific, executable subtasks for a robot:
        Task: {task_description}

        Provide the subtasks as a list of dictionaries with 'task' and 'parameters' keys.
        Example format:
        [
            {{"task": "navigate_to", "parameters": {{"location": "kitchen"}}}},
            {{"task": "detect_object", "parameters": {{"object_type": "cup"}}}},
            {{"task": "grasp_object", "parameters": {{"object_id": "detected_cup"}}}}
        ]

        Return only the JSON list, no other text.
        """

        # In practice, this would call the LLM
        # For this example, return a simple decomposition
        if "fetch" in task_description.lower() or "bring" in task_description.lower():
            return [
                {"task": "navigate_to", "parameters": {"location": self.extract_location(task_description)}},
                {"task": "detect_object", "parameters": {"object_type": self.extract_object(task_description)}},
                {"task": "grasp_object", "parameters": {"object_id": "detected_object"}},
                {"task": "navigate_to", "parameters": {"location": "delivery_location"}},
                {"task": "place_object", "parameters": {"placement_location": "delivery_surface"}}
            ]
        else:
            return [{"task": "unknown_task", "parameters": {"description": task_description}}]

    def extract_location(self, task: str) -> str:
        """Extract location from task description"""
        # Simple extraction (in practice, use NLP)
        locations = ["kitchen", "living room", "bedroom", "office", "dining room"]
        for loc in locations:
            if loc in task.lower():
                return loc
        return "unknown"

    def extract_object(self, task: str) -> str:
        """Extract object from task description"""
        # Simple extraction (in practice, use NLP)
        objects = ["cup", "mug", "bottle", "book", "phone", "keys"]
        for obj in objects:
            if obj in task.lower():
                return obj
        return "unknown"

class TaskExecutionManager:
    """Manage the execution of hierarchical tasks"""

    def __init__(self, ros_node: Node):
        self.ros_node = ros_node
        self.planner = HierarchicalTaskPlanner(None)  # Will be initialized with actual model
        self.active_tasks = []
        self.task_status = {}  # task_id -> status

    def execute_task(self, task_description: str) -> str:
        """Execute a high-level task by decomposing and running subtasks"""
        task_id = f"task_{int(time.time())}"

        # Decompose task
        subtasks = self.planner.decompose_task(task_description)

        # Execute subtasks sequentially
        for i, subtask in enumerate(subtasks):
            self.ros_node.get_logger().info(f'Executing subtask {i+1}/{len(subtasks)}: {subtask["task"]}')

            success = self.execute_subtask(subtask)

            if not success:
                self.ros_node.get_logger().error(f'Subtask failed: {subtask}')
                return f"Task failed at subtask {i+1}: {subtask['task']}"

        return f"Task completed successfully: {task_description}"

    def execute_subtask(self, subtask: Dict[str, Any]) -> bool:
        """Execute a single subtask"""
        task_type = subtask['task']
        params = subtask['parameters']

        try:
            if task_type == 'navigate_to':
                return self.execute_navigation(params)
            elif task_type == 'detect_object':
                return self.execute_detection(params)
            elif task_type == 'grasp_object':
                return self.execute_grasping(params)
            elif task_type == 'place_object':
                return self.execute_placement(params)
            else:
                self.ros_node.get_logger().warn(f'Unknown subtask type: {task_type}')
                return False
        except Exception as e:
            self.ros_node.get_logger().error(f'Error executing subtask {subtask}: {e}')
            return False

    def execute_navigation(self, params: Dict[str, Any]) -> bool:
        """Execute navigation subtask"""
        # Implementation would use navigation2
        location = params.get('location', 'unknown')
        self.ros_node.get_logger().info(f'Navigating to {location}')
        # In practice, send goal to navigation system
        return True

    def execute_detection(self, params: Dict[str, Any]) -> bool:
        """Execute object detection subtask"""
        obj_type = params.get('object_type', 'unknown')
        self.ros_node.get_logger().info(f'Detecting {obj_type}')
        # In practice, use perception system
        return True

    def execute_grasping(self, params: Dict[str, Any]) -> bool:
        """Execute grasping subtask"""
        obj_id = params.get('object_id', 'unknown')
        self.ros_node.get_logger().info(f'Grasping {obj_id}')
        # In practice, use manipulation system
        return True

    def execute_placement(self, params: Dict[str, Any]) -> bool:
        """Execute placement subtask"""
        location = params.get('placement_location', 'default')
        self.ros_node.get_logger().info(f'Placing object at {location}')
        # In practice, use manipulation system
        return True
```

## Safety and Validation

### Confidence Assessment and Validation

LLM outputs must be validated before execution to ensure safety:

```python
class ConfidenceAssessor:
    """Assess confidence in LLM outputs for robotic execution"""

    def __init__(self, min_confidence: float = 0.7):
        self.min_confidence = min_confidence
        self.action_validator = ActionValidator()

    def assess_command(self, command: str, llm_output: Dict[str, Any]) -> Dict[str, Any]:
        """Assess confidence and safety of LLM command interpretation"""
        results = {
            'confidence': self.calculate_confidence(command, llm_output),
            'safe_to_execute': False,
            'validation_errors': [],
            'suggested_corrections': []
        }

        # Validate action sequence
        validation_result = self.action_validator.validate_action_sequence(
            llm_output.get('action_sequence', [])
        )

        results['validation_errors'] = validation_result['errors']
        results['suggested_corrections'] = validation_result['corrections']

        # Check if command is safe to execute
        results['safe_to_execute'] = (
            results['confidence'] >= self.min_confidence and
            not validation_result['has_critical_errors']
        )

        return results

    def calculate_confidence(self, command: str, llm_output: Dict[str, Any]) -> float:
        """Calculate confidence score for command interpretation"""
        # Multiple factors contribute to confidence:

        # 1. LLM's own confidence (if provided)
        llm_confidence = llm_output.get('confidence', 0.5)

        # 2. Command clarity (length, keywords, structure)
        clarity_score = self.assess_command_clarity(command)

        # 3. Action feasibility
        feasibility_score = self.assess_action_feasibility(llm_output.get('action_sequence', []))

        # Weighted average
        confidence = 0.5 * llm_confidence + 0.3 * clarity_score + 0.2 * feasibility_score

        return min(confidence, 1.0)  # Cap at 1.0

    def assess_command_clarity(self, command: str) -> float:
        """Assess how clear and unambiguous the command is"""
        score = 0.5  # Base score

        # Positive factors
        if len(command.split()) >= 3:  # At least 3 words
            score += 0.2
        if any(word in command.lower() for word in ['the', 'a', 'an']):  # Definite articles
            score += 0.1
        if any(word in command.lower() for word in ['please', 'could you', 'would you']):  # Politeness
            score += 0.1

        # Negative factors
        if '?' in command:  # Questions might not be commands
            score -= 0.1
        if command.lower().strip().endswith('?'):
            score -= 0.1

        return max(0.0, min(1.0, score))

    def assess_action_feasibility(self, action_sequence: List[Dict[str, Any]]) -> float:
        """Assess whether the action sequence is feasible"""
        if not action_sequence:
            return 0.1  # Very low for empty sequences

        feasible_actions = 0
        total_actions = len(action_sequence)

        for action in action_sequence:
            if self.is_action_feasible(action):
                feasible_actions += 1

        return feasible_actions / total_actions if total_actions > 0 else 0.0

    def is_action_feasible(self, action: Dict[str, Any]) -> bool:
        """Check if a single action is feasible"""
        action_type = action.get('action', '')

        # Check for obviously invalid actions
        invalid_actions = ['destroy', 'break', 'damage', 'harm', 'attack']
        if any(invalid in action_type.lower() for invalid in invalid_actions):
            return False

        # Check for dangerous locations
        params = action.get('parameters', {})
        dangerous_locations = ['cliff', 'lava', 'fire', 'dangerous_area']
        if 'location' in params:
            if any(danger in params['location'].lower() for danger in dangerous_locations):
                return False

        return True

class ActionValidator:
    """Validate robot actions for safety and feasibility"""

    def __init__(self):
        self.known_actions = {
            'move_base', 'pick_object', 'place_object', 'grasp', 'release',
            'navigate', 'inspect', 'follow', 'stop', 'wait', 'speak'
        }
        self.dangerous_objects = {'knife', 'blade', 'sharp', 'fire', 'hot'}
        self.restricted_areas = {'restricted', 'danger', 'unsafe'}

    def validate_action_sequence(self, action_sequence: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Validate a sequence of actions"""
        errors = []
        corrections = []
        critical_errors = False

        for i, action in enumerate(action_sequence):
            action_errors, action_corrections, is_critical = self.validate_action(action, i)
            errors.extend(action_errors)
            corrections.extend(action_corrections)
            if is_critical:
                critical_errors = True

        return {
            'errors': errors,
            'corrections': corrections,
            'has_critical_errors': critical_errors
        }

    def validate_action(self, action: Dict[str, Any], index: int) -> tuple:
        """Validate a single action"""
        errors = []
        corrections = []
        is_critical = False

        # Check if action type is known
        action_type = action.get('action', '').lower()
        if action_type not in self.known_actions:
            errors.append(f"Unknown action type '{action_type}' at index {index}")
            is_critical = True

        # Check parameters
        params = action.get('parameters', {})

        # Validate object safety
        obj = params.get('object', '').lower()
        if any(danger in obj for danger in self.dangerous_objects):
            errors.append(f"Potentially dangerous object '{obj}' at index {index}")
            is_critical = True

        # Validate location safety
        location = params.get('location', '').lower()
        if any(restricted in location for restricted in self.restricted_areas):
            errors.append(f"Restricted location '{location}' at index {index}")
            is_critical = True

        # Check for required parameters
        if action_type in ['move_base', 'navigate']:
            if 'location' not in params:
                errors.append(f"Missing location parameter for {action_type} at index {index}")
                is_critical = True

        if action_type in ['pick_object', 'grasp']:
            if 'object' not in params and 'object_id' not in params:
                errors.append(f"Missing object parameter for {action_type} at index {index}")
                is_critical = True

        return errors, corrections, is_critical

class SafeCognitiveNode(CognitiveNode):
    """Cognitive node with safety validation"""

    def __init__(self):
        super().__init__()

        # Initialize safety components
        self.confidence_assessor = ConfidenceAssessor(min_confidence=0.7)
        self.task_executor = TaskExecutionManager(self)

        # Safety service
        self.safety_service = self.create_service(
            String,
            'safety_check',
            self.safety_callback
        )

    def command_callback(self, msg: String):
        """Handle commands with safety validation"""
        command = msg.data

        try:
            # Parse command with LLM
            result = self.llm_model.parse_command(command)

            # Assess confidence and safety
            safety_check = self.confidence_assessor.assess_command(command, result)

            if safety_check['safe_to_execute']:
                # Execute the task
                task_result = self.task_executor.execute_task(command)

                # Publish success
                response_msg = String()
                response_msg.data = f"Success: {task_result}"
                self.response_publisher.publish(response_msg)

                self.get_logger().info(f'Executed command safely: {command}')
            else:
                # Report safety issues
                error_msg = String()
                error_msg.data = f"Safety validation failed: {safety_check['validation_errors']}"
                self.response_publisher.publish(error_msg)

                self.get_logger().warn(f'Safety validation failed for command: {command}')

        except Exception as e:
            self.get_logger().error(f'Error processing command safely: {e}')
            error_msg = String()
            error_msg.data = f"Error: {str(e)}"
            self.response_publisher.publish(error_msg)

    def safety_callback(self, request: String, response):
        """Handle safety check requests"""
        try:
            result = self.llm_model.parse_command(request.data)
            safety_check = self.confidence_assessor.assess_command(request.data, result)

            response.data = str(safety_check)
        except Exception as e:
            response.data = f"Error in safety check: {str(e)}"

        return response
```

## Performance Optimization

### Caching and Efficient Processing

To maintain real-time performance, cognitive systems need efficient processing mechanisms:

```python
from functools import lru_cache
import threading
from queue import Queue, Empty
import asyncio

class OptimizedCognitiveNode(SafeCognitiveNode):
    """Cognitive node with performance optimizations"""

    def __init__(self):
        super().__init__()

        # LLM response cache
        self.response_cache = {}
        self.cache_lock = threading.Lock()

        # Async processing queue
        self.processing_queue = Queue()
        self.result_queue = Queue()

        # Start processing thread
        self.processing_thread = threading.Thread(target=self.process_commands_async, daemon=True)
        self.processing_thread.start()

    @lru_cache(maxsize=128)
    def cached_llm_process(self, command: str) -> Dict[str, Any]:
        """Cached LLM processing for repeated commands"""
        return self.llm_model.parse_command(command)

    def command_callback(self, msg: String):
        """Handle commands with caching and async processing"""
        command = msg.data

        # Check cache first
        with self.cache_lock:
            if command in self.response_cache:
                cached_result = self.response_cache[command]
                # Validate cache freshness if needed
                if time.time() - cached_result['timestamp'] < 300:  # 5 minutes
                    self.handle_cached_result(command, cached_result['result'])
                    return

        # Add to processing queue
        self.processing_queue.put({
            'command': command,
            'timestamp': time.time()
        })

    def process_commands_async(self):
        """Process commands asynchronously"""
        while rclpy.ok():
            try:
                task = self.processing_queue.get(timeout=0.1)
                command = task['command']

                # Process with LLM
                result = self.cached_llm_process(command)

                # Add to response queue
                self.result_queue.put({
                    'command': command,
                    'result': result,
                    'timestamp': task['timestamp']
                })

            except Empty:
                continue
            except Exception as e:
                self.get_logger().error(f'Error in async processing: {e}')

    def handle_cached_result(self, command: str, result: Dict[str, Any]):
        """Handle a cached LLM result"""
        safety_check = self.confidence_assessor.assess_command(command, result)

        if safety_check['safe_to_execute']:
            # Execute the task
            task_result = self.task_executor.execute_task(command)

            response_msg = String()
            response_msg.data = f"Success: {task_result}"
            self.response_publisher.publish(response_msg)
        else:
            error_msg = String()
            error_msg.data = f"Safety validation failed: {safety_check['validation_errors']}"
            self.response_publisher.publish(error_msg)

class BatchProcessingCognitiveNode(OptimizedCognitiveNode):
    """Cognitive node with batch processing capabilities"""

    def __init__(self):
        super().__init__()

        # Batch processing parameters
        self.batch_size = 5
        self.batch_timeout = 1.0  # seconds
        self.command_batch = []
        self.batch_timer = None

    def command_callback(self, msg: String):
        """Collect commands for batch processing"""
        command = msg.data
        self.command_batch.append({
            'command': command,
            'msg': msg,
            'timestamp': time.time()
        })

        # Start timer if first command in batch
        if len(self.command_batch) == 1:
            self.batch_timer = self.create_timer(
                self.batch_timeout,
                self.process_batch
            )

        # Process batch if it reaches the limit
        if len(self.command_batch) >= self.batch_size:
            self.process_batch()

    def process_batch(self):
        """Process a batch of commands"""
        if self.batch_timer:
            self.destroy_timer(self.batch_timer)
            self.batch_timer = None

        if not self.command_batch:
            return

        commands = [item['command'] for item in self.command_batch]

        # Process all commands at once (if model supports batching)
        try:
            batch_results = self.process_command_batch(commands)

            # Handle each result
            for i, result in enumerate(batch_results):
                original_item = self.command_batch[i]
                self.handle_single_result(original_item['command'], result)

        except Exception as e:
            self.get_logger().error(f'Error in batch processing: {e}')

        # Clear batch
        self.command_batch = []

    def process_command_batch(self, commands: List[str]) -> List[Dict[str, Any]]:
        """Process a batch of commands (simplified - in practice, use model's batch capabilities)"""
        return [self.llm_model.parse_command(cmd) for cmd in commands]
```

LLM integration with ROS 2 enables cognitive robotics systems that can understand natural language commands and execute complex tasks. The integration requires careful consideration of safety, real-time performance, and the distributed nature of ROS 2 systems. By implementing proper validation, caching, and safety mechanisms, robots can leverage the power of large language models while maintaining reliability and safety in real-world applications.

The key to successful LLM integration lies in creating robust interfaces between high-level language understanding and low-level robot control, with appropriate validation and safety checks at each step of the process.
