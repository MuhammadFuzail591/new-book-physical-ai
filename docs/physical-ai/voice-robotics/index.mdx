---
title: Chapter 12 - Voice-to-Action Robotics with Whisper & ROS 2
---

# Chapter 12: Voice-to-Action Robotics with Whisper & ROS 2

## Chapter Overview

This chapter explores the revolutionary integration of advanced speech recognition technology with robotic systems, specifically focusing on OpenAI's Whisper model and its integration with ROS 2 (Robot Operating System 2). Voice-to-action robotics represents a paradigm shift in human-robot interaction, enabling intuitive, natural communication between humans and robotic systems through spoken language. This technology is particularly crucial for Physical AI applications where robots need to understand and execute complex verbal commands in real-time.

The chapter covers the technical foundations of speech recognition in robotics, the integration of Whisper's state-of-the-art transcription capabilities with ROS 2's communication framework, and the implementation of natural language processing pipelines that translate spoken commands into robotic actions. We'll examine how modern AI speech recognition can be leveraged to create more intuitive and accessible robotic interfaces.

## Learning Outcomes

By the end of this chapter, you will be able to:
- Implement Whisper-based speech recognition in robotic systems using ROS 2
- Design voice command interpretation systems that translate speech to robot actions
- Integrate real-time speech processing with robotic control pipelines
- Develop natural language understanding modules for robot command execution
- Optimize voice-to-action systems for real-time performance and accuracy
- Create robust voice interfaces that handle environmental noise and ambiguity

## Introduction to Voice-to-Action Robotics

### The Evolution of Human-Robot Interaction

Traditional human-robot interaction has relied primarily on graphical user interfaces, dedicated control panels, or pre-programmed gesture recognition. Voice-to-action robotics introduces a more natural and intuitive interface that mirrors human-to-human communication patterns. This approach is particularly beneficial for:

- **Accessibility**: Enabling interaction for users with limited mobility or visual impairments
- **Hands-free Operation**: Allowing humans to control robots while performing other tasks
- **Intuitive Communication**: Using natural language that doesn't require specialized training
- **Multimodal Integration**: Combining voice commands with visual and tactile feedback

### Whisper Model in Robotics Context

OpenAI's Whisper model represents a breakthrough in speech recognition technology, offering exceptional accuracy across multiple languages and robustness to various acoustic conditions. In robotics applications, Whisper's capabilities are particularly valuable due to:

- **Multilingual Support**: Understanding commands in multiple languages
- **Noise Robustness**: Maintaining accuracy in noisy environments
- **Real-time Processing**: Capable of near real-time transcription with appropriate optimization
- **Context Understanding**: Ability to maintain context across multiple utterances
- **Customization Potential**: Capability to fine-tune for domain-specific vocabularies

### Voice Command Architecture

The voice-to-action pipeline in robotics typically follows this architecture:

```
Audio Input → Speech Recognition → Natural Language Processing → Action Planning → Robot Execution
```

Each stage must operate efficiently to maintain responsive interaction while ensuring accuracy in command interpretation.

## Technical Foundation: Whisper Integration with ROS 2

### Setting Up Whisper for Robotics

To integrate Whisper with ROS 2 systems, we need to establish a robust audio processing pipeline:

```python
import rospy
import whisper
import torch
import numpy as np
from std_msgs.msg import String
from audio_common_msgs.msg import AudioData
from geometry_msgs.msg import Twist
from sensor_msgs.msg import Joy

class WhisperROSNode:
    def __init__(self):
        # Initialize ROS node
        rospy.init_node('whisper_voice_control', anonymous=True)

        # Load Whisper model (choose appropriate size based on computational resources)
        self.model_size = rospy.get_param('~model_size', 'base')
        self.model = whisper.load_model(self.model_size)

        # Audio buffer for continuous processing
        self.audio_buffer = []
        self.buffer_size = rospy.get_param('~buffer_size', 16000 * 2)  # 2 seconds of audio

        # Publishers and subscribers
        self.voice_command_pub = rospy.Publisher('/voice/command', String, queue_size=10)
        self.robot_cmd_pub = rospy.Publisher('/cmd_vel', Twist, queue_size=10)
        self.audio_sub = rospy.Subscriber('/audio/audio', AudioData, self.audio_callback)

        # Configuration parameters
        self.silence_threshold = rospy.get_param('~silence_threshold', 0.01)
        self.vad_threshold = rospy.get_param('~vad_threshold', 0.3)  # Voice activity detection
        self.language = rospy.get_param('~language', 'en')

        # Command mapping dictionary
        self.command_map = {
            'move forward': self.move_forward,
            'move backward': self.move_backward,
            'turn left': self.turn_left,
            'turn right': self.turn_right,
            'stop': self.stop_robot,
            'come here': self.go_to_user,
            'follow me': self.follow_user,
            'pick up object': self.pick_object,
            'place object': self.place_object,
        }

        rospy.loginfo("Whisper voice control node initialized")

    def audio_callback(self, audio_msg):
        """Handle incoming audio data"""
        # Convert audio data to numpy array
        audio_array = np.frombuffer(audio_msg.data, dtype=np.int16).astype(np.float32) / 32768.0

        # Add to buffer
        self.audio_buffer.extend(audio_array)

        # Process if buffer is full
        if len(self.audio_buffer) >= self.buffer_size:
            self.process_audio_buffer()
            # Keep some overlap for continuity
            self.audio_buffer = self.audio_buffer[-int(self.buffer_size/4):]

    def process_audio_buffer(self):
        """Process the accumulated audio buffer"""
        if len(self.audio_buffer) == 0:
            return

        # Convert to appropriate format for Whisper
        audio_data = np.array(self.audio_buffer)

        # Check for voice activity (simplified approach)
        if np.max(np.abs(audio_data)) < self.silence_threshold:
            return  # Skip silent segments

        try:
            # Transcribe audio using Whisper
            result = self.model.transcribe(
                audio_data,
                language=self.language,
                task='transcribe'
            )

            transcription = result['text'].strip().lower()

            if transcription:  # Only process non-empty transcriptions
                rospy.loginfo(f"Transcribed: {transcription}")
                self.process_command(transcription)

        except Exception as e:
            rospy.logerr(f"Error in Whisper transcription: {e}")

    def process_command(self, command_text):
        """Process the transcribed command and execute appropriate action"""
        # Publish the recognized command
        cmd_msg = String()
        cmd_msg.data = command_text
        self.voice_command_pub.publish(cmd_msg)

        # Try to match command with known actions
        matched = False
        for keyword, action_func in self.command_map.items():
            if keyword in command_text:
                rospy.loginfo(f"Executing command: {keyword}")
                action_func()
                matched = True
                break

        if not matched:
            # Try fuzzy matching for similar commands
            best_match = self.find_best_command_match(command_text)
            if best_match:
                rospy.loginfo(f"Fuzzy match: executing {best_match}")
                self.command_map[best_match]()
            else:
                rospy.loginfo(f"Unknown command: {command_text}")

    def find_best_command_match(self, input_text):
        """Find the best matching command using fuzzy logic"""
        from difflib import SequenceMatcher

        best_match = None
        best_ratio = 0.0
        threshold = 0.6  # Minimum similarity ratio

        for command in self.command_map.keys():
            ratio = SequenceMatcher(None, input_text, command).ratio()
            if ratio > best_ratio and ratio > threshold:
                best_ratio = ratio
                best_match = command

        return best_match

    # Robot action implementations
    def move_forward(self):
        cmd = Twist()
        cmd.linear.x = 0.5  # Adjust speed as needed
        self.robot_cmd_pub.publish(cmd)

    def move_backward(self):
        cmd = Twist()
        cmd.linear.x = -0.5
        self.robot_cmd_pub.publish(cmd)

    def turn_left(self):
        cmd = Twist()
        cmd.angular.z = 0.5
        self.robot_cmd_pub.publish(cmd)

    def turn_right(self):
        cmd = Twist()
        cmd.angular.z = -0.5
        self.robot_cmd_pub.publish(cmd)

    def stop_robot(self):
        cmd = Twist()
        self.robot_cmd_pub.publish(cmd)

    def go_to_user(self):
        # This would involve more complex navigation logic
        rospy.loginfo("Initiating navigation to user")
        # Implementation would involve person detection and navigation

    def follow_user(self):
        # Follow-behavior implementation
        rospy.loginfo("Starting follow behavior")
        # Implementation would involve person tracking and following

    def pick_object(self):
        # Manipulation command implementation
        rospy.loginfo("Attempting to pick up object")
        # Implementation would involve perception and manipulation

    def place_object(self):
        # Manipulation command implementation
        rospy.loginfo("Attempting to place object")
        # Implementation would involve manipulation planning

    def spin(self):
        """Main loop"""
        rospy.spin()
```

### Advanced Natural Language Processing

For more sophisticated command interpretation, we can enhance our system with advanced NLP techniques:

```python
import spacy
import transformers
from transformers import pipeline
from typing import Dict, List, Tuple

class AdvancedVoiceCommandProcessor:
    def __init__(self):
        # Load spaCy model for linguistic analysis
        try:
            self.nlp = spacy.load("en_core_web_sm")
        except OSError:
            rospy.logwarn("spaCy English model not found. Install with: python -m spacy download en_core_web_sm")
            self.nlp = None

        # Load a question-answering model for command disambiguation
        self.qa_pipeline = pipeline(
            "question-answering",
            model="distilbert-base-cased-distilled-squad"
        )

        # Define command templates and entities
        self.command_templates = {
            'navigation': {
                'patterns': [
                    'go to the {location}',
                    'move to {location}',
                    'navigate to {location}',
                    'go to {location} room',
                ],
                'entities': ['location']
            },
            'manipulation': {
                'patterns': [
                    'pick up the {object}',
                    'grab the {object}',
                    'take the {object}',
                    'move the {object} to {destination}',
                ],
                'entities': ['object', 'destination']
            },
            'interaction': {
                'patterns': [
                    'talk to me',
                    'tell me about yourself',
                    'introduce yourself',
                    'what can you do',
                ],
                'entities': []
            }
        }

    def parse_command_advanced(self, text: str) -> Dict:
        """Advanced command parsing using NLP techniques"""
        if self.nlp:
            doc = self.nlp(text)

            # Extract entities
            entities = {}
            for ent in doc.ents:
                entities[ent.label_] = ent.text

            # Analyze dependencies and parts of speech
            action_verb = None
            for token in doc:
                if token.pos_ == "VERB":
                    action_verb = token.lemma_
                    break

            # Determine command type based on patterns
            command_type = self.classify_command_type(text)

            return {
                'command_type': command_type,
                'entities': entities,
                'action_verb': action_verb,
                'original_text': text,
                'parsed': True
            }
        else:
            # Fallback to simple keyword matching
            return self.simple_parse_command(text)

    def classify_command_type(self, text: str) -> str:
        """Classify command into predefined categories"""
        text_lower = text.lower()

        for cmd_type, template_info in self.command_templates.items():
            for pattern in template_info['patterns']:
                # Simple pattern matching (could be enhanced with regex)
                if any(keyword in text_lower for keyword in pattern.split() if '{' not in keyword):
                    return cmd_type

        return 'unknown'

    def simple_parse_command(self, text: str) -> Dict:
        """Fallback command parsing without advanced NLP"""
        # Simple keyword-based parsing
        entities = {}

        if 'room' in text:
            entities['location'] = text.split('room')[0].split()[-1] + ' room'
        elif 'kitchen' in text or 'bedroom' in text or 'living room' in text:
            for room_type in ['kitchen', 'bedroom', 'living room', 'bathroom']:
                if room_type in text:
                    entities['location'] = room_type
                    break

        if 'object' in text or 'thing' in text or 'item' in text:
            # Extract object mentions
            for word in text.split():
                if word.lower() in ['ball', 'cup', 'book', 'box', 'toy']:
                    entities['object'] = word
                    break

        return {
            'command_type': self.classify_command_type(text),
            'entities': entities,
            'action_verb': None,
            'original_text': text,
            'parsed': False
        }
```

## Real-time Voice Processing Optimization

### Streaming Audio Processing

For real-time applications, we need to implement efficient streaming audio processing:

```python
import pyaudio
import threading
import queue
import time

class RealTimeVoiceProcessor:
    def __init__(self, model_size='base'):
        self.model = whisper.load_model(model_size)

        # Audio configuration
        self.rate = 16000  # Sampling rate
        self.chunk = 1024  # Audio chunk size
        self.format = pyaudio.paInt16
        self.channels = 1

        # Processing buffers
        self.audio_queue = queue.Queue()
        self.transcription_queue = queue.Queue()

        # Voice activity detection parameters
        self.energy_threshold = 0.01
        self.silence_duration = 1.0  # Seconds of silence to trigger processing
        self.continuous_audio = np.array([], dtype=np.float32)

        # Threading
        self.processing_thread = threading.Thread(target=self.process_audio_stream)
        self.processing_thread.daemon = True
        self.running = True

        # Callback function for command processing
        self.command_callback = None

    def start_audio_capture(self):
        """Start capturing audio from microphone"""
        self.audio = pyaudio.PyAudio()

        self.stream = self.audio.open(
            format=self.format,
            channels=self.channels,
            rate=self.rate,
            input=True,
            frames_per_buffer=self.chunk,
            stream_callback=self.audio_callback
        )

        self.processing_thread.start()
        self.stream.start_stream()

    def audio_callback(self, in_data, frame_count, time_info, status):
        """Callback for real-time audio capture"""
        audio_data = np.frombuffer(in_data, dtype=np.int16).astype(np.float32) / 32768.0

        # Add to continuous buffer
        self.continuous_audio = np.concatenate([self.continuous_audio, audio_data])

        # Keep only the last 5 seconds to manage memory
        max_samples = self.rate * 5
        if len(self.continuous_audio) > max_samples:
            self.continuous_audio = self.continuous_audio[-max_samples:]

        return (in_data, pyaudio.paContinue)

    def process_audio_stream(self):
        """Continuously process audio for voice commands"""
        last_voice_time = time.time()

        while self.running:
            current_time = time.time()

            # Check if we have enough audio and sufficient silence has passed
            if (len(self.continuous_audio) > self.rate * 0.5 and  # At least 0.5 seconds
                current_time - last_voice_time > self.silence_duration):

                # Check for voice activity in the last segment
                recent_audio = self.continuous_audio[-int(self.rate * 2):]  # Last 2 seconds
                if np.max(np.abs(recent_audio)) > self.energy_threshold:
                    # Process this segment
                    self.process_segment(recent_audio)
                    last_voice_time = current_time
                    # Clear the processed portion to manage buffer size
                    self.continuous_audio = self.continuous_audio[:-int(self.rate * 2)]

            time.sleep(0.1)  # Small delay to prevent busy waiting

    def process_segment(self, audio_segment):
        """Process a segment of audio for speech recognition"""
        try:
            result = self.model.transcribe(
                audio_segment,
                language='en',
                task='transcribe',
                temperature=0.0  # More deterministic for commands
            )

            if result['text'].strip():  # If we got a transcription
                transcription = result['text'].strip()
                rospy.loginfo(f"Real-time transcription: {transcription}")

                # Process the command if callback is set
                if self.command_callback:
                    self.command_callback(transcription)

        except Exception as e:
            rospy.logerr(f"Error processing audio segment: {e}")

    def set_command_callback(self, callback_func):
        """Set callback function for processed commands"""
        self.command_callback = callback_func

    def stop(self):
        """Stop audio processing"""
        self.running = False
        if hasattr(self, 'stream'):
            self.stream.stop_stream()
            self.stream.close()
        if hasattr(self, 'audio'):
            self.audio.terminate()
```

## Integration with Robot Control Systems

### Command Execution Pipeline

The voice-to-action system needs to be tightly integrated with the robot's control architecture:

```python
import actionlib
from move_base_msgs.msg import MoveBaseAction, MoveBaseGoal
from geometry_msgs.msg import PoseStamped
from std_msgs.msg import String

class VoiceCommandExecutor:
    def __init__(self):
        # Initialize ROS components
        self.nav_client = actionlib.SimpleActionClient('move_base', MoveBaseAction)
        self.manipulation_client = actionlib.SimpleActionClient('manipulation_server', ManipulationAction)

        # Wait for servers
        rospy.loginfo("Waiting for move_base action server...")
        self.nav_client.wait_for_server(rospy.Duration(10.0))

        rospy.loginfo("Waiting for manipulation action server...")
        self.manipulation_client.wait_for_server(rospy.Duration(10.0))

        # Publishers for different robot systems
        self.cmd_vel_pub = rospy.Publisher('/cmd_vel', Twist, queue_size=10)
        self.speech_pub = rospy.Publisher('/tts/input', String, queue_size=10)

        # Location mapping
        self.location_map = {
            'kitchen': self.get_kitchen_pose(),
            'bedroom': self.get_bedroom_pose(),
            'living room': self.get_living_room_pose(),
            'office': self.get_office_pose(),
            'dining room': self.get_dining_room_pose()
        }

    def execute_navigation_command(self, destination):
        """Execute navigation command to specified destination"""
        if destination.lower() in self.location_map:
            goal_pose = self.location_map[destination.lower()]
            goal = MoveBaseGoal()
            goal.target_pose = goal_pose

            rospy.loginfo(f"Navigating to {destination}")
            self.nav_client.send_goal(goal)

            # Wait for result with timeout
            finished_within_time = self.nav_client.wait_for_result(rospy.Duration(60.0))

            if not finished_within_time:
                self.nav_client.cancel_goal()
                rospy.loginfo("Navigation timeout")
                return False
            else:
                state = self.nav_client.get_state()
                if state == actionlib.GoalStatus.SUCCEEDED:
                    rospy.loginfo(f"Successfully reached {destination}")
                    self.speak_response(f"I have reached the {destination}")
                    return True
                else:
                    rospy.loginfo(f"Failed to reach {destination}")
                    self.speak_response(f"Sorry, I couldn't reach the {destination}")
                    return False
        else:
            rospy.loginfo(f"Unknown destination: {destination}")
            self.speak_response(f"Sorry, I don't know where the {destination} is")
            return False

    def execute_manipulation_command(self, object_name, action='pick'):
        """Execute manipulation command for specified object"""
        goal = ManipulationGoal()
        goal.object_name = object_name
        goal.action = action  # 'pick', 'place', 'move'

        rospy.loginfo(f"Attempting to {action} {object_name}")
        self.manipulation_client.send_goal(goal)

        finished_within_time = self.manipulation_client.wait_for_result(rospy.Duration(30.0))

        if not finished_within_time:
            self.manipulation_client.cancel_goal()
            rospy.loginfo("Manipulation timeout")
            self.speak_response(f"Sorry, I couldn't {action} the {object_name}")
            return False
        else:
            state = self.manipulation_client.get_state()
            if state == actionlib.GoalStatus.SUCCEEDED:
                rospy.loginfo(f"Successfully {action}ed {object_name}")
                self.speak_response(f"I have successfully {action}ed the {object_name}")
                return True
            else:
                rospy.loginfo(f"Failed to {action} {object_name}")
                self.speak_response(f"Sorry, I couldn't {action} the {object_name}")
                return False

    def speak_response(self, text):
        """Publish text-to-speech response"""
        msg = String()
        msg.data = text
        self.speech_pub.publish(msg)

    def get_kitchen_pose(self):
        """Get predefined kitchen pose"""
        pose = PoseStamped()
        pose.header.frame_id = "map"
        pose.pose.position.x = 2.0
        pose.pose.position.y = 1.0
        pose.pose.orientation.w = 1.0
        return pose

    def get_bedroom_pose(self):
        """Get predefined bedroom pose"""
        pose = PoseStamped()
        pose.header.frame_id = "map"
        pose.pose.position.x = -1.0
        pose.pose.position.y = -2.0
        pose.pose.orientation.w = 1.0
        return pose

    def get_living_room_pose(self):
        """Get predefined living room pose"""
        pose = PoseStamped()
        pose.header.frame_id = "map"
        pose.pose.position.x = 0.0
        pose.pose.position.y = 0.0
        pose.pose.orientation.w = 1.0
        return pose

    def get_office_pose(self):
        """Get predefined office pose"""
        pose = PoseStamped()
        pose.header.frame_id = "map"
        pose.pose.position.x = 3.0
        pose.pose.position.y = -1.0
        pose.pose.orientation.w = 1.0
        return pose

    def get_dining_room_pose(self):
        """Get predefined dining room pose"""
        pose = PoseStamped()
        pose.header.frame_id = "map"
        pose.pose.position.x = 1.0
        pose.pose.position.y = 2.0
        pose.pose.orientation.w = 1.0
        return pose
```

## Performance Optimization and Accuracy Enhancement

### Model Optimization for Robotics

For robotics applications, we need to optimize Whisper for real-time performance:

```python
import torch
from transformers import WhisperForConditionalGeneration, WhisperProcessor
import time

class OptimizedWhisperProcessor:
    def __init__(self, model_name="openai/whisper-base"):
        # Use CUDA if available
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Load model and processor
        self.model = WhisperForConditionalGeneration.from_pretrained(model_name)
        self.processor = WhisperProcessor.from_pretrained(model_name)

        # Move model to device
        self.model.to(self.device)

        # Set model to evaluation mode
        self.model.eval()

        # Performance statistics
        self.processing_times = []
        self.accuracy_stats = []

    def transcribe_optimized(self, audio_input, language="en", task="transcribe"):
        """Optimized transcription with performance monitoring"""
        start_time = time.time()

        try:
            # Process audio input
            if isinstance(audio_input, np.ndarray):
                # Direct audio array
                input_features = self.processor(
                    audio_input,
                    sampling_rate=16000,
                    return_tensors="pt"
                ).input_features
            else:
                # File path or other input
                input_features = self.processor(
                    audio_input,
                    return_tensors="pt"
                ).input_features

            # Move to device
            input_features = input_features.to(self.device)

            # Generate token ids
            generated_ids = self.model.generate(
                input_features,
                language=language,
                task=task,
                # Optimization parameters
                max_new_tokens=128,  # Limit output length
                do_sample=False,     # Deterministic output for commands
                temperature=0.0,     # No randomness
            )

            # Decode transcription
            transcription = self.processor.batch_decode(
                generated_ids,
                skip_special_tokens=True
            )[0]

            # Record performance
            processing_time = time.time() - start_time
            self.processing_times.append(processing_time)

            rospy.loginfo(f"Transcription took {processing_time:.3f}s: {transcription}")

            return {
                'text': transcription.strip(),
                'processing_time': processing_time,
                'confidence': self.estimate_confidence(transcription)
            }

        except Exception as e:
            rospy.logerr(f"Error in optimized transcription: {e}")
            return {
                'text': '',
                'processing_time': time.time() - start_time,
                'error': str(e)
            }

    def estimate_confidence(self, transcription):
        """Estimate confidence in transcription"""
        # Simple confidence estimation based on various factors
        if not transcription or len(transcription.strip()) == 0:
            return 0.0

        # Length-based confidence (very short transcriptions might be unreliable)
        length_confidence = min(len(transcription) / 10.0, 1.0)  # Up to 10 characters = full confidence

        # Contains common words confidence
        common_words = ['the', 'and', 'is', 'are', 'to', 'for', 'on', 'in', 'at', 'by']
        word_count = len(transcription.split())
        common_word_ratio = sum(1 for word in transcription.lower().split() if word in common_words) / max(word_count, 1)

        # Combine confidences
        overall_confidence = (length_confidence * 0.6 + common_word_ratio * 0.4)

        return min(overall_confidence, 1.0)

    def get_performance_stats(self):
        """Get performance statistics"""
        if not self.processing_times:
            return {'avg_processing_time': 0, 'call_count': 0}

        return {
            'avg_processing_time': np.mean(self.processing_times),
            'min_processing_time': np.min(self.processing_times),
            'max_processing_time': np.max(self.processing_times),
            'call_count': len(self.processing_times)
        }
```

## Error Handling and Robustness

### Voice Command Error Recovery

Robust voice-to-action systems need comprehensive error handling:

```python
class VoiceCommandErrorRecovery:
    def __init__(self, command_executor):
        self.executor = command_executor
        self.command_history = []
        self.error_recovery_enabled = True

        # Context for disambiguation
        self.last_known_context = {}

    def execute_command_with_recovery(self, command_text, parsed_command=None):
        """Execute command with error recovery capabilities"""
        if not self.error_recovery_enabled:
            return self.executor.execute_simple_command(command_text)

        try:
            # Store command in history
            command_entry = {
                'text': command_text,
                'timestamp': rospy.Time.now(),
                'attempt_count': 0,
                'status': 'pending'
            }
            self.command_history.append(command_entry)

            # Attempt execution
            result = self.attempt_command_execution(command_text, parsed_command)

            if result['success']:
                command_entry['status'] = 'success'
                return result
            else:
                # Try recovery strategies
                recovery_result = self.attempt_error_recovery(command_text, result['error'])

                if recovery_result['success']:
                    command_entry['status'] = 'recovered'
                    return recovery_result
                else:
                    command_entry['status'] = 'failed'
                    self.handle_persistent_failure(command_text, recovery_result['error'])
                    return recovery_result

        except Exception as e:
            rospy.logerr(f"Error in command execution with recovery: {e}")
            return {'success': False, 'error': str(e)}

    def attempt_command_execution(self, command_text, parsed_command=None):
        """Attempt to execute a single command"""
        if parsed_command is None:
            parsed_command = self.parse_command(command_text)

        try:
            if parsed_command['command_type'] == 'navigation':
                success = self.executor.execute_navigation_command(parsed_command['entities'].get('location', ''))
            elif parsed_command['command_type'] == 'manipulation':
                object_name = parsed_command['entities'].get('object', '')
                action = self.determine_manipulation_action(command_text)
                success = self.executor.execute_manipulation_command(object_name, action)
            else:
                # Handle other command types
                success = self.handle_other_command_types(parsed_command)

            return {'success': success, 'command': parsed_command}

        except Exception as e:
            return {'success': False, 'error': str(e)}

    def attempt_error_recovery(self, command_text, error):
        """Attempt various recovery strategies"""
        recovery_strategies = [
            self.recovery_clarification_request,
            self.recovery_alternative_interpretation,
            self.recovery_context_aware_retry,
            self.recovery_simplification
        ]

        for strategy in recovery_strategies:
            try:
                result = strategy(command_text, error)
                if result['success']:
                    return result
            except Exception as e:
                rospy.logwarn(f"Recovery strategy failed: {e}")
                continue

        # If all recovery strategies fail
        return {'success': False, 'error': f"All recovery strategies failed: {error}"}

    def recovery_clarification_request(self, command_text, error):
        """Ask for clarification when command is ambiguous"""
        # Check if error is related to ambiguity
        if "ambiguous" in error.lower() or "unclear" in error.lower():
            # Ask for clarification
            clarification_prompt = f"I didn't understand your command '{command_text}'. Could you please repeat or rephrase it?"
            self.executor.speak_response(clarification_prompt)

            # In a real system, you would wait for a follow-up command
            # For now, return a recovery indication
            return {'success': False, 'needs_clarification': True, 'message': clarification_prompt}

        return {'success': False, 'error': error}

    def recovery_alternative_interpretation(self, command_text, error):
        """Try alternative interpretations of the command"""
        # Try different parsing approaches
        alternative_parses = [
            self.simple_keyword_parse(command_text),
            self.partial_match_parse(command_text),
            self.semantic_similarity_parse(command_text)
        ]

        for alt_parse in alternative_parses:
            if alt_parse and alt_parse.get('command_type'):
                try:
                    result = self.attempt_command_execution(command_text, alt_parse)
                    if result['success']:
                        return result
                except:
                    continue

        return {'success': False, 'error': error}

    def handle_persistent_failure(self, command_text, error):
        """Handle cases where command consistently fails"""
        rospy.logerr(f"Persistent failure for command '{command_text}': {error}")

        # Log the failure for analysis
        self.log_failure(command_text, error)

        # Provide user feedback
        feedback_msg = f"Sorry, I couldn't execute '{command_text}'. The command failed with error: {error}"
        self.executor.speak_response(feedback_msg)

    def log_failure(self, command_text, error):
        """Log command failures for system improvement"""
        failure_log = {
            'timestamp': rospy.Time.now(),
            'command': command_text,
            'error': error,
            'context': self.last_known_context.copy()
        }

        # In a real system, you might save this to a database or file
        rospy.loginfo(f"Logged failure: {failure_log}")
```

## Looking Forward

Voice-to-action robotics represents a significant advancement in human-robot interaction, making robots more accessible and intuitive to use. The integration of Whisper's advanced speech recognition with ROS 2's robust communication framework enables sophisticated voice-controlled robotic systems that can understand and execute complex commands in real-time.

As this technology continues to evolve, we can expect even more natural and sophisticated voice interfaces that will further blur the line between human and robot interaction, making robots truly collaborative partners in various applications from household assistance to industrial automation.

The techniques covered in this chapter provide the foundation for building robust, responsive voice-controlled robotic systems that can operate effectively in real-world environments with varying acoustic conditions and complex command requirements.
